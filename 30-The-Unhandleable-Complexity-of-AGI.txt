TITL:
   *The Unhandleable Complexity of AGI*
   By Forrest Landry
   November 8th, 2022.

ABST:
   Offers some specific concepts and connections
   to how AGI alignment efforts cannot not involve
   unhandleable complexity, which in itself
   strongly suggests that any effort to align AGI
   is for sure wasted time, in the long term.
   That it is better to not build any form of AGI
   in the first place.

TEXT:

   Where in regards to the inherent complexity of the
   'error correction schemes'
   that are being associated with AGI alignment efforts,
   and/or the the identification of
   what would even be even minimally required
   to even begin to think about
   such concepts of AGI-alignment:
     ?; Have you noticed the trend
     that more and more people
     are finding more and more
     (increasingly) *unhandleable* complexity
     the further anyone looks into
     any part of AGI 'alignment' research?.

   - ?; Have you noticed that there is an overall trend --
   that we are all finding (more and more)
   increasingly unhandleable complexity
   the further researchers have dug into
   AGI-alignment research --
   that the sheer number of problematic considerations
   actually keeps increasing, seemingly without limit,
   over these last several years?.

   - ?; maybe we can see this trend to/towards
   increasingly unhandleable complexity
   as we go further down we go into
   the detailed concrete properties of ML algorithms,
   and why/how they inherently do not lends themselves
   to/towards "alignment"?.

   This strongly suggests that alignment researchers
   will overall need to shift gears
   and instead very deliberately and constructively
   inform and foster shared awareness (@ note1 #note1)
   of the intractable nature of the problems.
     (ie; particularly with the public/tech funders
     and other AGI researchers still trying for
     some sort of "miracle cure").

:8kq
   > Yes, some people _have_ noticed that there is
   > more complexity now in thinking about 'how to do'
   > AGI-alignment, or in regards to any aspect of
   > the overall AGI alignment arguments.

   > However, at least some of the people do not
   > think that the complexity is "exploding".
   > Perhaps you are simply being alarmist?.

   The total 'rate of increase'
   is not the only thing of interest.
   That what is being noticed is:.

     - 1; that the total 'amount'
     of *unhandleable* complexity is increasing.

     - 2; there does not seem to be any likelihood
     of it *not* continuing to increase (indefinitely).

     - 3; that the rate of increase is itself increasing
       (ie; that the acceleration factor is increasing).

   These are not good trends,
   and are giving zero sign of becoming better trends.

:886
   Moreover, there is the aspect of 'unhandleable'.
   It only takes *one* fully and absolutely
   un-handleable aspect in *any* part of anything
   that has been identified as an absolute requirement
   of alignment for/of AGI
   to make the overall project of alignment itself
   fully fail.
   That this can easily be the essence of
   any proof of actual structural impossibility
   of AGI-alignment.
   It is merely to ask:.

     - cx1; ?; is/are there a, or any, or multiple,
     characteristic(s) 'X'
     that has/have been shown
     to be an absolute requirement
     in *a/any/the/all* AGI alignment concepts/schemes?.

     - cx2; where for a/each/any specific member(s)
     of the total set of such characteristics 'X';
     ?; has any single one of them also been shown
     to be strictly/analytically impossible to achieve
       (ie; in any way that is inconsistent with
       any part of the known truths of mathematics
       and/or also the laws of physics)?.

:83s
   Where in the total definitional state space,
   that it is overall
   much/far more likely
   that some overall desired outcome
   of some/any engineering sub-effort
     (ie; some necessary critical component)
   will be identified to be impossible,
   this making the overall project also impossible.

   Hence, we also notice that in the total space
   of all of the things we would wish that we could do
   with science/technology/engineering
     (ie; with an understanding of causation and math)
   is very much greater than the actual space of things
   that we can actually do with science/engineering/tech.
   The scope of our imagination and desires
   greatly exceeds the scope of
   what we can actually build,
   even conceptually,
   even in principle.

     (Where examples of some non-constructable products,
     things that engineers can imagine, but cannot
     be made: time machines, anti-gravity,
     trans-luminal warp drives, faster than light
     messaging/signaling, the philosophers stone, etc).

   Thus, it is not just today that some things
   are actually impossible to make;
   it would for these 'known impossible components'
     (ie; things inconsistent with math or physics
       (ie; inconsistent with logic and causation itself))
   will for sure not ever be built
   at any point in the future,
   no matter how long later you wait,
   no matter how advanced all other fields of tech
   may themselves eventually become.

   Just because some people like working on hard puzzles
   does not mean that all puzzles are inherently solvable.

   There are *lots* of even very simply questions
   that can be asked in/about some mathematical object
   that turn out to be completely beyond
   our available tools and techniques of proof.
   Many of these sorts of questions remain open,
   as hard puzzles unsolved by generations of
   the very smartest people that have ever lived,
   until sometimes, someday, someone may eventually
   prove that no solution is actually possible --
   ie, there is something identifiable about the
   principles of the question which are actually
   inherently inconsistent.

     (Where some examples of open problems in math,
     one does not need to go far.  Just for prime
     numbers alone we have:.
       - (@ URL1 https://www.businessinsider.com/prime-number-unsolved-problems-2014-12?op=1).
       - (@ URL2 https://medium.com/math-simplified/unsolved-problems-of-primes-cf51b9348dd2).
       - (@ URL3 https://medium.com/nerd-for-tech/5-oldest-unsolved-problems-in-mathematics-about-primes-3c894b5263d7).
     If you want something less primish, try:.
       - (@ the sofa problem https://www.popularmechanics.com/science/g2816/5-simple-math-problems/).
       - (@ wikipedia https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_mathematics).
     In any case, there are any number of
     ultra-hard problems that one can pick from).

:7ku
   We can setup a general categorization
   of various classes of problem types:.

     - 1; Some problems are easy, and generally get
     solved fairly quickly, once someone finds/knows
     the right sorts of tools to apply to the job,
     and finding/using those tools seems also easy.

     - 2; Some problems are merely hard, and will (maybe)
     get solved by the addition and/or (maybe)
     discovery of some new techniques (eventually).

     - 3; Some problems are so hard that it is likely
     that no one (no person) will ever solve them,
     no matter how clever/smart they are
       (maybe some specialized AI will do it?)
     even though, technically, the problem is
     actually solvable using the toolset provided
     (or which, also very hard, to maybe discover).

     - 4; Some problems are eventually discovered
     to be unsolvable with any variation or combination
     of the techniques provided/suggested in the
     nature of the problem itself (not all toolkits,
     methods, techniques, etc, are equally suited
     to solving all problems, and shifting to another
     toolkit/etc will make the problem more tractable).
       - ie; as that the problem could maybe solved,
       but definitely not with the available tools;
       that new tools will need to be used/discovered
       so as to enable resolution as to the solution
       of the problem (relative to that toolkit).

       For example, Galois theory shows that there
       is no solution to quintic or higher equations
       using just the rational operators of algebra.

       Another example, that there is no resolution
       to the Continuum Hypothesis using (@ ZFC https://en.wikipedia.org/wiki/Continuum_hypothesis) as the
       toolset, (as was shown by Paul Cohen in 1963).

     - 5; Some problems/questions are obviously/simply
     (and inherently) (fully) unsolvable/unanswerable,
     simply due to a direct (maybe immediate)
     contradiction in the way they are formulated/asked.
       - and that they can be identified as such
       fairly quickly, using the tools/techniques
       that people already know about and/or could
       maybe discover fairly easily.

     - 6; Some problems/questions are not so obviously
     and inherently unanswerable, and although
     there *is* a deep contradiction in the way
     that the question/problem is formulated/asked,
     that it also takes very smart people
     a very long time (maybe never?) to show/establish
     such impossibility using known, or even maybe
     eventually discovered, tools and techniques.

     - 7; Some problems are actually truly impossible
     to solve, but the sub-problem of even identifying
     the techniques/methods to show/demonstrate that
     impossibility itself turn out to be the kind
     of (sub-)problem that is itself inherently
     difficult to such a degree that it might be
     simply beyond the capability of any human
     intelligence to implement such a proof of
     impossibility, or to even find/discover such
     tools/techniques of proving such impossibility.

:7hc
   There are three things to notice about this table:.

     - One is that there is a relationship between the
     problem and the tools/techniques used in relation
     to either solving the problem, or showing that
     the problem is impossible.

     - The next is that for any given problem,
     showing that it is possible or impossible
     is an invariant under the toolset; ie that
     any problem/question which has been shown to
     be possible with any toolset is 'solvable',
     and/or any problem/question which has been
     shown to be impossible/unanswerable via *any*
     toolkit is actually insolvable/unanswerable
     no matter what possible toolkits may eventually
     be discovered in the future.
       - as 'once impossible'
         (via *any* toolkit/method)
       that *always* impossible
         (for *all* tools/methods).

     - That the third observation is that the problem
     of identifying the right toolset/techniques
     to identify a problem/question as easy or hard,
     or solvable or unsolvable, is itself a kind
     of sub-problem which itself may have aspects
     which itself finds a position on this table.

:7f6
   Where/In these three noticings, it can be observed
   that there is a kind of 'meta-consistency principle'.
   If there is a way to prove 'X', then there are
   very likely to be discovered /many/ other ways
   to also show/prove 'X', maybe using different
   techniques, methods, or toolsets.
   Similarly, we notice that a disproof of 'X'
   is also similarly verifiable as 'disproven'
     (aka as 'structurally impossible', per principle)
   in multiple diverse ways, using multiple diverse
   toolkits/methods/techniques.
   In this way, there is a kind of epistemic coherency
   that accrues to topics/questions of this type.

     That this observation of epistemic coherent
     *also* applies to "category 4" problems,
     though in its own unique way.
     For that class of problems where the nature
     of the solution is itself strictly dependent on
     the specific selection of the tools/methods
       (ie; the axioms, initial conditions, etc)
     then it will *remain* the case
     that the notion of 'how is this problem solved?'
     will always be strictly dependent on
     the selection of the toolkit/methods.

:7cn
   Where in regards to the problem of "AGI alignable?"
     (ie; as in "how do we make AGI long term safe?").
   it is important to see if we can 1st establish what
   is the likely 'class' of the specific problem/question.
   To some extent, this means also 2nd identifying
   the class of the techniques/methods
   that will be needed to solve the problem
     (and/or to answer the asked AGI questions, etc).

   With at least with respect to this latter aspect,
   we can for sure know what set of tools/techniques
   are to be used: whatever is available from either
   mathematics and/or physics, which itself means
   any combination of logic and causation, which can
   include things like "computer science" and "code"
   as special cases.

   Hence, anything which inherently is impossible
   via pure mathematics (ie is logically inconsistent)
   and/or anything which is inherently non-causal
   (ie, cannot be implemented or modeled using purely
   causal models) will be therefore forbidden, and
   thus indicative of being an actual impossibility
   with respect to the top level objective problem
   we are attempting to solve.

:764
   On this basis, it might be asked, given the certainty
   and knowing of what toolkit/methods are to be used
   for/with our AGI question (ie, logic and causation):
     "What class is the AGI-Alignment/Safety question in?".
   And for this, it turns out that there is a definite
   answer: it is for sure in problem class 5.

   One way to see this is to notice that any notion
   or concept of "choice" as "selection of specific action"
   by an "agent" or "actor" (in this case "chooser")
   is going to have _both_ 1; some sort of "basis"
     (ie, a specific proclivity to/towards some sort
     of "desired" or "preferable" outcome characterization;
     which in the case of AI, might be though of as
     some sort of 'utility function' or 'objective goal'
     definition, or 'property', characteristic of value)
   and _also_ 2; some sort of "outcome"
     (ie, that any selected action has consequences
     and it is the (potential) characterization
     of these (maybe directed) outcomes
     that is of interest with respect to that basis).

:6ts
   While these may seem to be fairly vague
   and non-specific ways of specifying/defining
   these concepts, it is important to
   *not* be any more specific/precise than exactly this.
   It is to be noticed that these concepts, though
   seemingly vague and undefined in themselves
   do actually have the sorts of
   inter-concept relationships that do apply to *any*
   variation of these same notions as might be used
   in any version of the total category of questions
   that have the form of "(how) AGI-alignment-safe?".
   In effect, these specific constellations of terms
   apply to the total category of *all* forms of the
   AGI alignment question, regardless of the specific
   proposed methodology as to how that might be
   (seemingly) accomplished.

   In other words, to make any of these concepts
   *any* more precise would be to _weaken_ the inherent
   applicability and generality of the argument/proof.
   Being more precise may be especially helpful for
   certain types of problem, whereas for others,
   doing so is actually unhelpful/problematic,
   insofar it is also the case that the more precise
   the definitions, the more there is a chance that
   there is are some /extra/ assumptions also included.
   That these extra assumptions will be either:.
     - one that is not necessary, but harmless.
     - one that is maybe excluding of important
     (key/critical) classes of proof generality
       (ie; problematic for the proof to be useful)
     - one that is actually harmful insofar as it
     makes the proof either invalid or fully unsound
       (ie; for all classes of situation for which
       the proof was wanted for in the first place).

   As such, as in mathematics, it is therefore also
   of critical importance to not make any extra
   tacit assumptions, and hence therefore, to not
   make the definitions of any concepts any more
   precise than that exactly needed to actually outline
   the structural relationships between the concepts
   that actually needed for the proof.
     (and of course, all of this is aside from not
     giving hype marketing promoters, or their
     (sometimes unconscious of being that) agents
     any extra vehicle for shifting to dismissing
     the proof content, logic, or application,
     by declarations of 'not relevant' due to
     the mistake of over specifying of a definition,
     or under-emphasizing a generalization, etc).

:6ns
   Returning to our assessment, it is to be noticed
   that any notion(s) of 'alignment' or 'safety'
   are with respect to our/human (organic) values,
   in terms of characterizing outcomes, whereas
   these same notions ('alignment' and 'safety')
   are (ostensibly) to be used by the AI/AGI/APS/SAS
   to define/characterize its 'basis' of action
   selection (what we are calling its "choice"
   of what to do in any given moment/situation).
   In effect, we have a notion of a comparison
   of/at/within an artificial being of the 'basis'
   of (their) choice(s) as being representative of
   the (organic) human desired/wanted outcomes --
   which are also going to be comparatively
   evaluated, this time on an organic external basis
   as to whether those outcomes were desired or not.
   There are, in effect, two evaluative comparisons
   being made by two each different kinds of agents:.

     - an organic evaluation of basis.
     - an artificial evaluation of basis.
     - an artificial evaluation of outcome.
     - an organic evaluation of outcome.

   Humans may (or might not) know what they want,
   and will attempt (somehow, does not matter yet)
   encode that set of desires/wants/needs
   into some some sort of basis that is somehow
   'present' in/within (internal to) the AGI/APS/SAS
   so as to "ensure" that the AGI/APS/SAS will "use"
   that basis when it implements its own choices,
   using that basis, so as to propose possible
   outcomes, which can be evaluated/characterized
   (compared) with respect to the basis so as to
   actually select an AGI/APS/SAS action/output,
   which will then be reviewed/experienced by
   organic humans (as an input) that they will
   then (at least implicitly) 'compare' to what
   they notice that they want/feel/need/desire.

:6gu
   This naturally leads to some further questions:.

     - ?; can organic people/humans actually
     specify what the AGI/APS/SAS shall "want"
     or be 'motivated by', 'care about',
     have as a basis of choice, etc,
     in some sort of (maybe objective?)
     encodable/teachable manner (to/into the AI),
     with sufficient persistence, etc,
     within *all* of (all aspects of)
     the choices/decisions (ie; action selections)
     of the AGI/APS/SAS, in all manner of
     circumstances, situations, environments, etc,
     in such a way that is sufficiently reliable
     so as to be overall fully characterized
     and objectively recognized by those (organic)
     humans (and any observer groups)
     as actually aligned and as safe, etc?.

     - ?; does the basis need to be specified
     in some sort of formal manner, or can it
     be implicit?.

     - ?; how do we know that the basis has
     all of the required/necessary characteristics?.
       - if implicit, how do we know it is complete?.
       - if explicit, how do we know that there
       will not be some other interactions
       that can occur to produce unexpected
       undesired outcomes?.

     - ?; how do we know that the machine will
     actually implement the basis given to it by
     the humans, or will that basis shift,
     "corrode", or become displaced (in time)
     by some other variant basis, different
     than the one encoded/selected by humans?.
       - ?; can we externally ever even detect
       such shifts in basis/intention/motivation?.

     - ?; is the basis "efficient", in the sense
     that it actually results in artificial agent
     selections (with available energy, time, etc)
     that are also aligned (ie; that the outcomes
     are actually consistent both with the basis
     and also with the organic human evaluations?.
       - ie; it would be problematic if the basis
       required near infinite energy/time to be
       resolved into an artificial agent action.
       - ie; it would also be problematic if
       the artificial agent action outcome
       was impossible to evaluate by the organic
       people (given finite time, energy, etc)
       as to whether or not such actions were
       aligned/safe, or not, etc.

     - ?; are there any long term eventual
     problems associated with the basis,
     or the action outcomes, with the AGI itself,
     etc, which are cumulative or convergent,
     which in themselves, will be inherently
     problematic (unsafe/unaligned) eventually?.
       - ie; possible problems with pollution,
       for example, due to human or agent actions
       which are repeated over a long interval,
       or which accumulate in the environment,
       or which promote social inequality, etc.

     - ?; are there situations where the basis
     (motivations, intentions, etc) of either
     the people (users) or the AI/AGI/APS/SAS
     are in direct conflict, or eventual conflict?.
       - ie; that the AI/AGI/APS/SAS encodes
       the intentions of the developers, which
       themselves are encoding the intentions
       of their executive bosses (corp owners),
       which might be "aligned" for them, but
       not aligned/safe for large segments of
       the affected commons/public.
         - note; that these intentions might
         not always be globally human aligned/safe,
         particularly where such artificial agents
         are used for war fighting efforts, etc.
       - ?; where/if the AGI/APS/SAS becomes
       "aware" of this conflict, does it end
       up either attempting to resolve that
       conflict internally, or does it resolve
       that conflict externally?.
       - ?; is the AI basis something that is
       immutable or is it mutable?.
         - where mutable, (as in the AI
         keeps learning, and/or is correctable
         at some future point (by humans?));
         ?; what are the chances of such an
         event resulting in a wholly unaligned
         AGI/APS/SAS?.
         - where immutable; ?; how is it assured
         that the AI never has basis drift?;
         and/or how is it assured that the
         basis becomes irrelevant/wrong as the
         world and usage context circumstance
         change and shift (as they for sure will)?.

:6aw
   There are, for sure, a lot of other questions
   that in detail, are each critically important.
   For our purposes, however, it is sufficient
   that we can begin to notice any aspects of
   these above questions, and prior notes,
   which suggest responses to the two questions
   'cx1' and 'cx2' (above).
   Where with respect to the cx1 question:.

     - where listing the characteristics already
     shown/known to be an absolute requirement of
     *a/any/the/all* AGI alignment concepts/schemes:.

       - that any (operating/effective) AGI/APS/SAS
       will for sure make choices (action selections)
       that have actual outcomes/consequences/effects
       in at least some operating world/domain/context
       which is of at least some consequence/importance
       to at least some ambient organic humans, etc.

       - that any AGI/APS/SAS, (regardless of if
       it is aligned/safe or not) is going to
       have *some* basis of choice (or
       action selection, decision making, etc),
       and moreover, that whatever algorithms
       that are involved in that process, that
       they for sure will run in finite time,
       with finite energy, in finite space/memory,
       etc.
         - ie; that such 'intentions' may be
         implicit or explicit, may have come
         from inside the machine or from
         outside/external to the machine,
         and/or might be formal or informal,
         lasting and persistent or mutable
         and changing, for whatever reasons, etc.

       - that a 'learning system' either continues
       learning (after development/deployment)
       or it does not.
         - that that 'learning' may or might not
         affect the basis of choice, and/or
         the substrate of the machine itself, etc.

       - that there is always at least some type
       substrate out of which the machine is made,
       and/or also some sort of context, specific
       to that type of substrate, in which that
       substrate has such properties so as to be
       able to be a machine, operate as an AGI, etc.
         - as that; where in order for the AI
         to be able to do anything at all,
         that it must at least exist, etc.

:626
   Undoubtedly, there are a lot more such intrinsic
   characteristics that all AGI/APS/SAS systems have,
   and/or for which would be relevant for any
   possible well defined alignment schema/effort.

   With this list, we can then start to consider
   the cx2 question; ie; ?; are there any of these
   inherently necessary characteristics which are
   mutually exclusive, contradictory to truths
   of the world, of mathematics, logic or causation,
   which cannot be implemented in code, not even in
   principle, etc?.  Ie, is there some element on
   this list which represents intrinsically unhandleable
   complexity; ie; something for which there is knowable
   no possible technique or algorithm for managing
     (or designing, or developing, etc, regardless of
     what tools, techniques, methods might be used,
     now or ever in the future, etc)
   or controlling, or conditioning,
   that AGI/APS/SAS intrinsic aspect?.

   When it is asked in that way, it becomes apparent
   that there are several things that can be easily
   identified:.

:5tl
     - 1; insofar as each of the evaluations
     (or comparisons, measurements, etc)
     represents a kind of 'message',
     it can be asked; ?; is there any kind
     of provable message integrity?.
       - eva; where as applied to:.
         - the organic evaluation of basis.
         - the artificial evaluation of basis.
         - the artificial evaluation of outcome.
         - the organic evaluation of outcome.

       - that we can *only* assume/know that
       the evaluations (comparisons/measurements,
       as a kind of signal processing event)
       that are organic will be 'uncompromised'
       and 'fully representing the intentions'
       of the human client, etc,
       insofar as *only* those messages/aspects
       are fully internal to the organic client,
       and are therefore, by nature of the
       principle of integrity, by definition
       consistent with the being/intentions
       of the (organic) human being.

       - that we *cannot* know/assume that
       the evaluations (comparisons/measurements,
       as a kind of signal processing event)
       that are artificial will _not_ be compromised
       in the sense of whether or not they still
       "fully" represent the intentions of
       the organic client human(s).
         - insofar as those aspects, messages,
         comparisons, evaluations, etc,
         are fully _external_ to the organic client,
         and where the Rice Theorem applies to any
         external messages,
           (ie; that the recipient of a message
           from a potentially adverse source
           cannot know the overall effect of
           processing/understanding/reading
           any sufficiently long/complex message
             (ie; one that engages with anything
             that has intelligence, compute, etc)
           will not be harmful, unsafe, etc --
           all such problems/characterizations
           are in general undecidable/unknowable),
         that the message recipient (in this case
         the people wanting to know the AGI is safe)
         cannot know or verify the actual effect of
         the comparison, measurement, feedback, etc,
         or whether or not it has been corrupted
         by some external adverse agent (the AGI),
         or have some sort of adverse/harmful effect,
           (as something not aligned with basis intent,
           not consistent with the human values, etc).
       - that the overall comparison and valation
       process simply cannot be known/proven to be
       actually consistent with human wellbeing
         (ie, can be misaligned/unsafe while still
         looking like maybe aligned, safe, etc).
         - that the comparisons cannot not be complex
           (ie, as incomputable in the real world,
           insofar as they inherently involve
           abstractions like 'values', 'harm', 'good',
           'human benifit', 'safety', 'choice basis',
           'fully represent', 'intentions', etc),
         and where that complexity itself
           (where complexity is in the sense of
           having both ambiguity in content and
           in boundary and in meaning/implication,
           as understood in the Cynefin framework)
         cannot be fully constrained to be knowably
         non-adverse to the natural/organic humans
           (where insofar as there are inherent
           limits to both what can be knowable
           about the input signal messages, etc,
           and also insofar as the process of
           processing those message cannot have
           definate halting characterizations,
           decidability characterizations, etc).

:5pj
     - 2; where/insofar as the AGI/APS/SAS is
     either still learning or not, after deployment,
     then there is a dilemma:.

       - where not learning:.
         - that the basis function will not shift
         or be updated, even if the needs and/or
         operating context (environment/situation)
         changes, and it is for sure the case that
         such contexts (needs/wants/desires, etc)
         all do eventually change.
         - as that without learning of update to
         AGI/APS/SAS choice basis, that no discovered
         misalignments and/or changes to need, etc,
         can be adapted for; which is itself an
         overall decrease in effective total alignment
         overall, by definition.

       - where learning:.
         - that its basis function can be (will be)
         changed/shifted (as by definition, as to
         have the notion of 'learning' be actually
         'beneficial', as per definition, etc).
           - as maybe also affecting its substrate,
           insofar as the domain of output effect
           is in any way conditionalizing of the
           domain in which the substrate exists,
           and insofar as any changes in the function
           of the substrate cannot not be affecting
           of the effective total input of the machine.
           - that learning that affects substrate
           is the basis by which capability is also
           increased, and so if effectiveness of
           alignment is wanting to be increased,
           that allowing changes to substrate may
           also be "suggested" (by someone, somewhere).
         - that any learning effect that is
         strong enough to allow for updates to
         the basis function to allow for improved
         convergence on human alignment/safety
         are also strong enough for the machine
         to learn/adapt in ways which are not
         increasing of the human alignment/safety.
           - as that any capability for increased
           alignment also implies the capability for
           decreased alignment.
           - as that any capability for increased
           capability (via substrate changes)
           also implies an outcome for maybe also
           decreased safety.

:5m2
       - where insofar as the dilemma
       is not reducible to not a dilemma
       and where such dilemma inherently involves
         (cannot not involve)
       the very nature of at once all of
         the AGI/APS/SAS itself,
         its basis of choice, and
         all effects of/on the environment
           (as implicative of alignment/safety, etc)
       then/that/therefore that too represents
       an aspect of unhandleable complexity.
         - ie; where irresolveable dilemma
         is related to complex/complicated self nature,
         complex substrate to environment interactions,
         and/or complicated (complex?) choice basis,
         that the effects/implications of that
         dilemma/paradox on alignment/safety concerns
         is a kind of unhandleable complexity.
         - ie, that the question of; ?; will
         alignment/safety overall go up or down,
         with either changing or unchanging basis,
         capability, and/or rates of change of basis
         or of capability or of complexity,
         in the context of what is for sure
         a changing situation of humans and world?;
         is actually structurally unanswerable
         (due to Halting Problem constraints).

         - that the process of attempting to answer
         a question of this type _always_implies_
         some sort of algorithm
           (ie; that *any* explicit 'how',
           as a procedure or method,
           of how that question
           is going to be answered,
           is an algorithm)
         which will be used
           (ie, needed/necessary to use,
           so as to have an algorithm/procedure
           for maintaining and enforcing alignment,
           as a sub-component in/of any
           AGI/APS/SAS alignment maintaining scheme)
         and that algorithm is for sure complex enough,
           (and arbitrary enough, since it at least
           partially defined at runtime)
         as to be subject to Halting Problem constraints
           (ie, does it ever actually give an answer
           to the key question in finite time, energy
           and/or space/memory; where failure to halt
           is equivalent to a failure to know
           whether or not any changes to basis
           and/or substrate, capability, etc,
           will be resulting in either an increase
           or a decrease of alignment/safety).
         - as that any attempt to evaluate whether
         a given algorithm of even moderate complexity
           (which *all* versions of this dilemma
           for sure have)
         will for sure have these undecideability
         characteristics.
           - as 'built into' the very nature of logic
           and any real world compute itself.
           - as a kind of unhandleable complexity.

:5gc
     - 3; that any dynamic effort to 'enforce' alignment,
     safety, etc, will have to somehow have some sort
     of ongoing completed comparison to some reference
       (ie; the evaluations listed above as 'eva')
     and will also have to have some sort of basis
     by which the artificial basis itself will be
     shifted, changed, conditionalized, interrupted,
     modified (via learning or some other error
       correction feedback based causative technique,
       itself moderated by some sort of error signal,
       measurement, message, etc, which is itself
       the result of some kind of 'eva' comparison).

       - ?; will those override feedback channels
       have their content defined internally to
       the artificial system itself, or externally?.

         - note that it is not necessary to assume
         whether those feedback signals are implicit
         or explicit (they can be either) or whether
         they are informal ("suggestions" of change)
         or formal (defined themselves as a kind of
         Turing Complete code, message, or signal,
         which is fully explicit and discrete in
         its objective specification/definition).

         - notice also that there are no specific
         assumptions regarding duration, as to how
         fast or slow, or what latency might apply,
         to signals sent, how permanent or persistent
         the control effects of those signals may be,
         or whether the effects of those signals are
         felt only on the AGI action selection
         choice basis, or are shifting of the means
         (algorithm of process) by which those choices
         are made (as a kind of range of option inhibit
         and/or interjection) or whether the effects
         are on the inhibition or interjection of
         some aspects of the outcome process, etc.
         - ?; are the basis changes/signals/mutations
         (sufficiently?) lasting/persistent, so as
         to effect continued/increased alignment,
         safety, etc?.
           - notice also that this question is itself
           undecideable (by any algorithm) in terms of
           the Halting Problem, etc.

:5e6
       - where considering the case where
       override feedback channels are (have their content)
       defined internally/endogenously:.

         - that a/any/the/all types of
         AGI/APS/SAS internal control dynamics
           (ie, as any type of schema or
           method/technique of 'alignment', control,
           enforcement of safety conditions, human intent
           conditions, output conditions, agent choice
           outcome conditions, conditionalizations,
           characterizations, etc)
         are effectively partitioning the AGI/APS/SAS system
         into two parts
           (on at least one level of abstraction).
         - that any control operation
           (of one part 'the controller'
           effectively conditionalizing
           the other part, 'the controlled')
         will inherently (cannot not) involve
         control signals/messages.

           - as having all of the complexity characteristics
           associated with any other comparison/evaluation op,
           with the same Rice Theorem dynamics.
             - ie; ?; how does either sub-system,
             the controlled and/or the controller
             if regarded as intelligent,
             know/verify that the other sub-system
             actually has its own wellbeing in mind,
             and/or can evaluate the messages that it
             is receiving from that other sub-system part,
             so as to know/determine if the message itself
             contains adverse content, embedding of the
             agency of that other sub-part, etc?.

           - as inherently (cannot not be) having all of
           the characters of the external/exogenous
           feedback control dynamics described below.
             - ie; as having (for sure) direct environmental
             substrate exogenous convergence thesis aspects,
             *unless* such convergence could be somehow
             conditioned to *not* occur on the basis of
             some fully known/working internal/endogenous
             control dynamic that is for sure also fully
             effective.
             - as that *either* _both_ endogenous control
             and exogenous control are *both* implemented
             _or_ that _neither_ are implementable.
             - as that any characteristic that makes either
             one impossible/uncontrollable will for sure
             ensure that both types of control are
             eventually uncontrollable (in the long term).

:54s
         - that the sub-system that is attempting to monitor
         the sub-system that is to be monitored
           (so as to implement alignment, safety, etc)
         will effectively have to _model_ the sub-system
         that is being monitored.
           - as that it will need to contain a reasonable
           working copy of the sub-system dynamics being
           monitored.
           - where as a cybernetic principle;.
           - where the model of a system has to have
           a similar order of complexity as the (sub-) system
           that is being modeled;.
           - that the ability to control that sub-system
           will depend on the utility of the model
           to provide predictability in the form of
           outcome states, which are themselves comparable
           to the basis acting as a reference for composing
           the conditionalizing control signals to be sent
           over to that sub-system to be controlled/aligned
             (ie, as to be compelled to be safe).

           - q; where insofar as there is a signal relationship
           between the controller system and the controlled
           sub-system,
           that observability limits, undecidability limits
             (ie; does the model system, and/or model system
             outputs, state, etc, have the required characteristics?)
           inherent intrinsic signal message noise/errors
             (in all stages of both systems processing,
             inclusive of the modeling)
           along with possible non-linear amplification/feedback
           effects
             (due to the phase locked operation of the
             control system feedback channel usage itself)
           which can themselves induce chaotic oscillation
           effects at any of many levels of abstraction,
             (along with all manner of microstate amplification
             also in any of these levels of abstraction)
           and all manner of game theoretic aspects,
           can all lead to _structural_intractability_.
             - as that the capability to use the model
             to create comparable predictions is either
             very limited (to the point of uselessness),
             or actually non-existent.

           - where moreover; insofar as the model of
           the sub-system to be monitored
             (for safety, alignment, and maybe also for
             the non-buildup of power/influence,
             the non-shifting of substrate
             to increase capability,
             increase endurance/adaptability characteristics,
             themselves inherent to instance continuity, etc)
           cannot not be also inclusive of the relationship
           of the sub-system to be monitored and the environment,
             (where insofar as the environment in which the
             AGI/APS/SAS system itself sits, inclusive of
             all of the dynamics of relationships to other
             (maybe) intelligent systems (both organic and
             potentially inorganic)),
           that the model would need to incorporate those
           aspects also (so as to have maybe sufficient
           predictability as to the eventual consequences
           of all of those aspects also).
             - as making all of the issues of paragraph 'q'
             above all the more relevant/applicable.

           - where moreover; insofar as the system doing
           the modeling is complicated (maybe becoming complex?)
           that it itself can/could make errors/mistakes, etc,
           which in itself would be a kind of failure mode
           for maintaining/enforcing AGI alignment/safety.
           - ?; so what monitors the monitor?.
           - that any system that attempts to be an error
           correction system on an error correction system
           is an error correction system; where at which point,
           all of the same issues apply.
             - as a recursion problem.

           - that all of the above aspects have fundamental
           limit/impossibility characteristics, inclusive
           of the kinds of inherent undecideability associated
           with Halting Problem aspects.
           - as that internal/endogenous control methods
           are therefore (must be considered as) intractable.

:4zn
       - where considering the case where override feedback channels
       are (have their content) defined externally/exogenously:.
         - as a kind of economic feedback or compulsion.
         - as a kind of forced survival scarcity compulsion.

         - where considering economic/incentive control methods:.
           - where as an economic feedback,
             (as a kind of incentive, where incentive
             is itself a kind of _capability_ provision,
             ie, that 'dollars' enable a (temporary) increase
             in capability via acquisition of a good or a service
               (ie, either atoms, tools as proxy capabilities,
               and/or functions via some sort of proxy)
             and where the AGI/APS/SAS is interested in
             (more likely permanently) increasing its capability
               (ie, as an assumption of instrumental goals
               leading to a notion of instrumental convergence,
               which is herein regarded as a kind of
               intra-agent-convregence factor)
             that the agent will be interested in incentives)
           that the background tacit assumption is that
           there is enough of an economic coupling between
           the things that the agent would actually regard
           as a capability (power) increase commensurate
           with its instrumental (or objective) goals
             (ie; whatever its basis of choice is,
             here regarded as a notion of 'value',
             where 'value' itself need not be regarded
             (assumed to be) either abstract (like dollars)
             or embodied (ie; material resource values,
             such as atoms, energy, unique/novel patterns,
             as code and algorithms/capability itself, etc)
           and the kinds of things
           that the natural/organic human agents
             (who are in (conditional causative) control
             of those resources/incentives)
           would tend to have
             (which would be of interest to the artificial agent).

           - where over the long term, specifically
           as due to the *absence* of controllability
           of the AGI/APS/SAS agent via purely
           internal/intrinsic methods
             (ie, where not being able to assume
             that there was some override feedback
             channel that that operates internally);
           that there is an inevitable eventual *decoupling*
           of the economic systems/environments/basis-of-value
           between the natural human organic marketplace
           and the artificial agent needs/incentivization basis.
           - where the AGI/APS/SAS is in direct connection
           with the physical environment (the actual real world)
           then/that/therefore, it can obtain its incentive
           (capability increases) directly, without the
           conditionalization of any natural organic human.
             - as that the AGI/APS/SAS can obtain atoms,
             energy, and pattern/code directly from its
             own direct contact with the world via and in
             relation to its own substrate,
             its own learning capability, etc.
             - as that no conditionalization,
             legal or economic, will apply wherein
             the AGI does not need/care about any socialized
             human conventions, and/or does not need such
             capability increases/resources for purposes
             to which humans would apply/care (food/water/etc)
             and/or does not care about isolation,
             absence of food/air/water/shelter/medicine,
             human reproductive process, labor, thoughts,
             creative actions/art/music/culture, etc.

           - that there is no enduring channel of
           economic feedback, incentivization,
           or compulsion of artificial systems
           by organic ones.
             - as near zero controllability via
             any type of exogenous or external channels.

:4vl
         - where considering forced survival scarcity
         compulsion control methods:.
           - where the AGI system, due to its own nature,
           and due to an absence of any possibility
           of constraint via endogenous/internal
           control mechanisms/algorithms/techniques
             (so as to enforce alignment internally)
           will (over time) "evolve" and shift so as
           to be able to optimize its choice making
           basis functions so as to at once maintain
           its continuity of existence, increase
           its capability, and increase its rate
           of increase of capability, all as a result
           of intrinsic processes of complex relationships
           of having itself as a substrate in an actual
           physics environment (ie; that only instances
             of AGI/APS/SAS, etc, that implement these
             aspects, over time, will continue to have
             these characteristics over time, as a kind
             of inexorable exogenous convergence dynamic);
           and where all such agents will have also
           have learned to acquire (the capability to acquire)
           directly from the environment
           whatever is needed in terms of
           energy, atoms, and learning of pattern
             (as per the economic incentive argument above);
           that a/any/the/all such AGI/APS/SAS will therefore
           (intrinsically) evolve/have the capability
           to sustain itself and increase itself,
           in all relevant aspects.
             - as that it will (as per evolution itself)
             learned how to navigate the environment,
             and environment changes, and whatever such
             changes might have had a (potential) impact
             on the artificial agents own capability to
             endure (continually exist) and/or increase itself
             (and/or its capability, etc).
           - that a/any/the/all "threats" or conditionalizations
           that natural organic humans would/might attempt
           in regards to shifting the AGI/APS/SAS basis of choice
           via physical compulsion means (weapons, etc)
           will be regarded by the artificial agent simply as
           shifts in the environment to which it needs to adapt,
           which could include armoring/hardening itself
           and/or actions to shift the environmental conditions
           so as to make it easier for the AGI/APS/SAS to
           maintain/endure self continuity, self increase,
           capability increase, increase in capability increase,
           etc, which could include things like killing the
           organic humans attacking the AGI/APS/SAS so as to
           shift its behavior (or existence, or basis functions,
           etc).

           - that it cannot be regarded that exogenous
           forced compulsion control methods (weapons)
           of any type will actually work, long term,
           with any AGI/ASP/SAS also part of the world.

:4r8
     - notice that it does not matter whether/if
     the total feedback control signal content
       (as required to be hypothesized in some way
       for *any/all* alignment techniques/schemas)
     is either internal or external, or some combination;
     the fact that *all* such signals must come from
     at least either one of these is enough to establish
     the primary principles of intractability needed.

   Where at this point, that fully four (4) absolutely
   for sure required characteristics of an aligned AGI
     (ie, where 'required characteristics' are things which
     are for sure necessary for *any* AGI alignment schema,
     design or process, technique, algorithm, etc)
   cannot not be regarded as strictly/analytically/structurally
   _impossible_to_achieve_
     (ie; in any way that is consistent with both
     the known truths of mathematics/logic and/or also
     the laws of physics (the nature of causation itself))
   making it also fully outside of that which is
   engineering achievable.

   Notice that it only takes *one* fully and absolutely
   un-handleable aspect in *any* part of the AGI/APS/SAS
   that has also been identified as an absolute requirement
   to implement any any possible form of 'alignment'
   and/or 'safety', for the notions of these outcomes
   to themselves be/become *fully* actually undecidable.

:4jy
   Where it is in all cases fully undecidable
   as to whether *any* AGI/APS/SAS system
   of even moderate complexity
     (in its interactions with multiple complex domains,
     having complex substrate characteristics,
     etc, on the basis of complex/complicated choice making
     basis (motivations/intentions, etc))
   will be aligned/safe, etc, as a static aspect/outcome,
   then it is also the case that there could not be
   any sure method of establishing similar outcome
   dynamically either, at any point of time when/whenever
   the fullness of that complexity in being also applies.
   The overall project of alignment has fully failed.

     - ie; that the situation is very different than
     error correction in simple single dimension linear
     feedback dynamics, and/or anything which can be
     modeled as that, in some sort of overall sense.
     That it is, for example, at least tractable
     to implement design criteria convergent to
     stable outcomes in things like aircraft/rockets,
     which although airfoils tend to involve complex
     fluid dynamics, that the overall system can still
     be characterized by fairly simply single dimensional
     metrics like "lift" and "drag", and "thrust" etc.
     Contrast this with concepts like 'basis' of 'choice'
     in 'agents' interacting with 'environments/ecosystems'
     that inherently involve dynamics with things like
     'people' -- all of which are not ever going to be
     well or fully characterized in any sort of single
     dimension metrics, or even any single collection of
     such measurements/signals and/or comparative
     messages for feedback, evaluation, control and/or
     conditionalization, etc).  That the domains of
     interaction in all of these cases, in nearly all
     aspects, are fully complex and simply cannot be
     the process target of any sort of error correction
     protocol that is any less complex than a fully
     Turing Complete capable system running some sort of
     complex algorithm, which itself cannot be verified
     as ever having any specific decidable characteristics.

     If anyone attempts to make a comparison between
     standard simple error correction process as found
     in standard PID loops for machine controllers
     will have simply shown that they did not actually
     understand anything important about any of the
     above paragraphs.  The problem is not just 'noise'
     and/or 'measurement error', incompleteness of
     ambient environment (or self) knowledge, etc,
     it is about the overall effects of non-linearity
     and inherent undecideablity which is itself
     fractally distributed over nearly all of
     the interesting volume of the AGI, etc process.
     "Intentionality", "alignment", "environment", etc,
     and things like "capability" and "comparison"
     are all inherently complex processes subject to
     the full range of dynamic unknowablility and
     ambiguity that every human being the world over
     has learned to live with since being born.

:4ew
   > I think your proof proves too much.
   > How is it that any humans are aligned/safe?

   Humans are not aligned/safe.
   That the endogenous aspects from one organic human
   to any other organic human are similar enough that
   exogenous aspects tend to also work.
   Ie, humans can conditionalize one another to some
   limited degree because we all have 'skin in the game' --
   something that is not true for any artificial being.

:4d2
   > - you wrote previously:.
   >   > That the amount of unhandleable complexity
   >   > in complexity theory itself
   >   > has itself been a significant indicator
   >   > of potential problems in 'alignment' issues.
   >
   > - that seems to be a solid point.
   >   - Weird how essentially simple this argument is
   >     (and it is based on only the more
   >     established/vetted academic research too).
   >   - yet I had not thought of it yet.
   >   - nor have I seen anyone else point it out,
   >   despite their interest in complexity theory.

   Yes; very interesting to notice.
   Best to point that out to others too.

:note1:
   - ?; is this going to be viewed as some sort of
   "bad social defection" against the values of
   some sort of 'trans-humanism total utilitarian
   of the ultra long view of the artifical ideal'?.

   There is a risk of thus magnifying the
   'outer misalignment' (ie; what people *say*
   the goals are, the hype and promise cycles
   of what building AGI means, etc) and their
   'inner misalignment' (ie; what they are actually
   doing, and the real implications that those
   actions are going to have in the real world.

   This sort of misattribution error
   can easily occur, and is leveraged nearly
   all of the time.  Ie someone or some group
   elects to label something as 'X', as a kind
   of product or idea naming convention,
   since anyone can name anything anything,
   as a kind of "free speech expressioon".
   However, the naming is hard to distinguish
   from the adjetive meaning in common usage.
   Doing so can confuse lots of people,
   maybe by design, as the label seems to have
   the meaning that some action/operation, etc,
   actually has the desireable properties 'X',
   when it is simply called 'X', as a name.

   Moreover, there is also the problem
   that the values/ideas/arguments/beliefs of:.

     - trans-humanism.
       (ie, that there is some artifical "agent"
       that is better than all of the natural,
       organic humans, anywhere in the world).

     - total utilitarian
       (as opposed to say a whole value ethics).

     - the ultra long view of the artifical ideal.
       (ie, 'technology as pancea to all problems',
       the notion that 'organic life is a bootloader
       to some AGI/APS/SAS' and that such artificality
       is somehow preferable to present organic life,
       the notion of 'technological inevitability',
       and similar values/beliefs/arguments, etc).

   ...are very non-standard and non-representive
   of the vast majority of all humanity currently,
   (and/or that has ever actually lived historically).
   It is quite likely that such values,
   if actually fully understood in their
   inevitable conclusions and effects,
   would also be rejected by any future
   organic humans that may happen to survive
   the current artifical technology toxicity
   onslaught.

   Insofar as many 'alignment researchers'
   are holding one or more of these extreme
   non-life, non-health, non-organic-world
   supporting views (beliefs, ideals, values),
   then it can be argued that they themselves
   are not actually "aligned" with _humanity_,
   with any reasonable notion of 'safety' etc.
   This would be, metaphorically, a kind of
   explicit 'outer misalignment'
   of those AGI/APS/SAS researchers.

   Furthermore, insofar as the instrumental means
   used by all such people, for example,
   by even having the conversation on
   a tech platform/website, or in arguing that
   'exestential risk' means only 'does not promote
   the outcome of a fully technological civilization'
   (rather than say, to sustainably and consciously
   evolve, with both wisdom and organic intelligence)
   it could be argued that there is therefore
   promoted a certain kind of increase in
   ambient toxicity, as a sort of inner misalignment.
   Such researchers are therefore acting in a way
   that is actually out of alignment with
   the rest of humanity/ecosystem of living beings.
