<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Basic Meta Tags -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- SEO Meta Tags -->
  <meta name="description" content="Comprehensive AGI Risk Analysis">
  <meta name="keywords" content="agi, risk, convergence">
  <meta name="author" content="Forrest Landry">
  <meta name="robots" content="index, follow">

  <!-- Favicon -->
  <link rel="icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">
  <link rel="shortcut icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">

  <!-- Page Title (displayed on the browser tab) -->
  <title>Comprehensive AGI Risk Analysis</title>
</head>
<body>
   <p>
   TITL:
      <b>Super-ordinate Claims</b>
      <b>By Forrest Landry</b>
      <b>October 12, 2022</b>.
   </p>
   <p>
   ABST:
      Some remarks on our claims to be '100% certain',
      in context, regarding the total actual absence
      of any possible safety/utility (of any type of)
      of AGI/APS/superintelligence.
   </p>
   <p>
   TEXT:
   </p>
   <p>
      Where based on the arguments given;
      Where/If AGI architecture is built;
      then/that it is 100% certain
      to cause run-away lethal and terminal effects
      unsolvable by any possible technical scheme,
      over the long term.
   </p>
   <p>
   :ftl
      > "100%" is not a probability.
      > Saying "100%" anything at all
      > will make you sound/seem
      > ludicrously overconfident.
   </p>
   <p>
      We do understand and agree
      that '100%' sounds overconfident.
      However, regardless of how it
      "sounds" or "seems" publicly,
      it would <b>not</b> be appropriate
      to change the claim, or soften it,
      just so that the 'rhetoric spin'
      is more 'socially acceptable'.
   </p>
   <p>
      We accept that people seeing this
      will initially have
      less favorable first impressions
      of the quality of our thinking.
      They will judge us unreasonable,
      and sanction us as "unworthy",
      just on the way the claim is stated.
      It does not match their heuristics
      of what constitutes 'preferable reason'.
   </p>
   <p>
      Nonetheless, the topic is important,
      and we do still therefore have some hope
      that at least <b>some</b> interested people
      will go on to inspect the actual arguments.
      At that point, the "judgement" has moved
      into "evaluation" and the personal/social
      has moved into the technical/informational --
      as something impersonal, unbiased by rhetoric.
   </p>
   <p>
      Maybe our hope to have reasonable conversations
      with what are overtly judgemental people
      is inherently unreasonable in itself?
      However, from our perspective,
      the whole situation of the x-risk is in itself
      a result of unreasonableness --
      and there is not much we can do about that.
   </p>
   <p>
      Perhaps our remarks do seem overconfident.
      Yet, almost all AI x-safety researchers
      that we have talked with, so far at least,
      all appear to assume the same false a-priori:.
   </p>
   <p>
        Where for maintaining long-term functional alignment;
        "that it is possible
        for all prerequisite possibilities
          (ie; all crucial and by-default-deficient
          technical desiderata)
        to simultaneously exist".
   </p>
   <p>
      We are attempting to describe how certain propositions,
      as tacitly assumed by current safety researchers,
      <b>will</b> inherently contradict each other.
      This is a kind of proof by contradiction,
      of the impossibility of long term safe AGI.
   </p>
   <p>
      Where long term AGI/superintelligence dynamics
      are modeled with empirically sound premises,
      we find that there is always an inconsistency;
      that one set of hypothetically possible conditions
      cannot exist alongside one or more other conditions,
      that are also for sure required to also be present.
      It corresponds to how the 'system' is modeled --
      for a very wide range of model system types,
      that AGI cannot actually be safe in practice,
      in the long term, no matter how you construct it.
   </p>
   <p>
      What we are setting up is not an unusual proof form.
      There are other examples of the use of the method
      of proving internal contradictions
      in the computability of (distributed) algorithms:.
   </p>
   <p>
         - Godel's incompleteness theorems.
         - CAP theorem.
         - Byzantine Faults (to a more limited extent).
   </p>
   <p>
   :fvg
      > Like, really?
      > You are absolutely certain
      > that there is no possible future observations/arguments
      > which would make you even slightly less confident
      > of the assertions you are making here?
   </p>
   <p>
      Are you asking if it is possible that,
      in some previously unobserved, unsuspected,
      strange and exotic outer realm,
      where the conditions are such that, actually,
      the premises of the arguments do not apply?.
   </p>
   <p>
      Of course, and we can even help you think of one:
      any world where the level of artificiality
      was already 100% total, these types of argument
      would not apply
      (ie; there would be no "humans",
      and no prior form of carbon based life to protect).
   </p>
   <p>
      However, even in that world,
      the truths of math
      will still be that.
   </p>
   <p>
      As such, we simply do not see
      that the general form of
      attaining technical alignment
      via any means, will <b>ever</b> be possible,
      in any universe, no matter how constructed.
   </p>
   <p>
   :fx2
      > Your proof could be wrong.
      > To think what you are thinking is insane.
   </p>
   <p>
      As in, "is it possible"
      that we are 'just dreaming',
      or 'having false memories',
      of having gone through the argumentation
      before reaching the "this is impossible" conclusion?.
   </p>
   <p>
      Ie, maybe we are somehow deluded, insane,
      and completely unable to make any sense,
      or to see/know sense, reason, or logic,
      and that therefore, any sense of "conclusion"
      that we have arrived at, is inherently faulty?
   </p>
   <p>
      Perhaps,
      but we do not think those sorts of scenarios
      are helpful to account for
      our current levels of confidence,
      as indicated in the probability assessment.
   </p>
   <p>
      Rather, what we would more naturally expect
      is for people to look at the arguments themselves
      regardless of whatever their bias is,
      whatever their optimism or pessimism is,
      and consider the logic just in itself.
      As in, do not take our word for it --
      examine the logic and evidence as given,
      and see if it makes sense to you,
      <b>without</b> attempting to simply assume
      whatever you have previously pre-judged
      to be the case.
   </p>
   <p>
      The only reason that we can see that someone
      would start out with 'judgements of insanity'
      and/or of our 'obvious unreasonableness' --
      just based on only one sentence,
      taken out of context, no less --
      is much more likely to be taken, by us,
      as more of a political and rhetorical move,
      designed to discredit in advance,
      as a kind of social performance.
      Ie, that conflict theory is more the case
      than any form of mistake theory,
      on the part of whomever we are talking to.
      In such cases, we legitimately become skeptical
      that some other (unstated/implied) agenda
      is attempting to be (covertly) moved forward
      by such declaritively judgemental people.
   </p>
   <p>
   :fyl
      > You should account for the possibility
      > that other people will have arguments
      > that you have not heard yet
      > to dampen your confidence.
   </p>
   <p>
      Of course.
   </p>
   <p>
      Could it be the case that someone finds
      some problem with the argument logic?
      Could maybe someone share an argument
      that extends or maybe shows some gaps,
      however tiny they may be, that open hope
      in the substrate-dependent misalignment arguments,
      such as to show, that actually, somehow,
      at least in some limited rare cases,
      that alignment would maybe be possible?.
   </p>
   <p>
      And we would like for that to happen.
      Our result is not at all a happy one,
      and we are ourselves
      fully well aware of that.
      We also would like it to be different.
   </p>
   <p>
      As such, we will, and do, keep an open mind,
      and will keep staying open for someone
      to point out something, some more nuanced point,
      that we have overlooked, and/or have not heard before.
      So far, we simply have been repeatedly noticing
      that our interlocutors are reacting abrasively
      and/or stating forms of the same disagreements
      that we have already addressed in our work.
   </p>
   <p>
   :g26
      > You cannot assume that any readers
      > have your context.
      > Therefore, most people
      > will likely take your 'overconfidence'
      > as a evidence
      > that the rest of your work
      > is not worth thoroughly considering.
   </p>
   <p>
      This is a bit of a 'chicken and egg' problem.
   </p>
   <p>
      Do we make an offensive claim 1st,
      and thus both acknowledge and advise
      what is coming, and why it is important,
      as a kind of "truth in advertising move"
      accepting also the fact that at that moment,
      no context has been provided to them?
   </p>
   <p>
      Or do we provide that context,
      as a some sort of framing preface,
      without giving the reader some sense
      of at least some sort of reason
      as to why anyone should bother
      to review our arguments,
      prior to conclusion, in abstract,
      given that everyone involved
      is generally too busy for such things?
   </p>
   <p>
      You would think that some mixture
      of these two approaches would be possible.
      Unfortunately, the conclusion is so adverse
      that nearly everyone wants to avoid it,
      and is thus heavily defended, unconsciously,
      against <b>any</b> approach of <b>any</b> type at all.
   </p>
   <p>
      Options;.
   </p>
   <p>
        - If we do context 1st,
        and build up to the idea, step by step,
        people say "this is a waste of time"
        (and do not bother to continue reading).
   </p>
   <p>
        - If we say where we are trying to go,
        they reject it immediately, saying
        "there is no possible way you can do that
        (so I am going to ignore you)"
        which is a kind of an irony, really.
          - that they want context (more/less),
          and then to limit the context, etc.
   </p>
   <p>
        - If we given an outline, a summary,
        people declare we are 'not precise enough',
        and immediately dismiss the entire thing
        (no further future consideration given).
   </p>
   <p>
        - If we are precise, and define things,
        with care, nuance etc, they then miss
        the overall idea, and loudly declare:
        "what is the point of all of this?"
          (they get lost, and want an overview --
          which if we give one, and put the
          conclusion in a visible way,
          we are now going back upwards this list).
   </p>
   <p>
      Thus, every variation so far tried
      has resulted in each person suggesting
      that we take some other approach,
      and set the balance elsewhere than we have.
   </p>
   <p>
      So we have elected to go for
      the very most simple method --
      ie; to say the very worst thing first,
      and to use that as a filter,
      a kind of 'separate the people' device
      so as to determine who <b>we</b> will find
      as being worth taking to,
      rather than just assuming that they
      will be the ones to determine that.
   </p>
   <p>
   :g42
      > The issue with saying "100%" anything
      > are deeper than just
      > "it makes you sound overconfident".
      > You do not just _sound_ overconfident
      > when you claim something
      > with 100% probability.
      > You are actually _being_ overconfident
      > when you do that.
   </p>
   <p>
      > The "100%" is _not_ to be understood
      > as "1 percent more confident than 99%".
      > It is infinitely more confident than 99%.
      > It takes literally infinite amounts
      > of evidence to reach 100% certainty
      > of anything.
   </p>
   <p>
      The notion of 'infinite' here
      is making a reference to
      what would <b>seem</b> to be
      an inductive argument,
      much as would occur in
      any science experiment.
      All scientific claims to truth
      are in the form of
        "we saw evidence X
        on Y occasions".
      Where Y is some large number
      of 'observance occurrences' in the past
      that we can reasonably, inductively,
      expect that same thing
      to happen in the future.
   </p>
   <p>
      For example, the sun has come up,
      every day, more than 300 billion times.
      Hence, we have a very good,
      though not an absolutely <b>perfect</b> 100%
      confidence that the sun
      will come up tomorrow.
   </p>
   <p>
      The problem with this sort of thinking
      is that it tacitly and invisibly conflates
      scientific-technical understanding (inductive)
      with mathematical reasoning/knowing (deductive).
   </p>
   <p>
      Neither half of our argument form
      is a statement of the kind:
         "we have observed
         x evidence in the past,
         and therefore
         have Y expectations
         in the long future".
      We are not making any such claim.
      Instead the form of what we are saying is:
         "when modeled in any variation of pattern P,
         we notice that contradictions X and Y apply;
         therefore we can know that Q is impossible".
   </p>
   <p>
      Ie, our argument is in the domain of mathematics,
      with strictly deductive reasoning, and is <b>not</b>
      in the domain of science and technology --
      it is actually in the domain of mathematics,
      ie; the world of modeling of the modeling itself.
      Even though it would <b>seem</b> that any considerations
      of AGI would be or "should be", somehow,
      just scientific and/or just technological,
      it turns out that that is NOT the case.
   </p>
   <p>
      It is easy for us to see and accept
      that it would be very easy for anyone
      who is looking at our topic papers, etc,
      for the first time, etc, and to think
      that this is a scientific/technical issue.
      But it is actually not that kind of issue.
      This fact is even more so, given that,
      while we overall speak to a safety issue,
      we are not just considering safety here.
      The concern is more than just about
      potential deployed tech --
      it is also about civic policy,
      what sorts of things we, as humanity,
      value about the world we live in,
      what ideals we will hold as sacred,
      and have as a basis of the practice of
      what is clearly a public commons choice.
   </p>
   <p>
      For discernments of these kinds (x-risk),
      and at these sorts of total scale,
      we need stronger, more reliable
      forms of reason, ie; mathematics.
   </p>
   <p>
      For things that matter this much,
      placing a bet on the entire future,
      of the world, of our species, of life,
      we <b>all</b> need to be _absolutely_certain_.
      Science is not enough for that --
      only a kind of well structured mathematics
      and/or philosophy, grounded in the metaphysics
      of choice, change, and causation itself,
      could ever even possibly offer
      the needed basis of our thinking
      (about x-risk, civilization design, world, etc).
      The completion/closure of a certainty arrived at
      is therefore not a liability, not disadvantage,
      rather it is an absolutely necessary advantage.
      For something this important,
      Nothing even slightly less will do.
   </p>
   <p>
   :g5w
      > In uncertain domains
      > with very sparse evidence
      > (like AI alignment, for example)
      > that proper epistemic/epidemiological grounding
      > is incredibly important to correctly weigh
      > the what little evidence we do have.
   </p>
   <p>
      Yes, we agree.
   </p>
   <p>
      The specific epistemic grounding we need
      is in the form of mathematical logic
      about the dynamics of modeling itself,
      and sound reasoning about the application
      and relevance of that modeling
      to our real world physical situation.
   </p>
   <p>
      Of course, it can also be asked:
        what sort of universe do we live in?.
   </p>
   <p>
      For example, are the physical laws
      so far identified the only ones?
      Do they actually apply universally?
      Are they absolute, or can there ever
      be exceptions, occasions where things
      are just a little bit different?
   </p>
   <p>
      For example, is it possible,
      that never,
      in the whole history of
      the entire/whole universe,
      that any particle ever exceeded
      the speed of light limit?
   </p>
   <p>
      Of course, we could even ask
      if the notion of "whole universe"
      and/or "total history" is even
      an internally consistent concept?.
        - There are good reasons to maybe think
        that such notions (and tacit assumptions)
        of the concept of totality,
        are sometimes mis- and over- applied.
        - ?; Is there even a right form of
        the concept of totality,
        that can be used at all?.
   </p>
   <p>
      This is not just about quibbles as to
      whether the 'light-speed limit concept'
      is true only just 99.999999% of the time,
      or whether more or fewer nines are needed,
      or if such a law is just simply
      always and absolutely true.
   </p>
   <p>
   :g7s
      > When reading any published work by others,
      > I am evaluating their epistemological rigor
      > in addition to their first-order arguments.
   </p>
   <p>
      We will, of course, do the same
      with anyone who is communicating with us.
      We will, of course, continue to seek
      to determine and discern (and maybe judge)
      if they are actually reasonable enough
      to be conversational partners with us,
      to be also worth our own time explaining
      such matters to them.
   </p>
   <p>
      Expect that we will notice, for example,
      whether or not you have even noticed
      that such questions as distinguishing
      mathematics from physics from social
      might even apply.
      We will notice if connotative rhetoric
      is being used, and in what way,
      or if the arguments are inductive,
      or deductive, and if so, on what basis.
   </p>
   <p>
      We will even notice the more subtle things,
      like whether such questions as to the
      nature of the universe, of law and causation,
      are the same, or are different,
      and of what sorts of implications
      that such constancy questions might have.
   </p>
   <p>
   :g9c
      > Taking any position
      > with 100% certainty
      > is an immediate red flag.
      > Therefore, your true certainty
      > should be less than 100%.
   </p>
   <p>
      Mistaking or equivocating regarding
      the difference between scientific evidence
      and reasoning purely on the basis of logic
      is a red flag for us too.
   </p>
   <p>
      We acknowledge that there
      are possible ways that we could be wrong,
      but that also means that are assuming
      that we will at least be actually met
      in the proper domain of argumentation.
   </p>
   <p>
      The suggestion given, as a normative,
      is actually in the domain of the social --
      ie, not even in the domain of science
      nor of mathematics (reason and logic).
      Being in the wrong domain entirely,
      your suggestion is actually
      neither valid nor sound,
      and is therefore,
      not relevant.
   </p>
   <p>
   :gal
      > I am not suggesting that you
      > moderate your probability estimates
      > for social reasons.
      > I am suggesting that you put your
      > honest best guess probability estimate.
   </p>
   <p>
      We are honestly making a statement
      which is akin to the "belief"
      that "2 + 2 == 4",
      and we are really truly trustfully
      100% confident in that.
   </p>
   <p>
      We are not guessing.
      To assume otherwise
      is a pejorative on your part.
      And it does <b>not</b> help you.
   </p>
   <p>
      Based on the logic of our argument,
      we are as near to '100% certain'
      as it is possible to be,
      in any context, using any model,
      regarding the total actual absence
      of any possible safety/utility
      of any type, kind, or deployment,
      of AGI/APS/superintelligence.
   </p>
   <p>
   :gbu
      > "100%" is never the correct answer
      > for any question of probability.
      > Your confidence should be less than 100%.
      > If you want, put "99.99999%" --
      > ie; a 1 in 10 million chance
      > you are wrong.
   </p>
   <p>
      > Maybe there are at some rare questions
      > for which a 'six nines' answer
      > could be the correct thing to do.
      > I do not believe you are there.
   </p>
   <p>
      > I think that you should put odds
      > far lower than 99.99% (two nines).
      > Something more like maybe 80%.
   </p>
   <p>
      Boolean logic is not a continuum.
      Matters of truth and falsity (proof)
      are digital, not analogue.
      Hence it is not a matter of probability,
      it is a matter of possibility,
      and there is a key difference
      between these two concepts,
      same as there is an inherent difference
      between 'content' (what exactly is said)
      and context (who, when, where, and how
      something is said, in what language, etc).
   </p>
   <p>
      To suggest otherwise, would be dishonest.
      What probability do you want to put on
      perpetual motion being impossible?
   </p>
   <p>
   :gdq
      > I think that you are massively overconfident.
   </p>
   <p>
      Do not depend on our confidence;
      Review the arguments for yourself,
      and come to your own conclusions.
      Do not just take our word for it.
      Analyze the situation yourself,
      using the proper epistemic tools,
      and you will not need anything from us;
      no changes in our presentation,
      or our style of speaking --
      you will be/become your own advocate.
      Try it yourself, (if you dare!).
   </p>
</body>
</html>