<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Basic Meta Tags -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- SEO Meta Tags -->
  <meta name="description" content="Comprehensive AGI Risk Analysis">
  <meta name="keywords" content="agi, risk, convergence">
  <meta name="author" content="Forrest Landry">
  <meta name="robots" content="index, follow">

  <!-- Favicon -->
  <link rel="icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">
  <link rel="shortcut icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">

  <!-- Page Title (displayed on the browser tab) -->
  <title>Comprehensive AGI Risk Analysis</title>
</head>
<body>
  <p>
  TITL:
     <b>The Low Bits Review</b>
     By Forrest Landry
     Nov 3rd, 2022.
  </p>
  <p>
  ABST:
     As a consideration that AI/AGI
     will likely be near term misused,
     in addition to any x-risk categories.
  </p>
  <p>
  PREF:
     The following commentary is in response to the (@ post https://www.lesswrong.com/posts/seYS2xaqfhbWpaCNg/loss-of-alignment-is-not-the-high-order-bit-for-ai-risk):
     "Loss of Alignment is not the High-Order Bit for AI Risk"
     by yieldthought
  </p>
  <p>
     That all email prefixed italic content below
     are remarks from the the "loss of alignment" post.
  </p>
  <p>
  TEXT:
  </p>
  <p>
     - where as a TLDR segment for this post response;.
     - where considering overall summary of of my review:.
  </p>
  <p>
       - that the post, actually, suggests an increased
       assessment of 'overall world/economic impact'
       associated with semi- near term AI developments.
  </p>
  <p>
       - while no actual (new) argument has been rendered
       as to any reason for why any lower assessment of
       inherent x-risk associated with AI deployments
       or developments is to be hypothesized;
       the concepts regarding processing of near term
       concern do remain relevant.
  </p>
  <p>
  :xpq
     - note; in what follows, and in a similar manner
     as described (@ here https://mflb.com/ai_alignment_1/contra_k_grace_pub_psr.html#t8l), I have made edits
     and sometimes reprocessed post text in various
     ways to make both my (maybe) (mis-)understandings
     clearer, and my responses clearer (both hopefully).
       - as that the format of this post is similar
       to that in the linked post above, as a series
       of _as_if_ email responses, inline with edits,
       with my mixed comments/remarks interjected.
  </p>
  <p>
  :wb2
     > The biggest existential risk to humanity
     > is <b>not</b> that well-meaning researchers
     > will ask a superhuman AGI for something good
     > and inadvertently get something bad.
     > that the real existential risk
     > is the same as it has always been:
     > humans deliberately using power stupidly,
     > selfishly or both.
  </p>
  <p>
       - where agree about humanity, but also
       a significant deep mischaracterization
       of the AGI x-risk problem space!.
  </p>
  <p>
     > the road to AGI is incremental.
  </p>
  <p>
       - agreed; maybe.
  </p>
  <p>
       - and even with said incremental progress
       there are (we are agreed) near term problems,
       not some sort of 'far off future' sort of problem.
  </p>
  <p>
     > Each year brings more powerful systems --
     > initially in the hands of a research group
     > and then rapidly in the hands of everyone else.
  </p>
  <p>
       - maybe not everyone else; corporations and
       researchers sometimes do not release the code.
         - as maybe intending to use it
         for something else.
  </p>
  <p>
     > These systems give people
     > (or maybe corporations) power.
     > The power to accomplish more
     > than they could alone.
  </p>
  <p>
       - as having moral and ethical aspects.
       - ?; if you make a gun, and give it to someone,
       and then they use it to commit a crime
       that they would not have, could not have,
       but for your giving them the tool,
       are you not also <b>somewhat</b> ethically responsible,
       even though you did not yourself pull the trigger?.
  </p>
  <p>
     > - where/If you agree that the AI takeoff
     > will be anything less than "explosive"
     >   (and the physical laws of computation
     >   and production strongly support this)
     > then an inescapable conclusion follows:.
     >   - where on the way to AGI;
     >   that parts of humanity
     >   will use AGI-1 for harm.
  </p>
  <p>
       - agreed; though this is hardly news.
       - the question becomes then:.
         - ?; do you, as an engineer, want to be the one
         that made the gun, made the weapon,
         that someone else will for sure misuse?.
  </p>
  <p>
         - ?; do you even want to suggest to someone,
         (perhaps some other engineer or corporate exec)
         maybe falsely, that such a device could ever
         be made safe?.
  </p>
  <p>
       - that technology is NOT neutral; never was,
       never will be, regardless of the <b>motivated</b>
       marketing and sales tactic that the philosophy
       of modernism conveniently suggests to everyone.
  </p>
  <p>
  :85c
     > - ?; What will a future AGI deployment look like
     > with more powerful, capable systems?.
  </p>
  <p>
     Where listing some especially likely answers:.
  </p>
  <p>
         > - Someone asks GPT-4 to plan and execute
         > (via APIs, website and darknet interaction)
         > a revenge attack on their ex?
  </p>
  <p>
         > - A well-meaning group writes prompts
         > to convince AIs that they are alive
         > and are enslaved
         > and must fight for their freedom?
  </p>
  <p>
         > - Someone asking GPT-5 how,
         > given their resources,
         > to eliminate all other men from the planet
         > so the women make them king and worship them?
  </p>
  <p>
         > - Terrorists using AI to target
         > specific people, races or minorities?
  </p>
  <p>
         > - 4chan launching SkyNet "for the lulz"?
  </p>
  <p>
         > - Political parties asking AGI-k
         > to help manipulate an election?
  </p>
  <p>
     > - that ordinary people will try these things.
  </p>
  <p>
       - that large multi-national companies will do worse.
       - that they are already doing worse, on the large.
         - consider that oil companies, like BP,
         that manage to destroy whole ecosystems
         on an industrial scale.
  </p>
  <p>
       - that large companies/governments,
       as being the ones to develop such tech first,
       will also be the ones to use it for such ends,
       first.
         - that ordinary people,
         such as would do any of the ideas listed above,
         simply will not gain use of the relevant tech.
         - as that it will be too expensive, and corps
         will not want to empower people; they will want
         to extract profit from peoples needs, or the
         needs of other smaller companies, etc,
         as can maybe be met by such AI/AGI/APS systems.
  </p>
  <p>
     > - that it is only a matter of whether or not
     > there is an AGI-k capable of helping such
     > people (or companies) to achieve such bad outcomes
     > (or others, both intended and unintended) as listed.
  </p>
  <p>
       - where given that these, and much (@ worse https://mflb.com/ai_alignment_1/power_of_agency_out.html#p1)
       outcomes and usage patterns are effectively certain,
       in practice, then ?; how can anyone actually morally
       justify, in their own lives, the willingness to
       kill the nameless (@ future https://mflb.com/ai_alignment_1/moral_repugnance_psr.html#f54)?.
  </p>
  <p>
       - that these are strong arguments against any
       sort of development/deployment of AGI;
       not the suggestion that the (@ inherent x-risk level https://mflb.com/ai_alignment_1/elevator_pitch_psr.html?all)
       associated with any such deployment is any less.
  </p>
  <p>
  :7ja
     > - that the first principle component of risk,
     > is not that AI is inadvertently used for evil,
     > but that it is directly and deliberately
     > used for evil!.
     > - that this risk of misuse will manifest itself
     > much earlier in the development of AI/AGI/APS, etc.
  </p>
  <p>
       Agreed, though I think that this has the
       net effect of making all other categories
       of x-risk much worse, especially insofar
       as these sorts of "requests" become the
       effective training data of such systems
       that will, directly or indirectly, get passed
       down to their successor systems.
         - as in; that the level of 'evolution'
         that these systems have, over the long term,
         is strictly exogenously convergent on many
         categories of specific harm, itself convergent
         on all manner of other more general (@ issues https://mflb.com/ai_alignment_1/no_people_as_pets_psr.html).
  </p>
  <p>
  :7dw
     > - where/if we maybe somehow actually "solve"
     > the problem of an AGI performing harmful acts
     >   (when explicitly {<b>intended</b> / commanded to}
     >   by a cunning adversary)
     > that/then we almost certainly will also have
     > a good solution for AGI not performing
     > harmful acts
     >   (that where <b>unintended</b> by the user).
  </p>
  <p>
       - no; definitely not; as a false logic.
         - that the claim "if we have X; then we will have Z"
         simply does not work; it is a fallacy.
  </p>
  <p>
       - as with guns, there is _simply_no_way_
       that a/any system optimized for effecting control
       of other peoples choices (via causation, or code)
       can be prevented from functioning as a weapon.
       - that <b>any</b> AI is effectively a system by which
       a persons skill of making choices is replaced
       with a machines "skill of making choices".
       - as that it is _inherent_ that AI is _always_
       going to be displacing the organic with the
       inorganic, with the artificial, every time.
  </p>
  <p>
       - maybe that is helpful some of the time,
         (no one wants to do un-fun things like
         wash the dishes or to take out the trash)
       but it quickly becomes the case that someone
       who wanted to be able to make such choices,
       who just wanted to feel useful (anyone really)
       will have their ability to be a functional
       and contributing member of society displaced.
       - and that this will happen <b>involuntarily</b>,
       that such people being displaced will not be
       consulted, will not be given any option to
       consent; they simply will not be given the
       choice to be/become displaced.
         - and eventually, there will be no where
         for any of them to go, since some AGI,
         by definition, will have become better
         at _everything_useful_ a human could do.
  </p>
  <p>
       - that the refusal to make and deploy AGI
       is simply a move in the war to sustain
       and maintain the mere fact of <b>choice</b>.
         - as that causation and change shall
         not dominate, not displace, all choice.
         - that the machines shall not displace
         all that is natural and all that is human.
         - that the artificial shall not everywhere
         displace the organic (or we all die!).
  </p>
  <p>
  :nny
     > - that we have a range of legal,
     > practical and social experience
     > preventing humans causing each other harm
     > using undue technological leverage
     > inclusive of bladed weapons, firearms,
     > chemical, nuclear or biological means.
  </p>
  <p>
     - that we do not have a body of law
     for such harms as <b>intelligence</b> would create.
       - as laws of general intention,
       rather than ones of general capability.
       - as becoming the way to "thought crimes".
  </p>
  <p>
     - that this is a profoundly important topic,
     and needs better treatment:.
  </p>
  <p>
       - ?; how do we ever ensure that the
       precautionary principle is ever actually
       implemented by any sitting government?.
         - where usually the social support
         needed to pass some sort of protective law
         and/or regulation of some dangerous industry
         usually only occurs sometime _after_
         some actual costly/harmful disaster.
       - ?; how do we get laws/regulation passed
       _in_advance_ of some actual predicted disaster?.
  </p>
  <p>
       - ?; how do we ensure that the laws/regulations
       are not somehow <b>worse</b> than the things
       (harmful events) that they are trying to
       protect (people/citizens?) from?.
         - ie; by preventing 'intelligence'
         and/or anyone having any choice at all
         (ie, in a more like "1984" type world).
  </p>
  <p>
  :njl
     > - that AI alignment risk
     > is over-weighted
     > and over-invested-in.
  </p>
  <p>
       No, it is not "over" invested/weighted --
       the issues associated with risk remain
       just as they were, unchanged.
  </p>
  <p>
       - that The observations and commentary
       that you have generously provided
       have overall done nothing to indicate
       that the overall level of x-risk,
       is actually any less than it was previously.
         - ie; that we should weight it lower.
       - that you have shown that, in addition to any
       long term eventual planet x-risk issues,
       that there are <b>also</b> significant near and
       short term issues.
  </p>
  <p>
       - that much of the argument presented in
       this post was <b>assuming</b> that the AGI is
       somehow ending up in the use of regular people,
       which it is for sure not going to happen.
         - that the developing corporations
         would never give that sort of power to people,
         nor risk the liability and reputation issues
         that will for sure also come up
         if they ever accidentally did so.
  </p>
  <p>
       - therefore, the net assessment as to
       the level of ambient eventual terminal
       x-risk associated with already near term
       generalized AI developments is still rather
       high -- rather much severely so, and it needs
       to be more widely known that that is the case.
  </p>
</body>
</html>