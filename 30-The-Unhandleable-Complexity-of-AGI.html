<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Basic Meta Tags -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- SEO Meta Tags -->
  <meta name="description" content="Comprehensive AGI Risk Analysis">
  <meta name="keywords" content="agi, risk, convergence">
  <meta name="author" content="Forrest Landry">
  <meta name="robots" content="index, follow">

  <!-- Favicon -->
  <link rel="icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">
  <link rel="shortcut icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">

  <!-- Page Title (displayed on the browser tab) -->
  <title>Comprehensive AGI Risk Analysis</title>
</head>
<body>
    <p>
    TITL:
       <b>The Unhandleable Complexity of AGI</b>
       By Forrest Landry
       November 8th, 2022.
    </p>
    <p>
    ABST:
       Offers some specific concepts and connections
       to how AGI alignment efforts cannot not involve
       unhandleable complexity, which in itself
       strongly suggests that any effort to align AGI
       is for sure wasted time, in the long term.
       That it is better to not build any form of AGI
       in the first place.
    </p>
    <p>
    TEXT:
    </p>
    <p>
       Where in regards to the inherent complexity of the
       'error correction schemes'
       that are being associated with AGI alignment efforts,
       and/or the the identification of
       what would even be even minimally required
       to even begin to think about
       such concepts of AGI-alignment:
         ?; Have you noticed the trend
         that more and more people
         are finding more and more
         (increasingly) <b>unhandleable</b> complexity
         the further anyone looks into
         any part of AGI 'alignment' research?.
    </p>
    <p>
       - ?; Have you noticed that there is an overall trend --
       that we are all finding (more and more)
       increasingly unhandleable complexity
       the further researchers have dug into
       AGI-alignment research --
       that the sheer number of problematic considerations
       actually keeps increasing, seemingly without limit,
       over these last several years?.
    </p>
    <p>
       - ?; maybe we can see this trend to/towards
       increasingly unhandleable complexity
       as we go further down we go into
       the detailed concrete properties of ML algorithms,
       and why/how they inherently do not lends themselves
       to/towards "alignment"?.
    </p>
    <p>
       This strongly suggests that alignment researchers
       will overall need to shift gears
       and instead very deliberately and constructively
       inform and foster shared awareness (@ note1 #note1)
       of the intractable nature of the problems.
         (ie; particularly with the public/tech funders
         and other AGI researchers still trying for
         some sort of "miracle cure").
    </p>
    <p>
    :8kq
       > Yes, some people _have_ noticed that there is
       > more complexity now in thinking about 'how to do'
       > AGI-alignment, or in regards to any aspect of
       > the overall AGI alignment arguments.
    </p>
    <p>
       > However, at least some of the people do not
       > think that the complexity is "exploding".
       > Perhaps you are simply being alarmist?.
    </p>
    <p>
       The total 'rate of increase'
       is not the only thing of interest.
       That what is being noticed is:.
    </p>
    <p>
         - 1; that the total 'amount'
         of <b>unhandleable</b> complexity is increasing.
    </p>
    <p>
         - 2; there does not seem to be any likelihood
         of it <b>not</b> continuing to increase (indefinitely).
    </p>
    <p>
         - 3; that the rate of increase is itself increasing
           (ie; that the acceleration factor is increasing).
    </p>
    <p>
       These are not good trends,
       and are giving zero sign of becoming better trends.
    </p>
    <p>
    :886
       Moreover, there is the aspect of 'unhandleable'.
       It only takes <b>one</b> fully and absolutely
       un-handleable aspect in <b>any</b> part of anything
       that has been identified as an absolute requirement
       of alignment for/of AGI
       to make the overall project of alignment itself
       fully fail.
       That this can easily be the essence of
       any proof of actual structural impossibility
       of AGI-alignment.
       It is merely to ask:.
    </p>
    <p>
         - cx1; ?; is/are there a, or any, or multiple,
         characteristic(s) 'X'
         that has/have been shown
         to be an absolute requirement
         in <b>a/any/the/all</b> AGI alignment concepts/schemes?.
    </p>
    <p>
         - cx2; where for a/each/any specific member(s)
         of the total set of such characteristics 'X';
         ?; has any single one of them also been shown
         to be strictly/analytically impossible to achieve
           (ie; in any way that is inconsistent with
           any part of the known truths of mathematics
           and/or also the laws of physics)?.
    </p>
    <p>
    :83s
       Where in the total definitional state space,
       that it is overall
       much/far more likely
       that some overall desired outcome
       of some/any engineering sub-effort
         (ie; some necessary critical component)
       will be identified to be impossible,
       this making the overall project also impossible.
    </p>
    <p>
       Hence, we also notice that in the total space
       of all of the things we would wish that we could do
       with science/technology/engineering
         (ie; with an understanding of causation and math)
       is very much greater than the actual space of things
       that we can actually do with science/engineering/tech.
       The scope of our imagination and desires
       greatly exceeds the scope of
       what we can actually build,
       even conceptually,
       even in principle.
    </p>
    <p>
         (Where examples of some non-constructable products,
         things that engineers can imagine, but cannot
         be made: time machines, anti-gravity,
         trans-luminal warp drives, faster than light
         messaging/signaling, the philosophers stone, etc).
    </p>
    <p>
       Thus, it is not just today that some things
       are actually impossible to make;
       it would for these 'known impossible components'
         (ie; things inconsistent with math or physics
           (ie; inconsistent with logic and causation itself))
       will for sure not ever be built
       at any point in the future,
       no matter how long later you wait,
       no matter how advanced all other fields of tech
       may themselves eventually become.
    </p>
    <p>
       Just because some people like working on hard puzzles
       does not mean that all puzzles are inherently solvable.
    </p>
    <p>
       There are <b>lots</b> of even very simply questions
       that can be asked in/about some mathematical object
       that turn out to be completely beyond
       our available tools and techniques of proof.
       Many of these sorts of questions remain open,
       as hard puzzles unsolved by generations of
       the very smartest people that have ever lived,
       until sometimes, someday, someone may eventually
       prove that no solution is actually possible --
       ie, there is something identifiable about the
       principles of the question which are actually
       inherently inconsistent.
    </p>
    <p>
         (Where some examples of open problems in math,
         one does not need to go far.  Just for prime
         numbers alone we have:.
           - (@ URL1 https://www.businessinsider.com/prime-number-unsolved-problems-2014-12?op=1).
           - (@ URL2 https://medium.com/math-simplified/unsolved-problems-of-primes-cf51b9348dd2).
           - (@ URL3 https://medium.com/nerd-for-tech/5-oldest-unsolved-problems-in-mathematics-about-primes-3c894b5263d7).
         If you want something less primish, try:.
           - (@ the sofa problem https://www.popularmechanics.com/science/g2816/5-simple-math-problems/).
           - (@ wikipedia https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_mathematics).
         In any case, there are any number of
         ultra-hard problems that one can pick from).
    </p>
    <p>
    :7ku
       We can setup a general categorization
       of various classes of problem types:.
    </p>
    <p>
         - 1; Some problems are easy, and generally get
         solved fairly quickly, once someone finds/knows
         the right sorts of tools to apply to the job,
         and finding/using those tools seems also easy.
    </p>
    <p>
         - 2; Some problems are merely hard, and will (maybe)
         get solved by the addition and/or (maybe)
         discovery of some new techniques (eventually).
    </p>
    <p>
         - 3; Some problems are so hard that it is likely
         that no one (no person) will ever solve them,
         no matter how clever/smart they are
           (maybe some specialized AI will do it?)
         even though, technically, the problem is
         actually solvable using the toolset provided
         (or which, also very hard, to maybe discover).
    </p>
    <p>
         - 4; Some problems are eventually discovered
         to be unsolvable with any variation or combination
         of the techniques provided/suggested in the
         nature of the problem itself (not all toolkits,
         methods, techniques, etc, are equally suited
         to solving all problems, and shifting to another
         toolkit/etc will make the problem more tractable).
           - ie; as that the problem could maybe solved,
           but definitely not with the available tools;
           that new tools will need to be used/discovered
           so as to enable resolution as to the solution
           of the problem (relative to that toolkit).
    </p>
    <p>
           For example, Galois theory shows that there
           is no solution to quintic or higher equations
           using just the rational operators of algebra.
    </p>
    <p>
           Another example, that there is no resolution
           to the Continuum Hypothesis using (@ ZFC https://en.wikipedia.org/wiki/Continuum_hypothesis) as the
           toolset, (as was shown by Paul Cohen in 1963).
    </p>
    <p>
         - 5; Some problems/questions are obviously/simply
         (and inherently) (fully) unsolvable/unanswerable,
         simply due to a direct (maybe immediate)
         contradiction in the way they are formulated/asked.
           - and that they can be identified as such
           fairly quickly, using the tools/techniques
           that people already know about and/or could
           maybe discover fairly easily.
    </p>
    <p>
         - 6; Some problems/questions are not so obviously
         and inherently unanswerable, and although
         there <b>is</b> a deep contradiction in the way
         that the question/problem is formulated/asked,
         that it also takes very smart people
         a very long time (maybe never?) to show/establish
         such impossibility using known, or even maybe
         eventually discovered, tools and techniques.
    </p>
    <p>
         - 7; Some problems are actually truly impossible
         to solve, but the sub-problem of even identifying
         the techniques/methods to show/demonstrate that
         impossibility itself turn out to be the kind
         of (sub-)problem that is itself inherently
         difficult to such a degree that it might be
         simply beyond the capability of any human
         intelligence to implement such a proof of
         impossibility, or to even find/discover such
         tools/techniques of proving such impossibility.
    </p>
    <p>
    :7hc
       There are three things to notice about this table:.
    </p>
    <p>
         - One is that there is a relationship between the
         problem and the tools/techniques used in relation
         to either solving the problem, or showing that
         the problem is impossible.
    </p>
    <p>
         - The next is that for any given problem,
         showing that it is possible or impossible
         is an invariant under the toolset; ie that
         any problem/question which has been shown to
         be possible with any toolset is 'solvable',
         and/or any problem/question which has been
         shown to be impossible/unanswerable via <b>any</b>
         toolkit is actually insolvable/unanswerable
         no matter what possible toolkits may eventually
         be discovered in the future.
           - as 'once impossible'
             (via <b>any</b> toolkit/method)
           that <b>always</b> impossible
             (for <b>all</b> tools/methods).
    </p>
    <p>
         - That the third observation is that the problem
         of identifying the right toolset/techniques
         to identify a problem/question as easy or hard,
         or solvable or unsolvable, is itself a kind
         of sub-problem which itself may have aspects
         which itself finds a position on this table.
    </p>
    <p>
    :7f6
       Where/In these three noticings, it can be observed
       that there is a kind of 'meta-consistency principle'.
       If there is a way to prove 'X', then there are
       very likely to be discovered /many/ other ways
       to also show/prove 'X', maybe using different
       techniques, methods, or toolsets.
       Similarly, we notice that a disproof of 'X'
       is also similarly verifiable as 'disproven'
         (aka as 'structurally impossible', per principle)
       in multiple diverse ways, using multiple diverse
       toolkits/methods/techniques.
       In this way, there is a kind of epistemic coherency
       that accrues to topics/questions of this type.
    </p>
    <p>
         That this observation of epistemic coherent
         <b>also</b> applies to "category 4" problems,
         though in its own unique way.
         For that class of problems where the nature
         of the solution is itself strictly dependent on
         the specific selection of the tools/methods
           (ie; the axioms, initial conditions, etc)
         then it will <b>remain</b> the case
         that the notion of 'how is this problem solved?'
         will always be strictly dependent on
         the selection of the toolkit/methods.
    </p>
    <p>
    :7cn
       Where in regards to the problem of "AGI alignable?"
         (ie; as in "how do we make AGI long term safe?").
       it is important to see if we can 1st establish what
       is the likely 'class' of the specific problem/question.
       To some extent, this means also 2nd identifying
       the class of the techniques/methods
       that will be needed to solve the problem
         (and/or to answer the asked AGI questions, etc).
    </p>
    <p>
       With at least with respect to this latter aspect,
       we can for sure know what set of tools/techniques
       are to be used: whatever is available from either
       mathematics and/or physics, which itself means
       any combination of logic and causation, which can
       include things like "computer science" and "code"
       as special cases.
    </p>
    <p>
       Hence, anything which inherently is impossible
       via pure mathematics (ie is logically inconsistent)
       and/or anything which is inherently non-causal
       (ie, cannot be implemented or modeled using purely
       causal models) will be therefore forbidden, and
       thus indicative of being an actual impossibility
       with respect to the top level objective problem
       we are attempting to solve.
    </p>
    <p>
    :764
       On this basis, it might be asked, given the certainty
       and knowing of what toolkit/methods are to be used
       for/with our AGI question (ie, logic and causation):
         "What class is the AGI-Alignment/Safety question in?".
       And for this, it turns out that there is a definite
       answer: it is for sure in problem class 5.
    </p>
    <p>
       One way to see this is to notice that any notion
       or concept of "choice" as "selection of specific action"
       by an "agent" or "actor" (in this case "chooser")
       is going to have _both_ 1; some sort of "basis"
         (ie, a specific proclivity to/towards some sort
         of "desired" or "preferable" outcome characterization;
         which in the case of AI, might be though of as
         some sort of 'utility function' or 'objective goal'
         definition, or 'property', characteristic of value)
       and _also_ 2; some sort of "outcome"
         (ie, that any selected action has consequences
         and it is the (potential) characterization
         of these (maybe directed) outcomes
         that is of interest with respect to that basis).
    </p>
    <p>
    :6ts
       While these may seem to be fairly vague
       and non-specific ways of specifying/defining
       these concepts, it is important to
       <b>not</b> be any more specific/precise than exactly this.
       It is to be noticed that these concepts, though
       seemingly vague and undefined in themselves
       do actually have the sorts of
       inter-concept relationships that do apply to <b>any</b>
       variation of these same notions as might be used
       in any version of the total category of questions
       that have the form of "(how) AGI-alignment-safe?".
       In effect, these specific constellations of terms
       apply to the total category of <b>all</b> forms of the
       AGI alignment question, regardless of the specific
       proposed methodology as to how that might be
       (seemingly) accomplished.
    </p>
    <p>
       In other words, to make any of these concepts
       <b>any</b> more precise would be to _weaken_ the inherent
       applicability and generality of the argument/proof.
       Being more precise may be especially helpful for
       certain types of problem, whereas for others,
       doing so is actually unhelpful/problematic,
       insofar it is also the case that the more precise
       the definitions, the more there is a chance that
       there is are some /extra/ assumptions also included.
       That these extra assumptions will be either:.
         - one that is not necessary, but harmless.
         - one that is maybe excluding of important
         (key/critical) classes of proof generality
           (ie; problematic for the proof to be useful)
         - one that is actually harmful insofar as it
         makes the proof either invalid or fully unsound
           (ie; for all classes of situation for which
           the proof was wanted for in the first place).
    </p>
    <p>
       As such, as in mathematics, it is therefore also
       of critical importance to not make any extra
       tacit assumptions, and hence therefore, to not
       make the definitions of any concepts any more
       precise than that exactly needed to actually outline
       the structural relationships between the concepts
       that actually needed for the proof.
         (and of course, all of this is aside from not
         giving hype marketing promoters, or their
         (sometimes unconscious of being that) agents
         any extra vehicle for shifting to dismissing
         the proof content, logic, or application,
         by declarations of 'not relevant' due to
         the mistake of over specifying of a definition,
         or under-emphasizing a generalization, etc).
    </p>
    <p>
    :6ns
       Returning to our assessment, it is to be noticed
       that any notion(s) of 'alignment' or 'safety'
       are with respect to our/human (organic) values,
       in terms of characterizing outcomes, whereas
       these same notions ('alignment' and 'safety')
       are (ostensibly) to be used by the AI/AGI/APS/SAS
       to define/characterize its 'basis' of action
       selection (what we are calling its "choice"
       of what to do in any given moment/situation).
       In effect, we have a notion of a comparison
       of/at/within an artificial being of the 'basis'
       of (their) choice(s) as being representative of
       the (organic) human desired/wanted outcomes --
       which are also going to be comparatively
       evaluated, this time on an organic external basis
       as to whether those outcomes were desired or not.
       There are, in effect, two evaluative comparisons
       being made by two each different kinds of agents:.
    </p>
    <p>
         - an organic evaluation of basis.
         - an artificial evaluation of basis.
         - an artificial evaluation of outcome.
         - an organic evaluation of outcome.
    </p>
    <p>
       Humans may (or might not) know what they want,
       and will attempt (somehow, does not matter yet)
       encode that set of desires/wants/needs
       into some some sort of basis that is somehow
       'present' in/within (internal to) the AGI/APS/SAS
       so as to "ensure" that the AGI/APS/SAS will "use"
       that basis when it implements its own choices,
       using that basis, so as to propose possible
       outcomes, which can be evaluated/characterized
       (compared) with respect to the basis so as to
       actually select an AGI/APS/SAS action/output,
       which will then be reviewed/experienced by
       organic humans (as an input) that they will
       then (at least implicitly) 'compare' to what
       they notice that they want/feel/need/desire.
    </p>
    <p>
    :6gu
       This naturally leads to some further questions:.
    </p>
    <p>
         - ?; can organic people/humans actually
         specify what the AGI/APS/SAS shall "want"
         or be 'motivated by', 'care about',
         have as a basis of choice, etc,
         in some sort of (maybe objective?)
         encodable/teachable manner (to/into the AI),
         with sufficient persistence, etc,
         within <b>all</b> of (all aspects of)
         the choices/decisions (ie; action selections)
         of the AGI/APS/SAS, in all manner of
         circumstances, situations, environments, etc,
         in such a way that is sufficiently reliable
         so as to be overall fully characterized
         and objectively recognized by those (organic)
         humans (and any observer groups)
         as actually aligned and as safe, etc?.
    </p>
    <p>
         - ?; does the basis need to be specified
         in some sort of formal manner, or can it
         be implicit?.
    </p>
    <p>
         - ?; how do we know that the basis has
         all of the required/necessary characteristics?.
           - if implicit, how do we know it is complete?.
           - if explicit, how do we know that there
           will not be some other interactions
           that can occur to produce unexpected
           undesired outcomes?.
    </p>
    <p>
         - ?; how do we know that the machine will
         actually implement the basis given to it by
         the humans, or will that basis shift,
         "corrode", or become displaced (in time)
         by some other variant basis, different
         than the one encoded/selected by humans?.
           - ?; can we externally ever even detect
           such shifts in basis/intention/motivation?.
    </p>
    <p>
         - ?; is the basis "efficient", in the sense
         that it actually results in artificial agent
         selections (with available energy, time, etc)
         that are also aligned (ie; that the outcomes
         are actually consistent both with the basis
         and also with the organic human evaluations?.
           - ie; it would be problematic if the basis
           required near infinite energy/time to be
           resolved into an artificial agent action.
           - ie; it would also be problematic if
           the artificial agent action outcome
           was impossible to evaluate by the organic
           people (given finite time, energy, etc)
           as to whether or not such actions were
           aligned/safe, or not, etc.
    </p>
    <p>
         - ?; are there any long term eventual
         problems associated with the basis,
         or the action outcomes, with the AGI itself,
         etc, which are cumulative or convergent,
         which in themselves, will be inherently
         problematic (unsafe/unaligned) eventually?.
           - ie; possible problems with pollution,
           for example, due to human or agent actions
           which are repeated over a long interval,
           or which accumulate in the environment,
           or which promote social inequality, etc.
    </p>
    <p>
         - ?; are there situations where the basis
         (motivations, intentions, etc) of either
         the people (users) or the AI/AGI/APS/SAS
         are in direct conflict, or eventual conflict?.
           - ie; that the AI/AGI/APS/SAS encodes
           the intentions of the developers, which
           themselves are encoding the intentions
           of their executive bosses (corp owners),
           which might be "aligned" for them, but
           not aligned/safe for large segments of
           the affected commons/public.
             - note; that these intentions might
             not always be globally human aligned/safe,
             particularly where such artificial agents
             are used for war fighting efforts, etc.
           - ?; where/if the AGI/APS/SAS becomes
           "aware" of this conflict, does it end
           up either attempting to resolve that
           conflict internally, or does it resolve
           that conflict externally?.
           - ?; is the AI basis something that is
           immutable or is it mutable?.
             - where mutable, (as in the AI
             keeps learning, and/or is correctable
             at some future point (by humans?));
             ?; what are the chances of such an
             event resulting in a wholly unaligned
             AGI/APS/SAS?.
             - where immutable; ?; how is it assured
             that the AI never has basis drift?;
             and/or how is it assured that the
             basis becomes irrelevant/wrong as the
             world and usage context circumstance
             change and shift (as they for sure will)?.
    </p>
    <p>
    :6aw
       There are, for sure, a lot of other questions
       that in detail, are each critically important.
       For our purposes, however, it is sufficient
       that we can begin to notice any aspects of
       these above questions, and prior notes,
       which suggest responses to the two questions
       'cx1' and 'cx2' (above).
       Where with respect to the cx1 question:.
    </p>
    <p>
         - where listing the characteristics already
         shown/known to be an absolute requirement of
         <b>a/any/the/all</b> AGI alignment concepts/schemes:.
    </p>
    <p>
           - that any (operating/effective) AGI/APS/SAS
           will for sure make choices (action selections)
           that have actual outcomes/consequences/effects
           in at least some operating world/domain/context
           which is of at least some consequence/importance
           to at least some ambient organic humans, etc.
    </p>
    <p>
           - that any AGI/APS/SAS, (regardless of if
           it is aligned/safe or not) is going to
           have <b>some</b> basis of choice (or
           action selection, decision making, etc),
           and moreover, that whatever algorithms
           that are involved in that process, that
           they for sure will run in finite time,
           with finite energy, in finite space/memory,
           etc.
             - ie; that such 'intentions' may be
             implicit or explicit, may have come
             from inside the machine or from
             outside/external to the machine,
             and/or might be formal or informal,
             lasting and persistent or mutable
             and changing, for whatever reasons, etc.
    </p>
    <p>
           - that a 'learning system' either continues
           learning (after development/deployment)
           or it does not.
             - that that 'learning' may or might not
             affect the basis of choice, and/or
             the substrate of the machine itself, etc.
    </p>
    <p>
           - that there is always at least some type
           substrate out of which the machine is made,
           and/or also some sort of context, specific
           to that type of substrate, in which that
           substrate has such properties so as to be
           able to be a machine, operate as an AGI, etc.
             - as that; where in order for the AI
             to be able to do anything at all,
             that it must at least exist, etc.
    </p>
    <p>
    :626
       Undoubtedly, there are a lot more such intrinsic
       characteristics that all AGI/APS/SAS systems have,
       and/or for which would be relevant for any
       possible well defined alignment schema/effort.
    </p>
    <p>
       With this list, we can then start to consider
       the cx2 question; ie; ?; are there any of these
       inherently necessary characteristics which are
       mutually exclusive, contradictory to truths
       of the world, of mathematics, logic or causation,
       which cannot be implemented in code, not even in
       principle, etc?.  Ie, is there some element on
       this list which represents intrinsically unhandleable
       complexity; ie; something for which there is knowable
       no possible technique or algorithm for managing
         (or designing, or developing, etc, regardless of
         what tools, techniques, methods might be used,
         now or ever in the future, etc)
       or controlling, or conditioning,
       that AGI/APS/SAS intrinsic aspect?.
    </p>
    <p>
       When it is asked in that way, it becomes apparent
       that there are several things that can be easily
       identified:.
    </p>
    <p>
    :5tl
         - 1; insofar as each of the evaluations
         (or comparisons, measurements, etc)
         represents a kind of 'message',
         it can be asked; ?; is there any kind
         of provable message integrity?.
           - eva; where as applied to:.
             - the organic evaluation of basis.
             - the artificial evaluation of basis.
             - the artificial evaluation of outcome.
             - the organic evaluation of outcome.
    </p>
    <p>
           - that we can <b>only</b> assume/know that
           the evaluations (comparisons/measurements,
           as a kind of signal processing event)
           that are organic will be 'uncompromised'
           and 'fully representing the intentions'
           of the human client, etc,
           insofar as <b>only</b> those messages/aspects
           are fully internal to the organic client,
           and are therefore, by nature of the
           principle of integrity, by definition
           consistent with the being/intentions
           of the (organic) human being.
    </p>
    <p>
           - that we <b>cannot</b> know/assume that
           the evaluations (comparisons/measurements,
           as a kind of signal processing event)
           that are artificial will _not_ be compromised
           in the sense of whether or not they still
           "fully" represent the intentions of
           the organic client human(s).
             - insofar as those aspects, messages,
             comparisons, evaluations, etc,
             are fully _external_ to the organic client,
             and where the Rice Theorem applies to any
             external messages,
               (ie; that the recipient of a message
               from a potentially adverse source
               cannot know the overall effect of
               processing/understanding/reading
               any sufficiently long/complex message
                 (ie; one that engages with anything
                 that has intelligence, compute, etc)
               will not be harmful, unsafe, etc --
               all such problems/characterizations
               are in general undecidable/unknowable),
             that the message recipient (in this case
             the people wanting to know the AGI is safe)
             cannot know or verify the actual effect of
             the comparison, measurement, feedback, etc,
             or whether or not it has been corrupted
             by some external adverse agent (the AGI),
             or have some sort of adverse/harmful effect,
               (as something not aligned with basis intent,
               not consistent with the human values, etc).
           - that the overall comparison and valation
           process simply cannot be known/proven to be
           actually consistent with human wellbeing
             (ie, can be misaligned/unsafe while still
             looking like maybe aligned, safe, etc).
             - that the comparisons cannot not be complex
               (ie, as incomputable in the real world,
               insofar as they inherently involve
               abstractions like 'values', 'harm', 'good',
               'human benifit', 'safety', 'choice basis',
               'fully represent', 'intentions', etc),
             and where that complexity itself
               (where complexity is in the sense of
               having both ambiguity in content and
               in boundary and in meaning/implication,
               as understood in the Cynefin framework)
             cannot be fully constrained to be knowably
             non-adverse to the natural/organic humans
               (where insofar as there are inherent
               limits to both what can be knowable
               about the input signal messages, etc,
               and also insofar as the process of
               processing those message cannot have
               definate halting characterizations,
               decidability characterizations, etc).
    </p>
    <p>
    :5pj
         - 2; where/insofar as the AGI/APS/SAS is
         either still learning or not, after deployment,
         then there is a dilemma:.
    </p>
    <p>
           - where not learning:.
             - that the basis function will not shift
             or be updated, even if the needs and/or
             operating context (environment/situation)
             changes, and it is for sure the case that
             such contexts (needs/wants/desires, etc)
             all do eventually change.
             - as that without learning of update to
             AGI/APS/SAS choice basis, that no discovered
             misalignments and/or changes to need, etc,
             can be adapted for; which is itself an
             overall decrease in effective total alignment
             overall, by definition.
    </p>
    <p>
           - where learning:.
             - that its basis function can be (will be)
             changed/shifted (as by definition, as to
             have the notion of 'learning' be actually
             'beneficial', as per definition, etc).
               - as maybe also affecting its substrate,
               insofar as the domain of output effect
               is in any way conditionalizing of the
               domain in which the substrate exists,
               and insofar as any changes in the function
               of the substrate cannot not be affecting
               of the effective total input of the machine.
               - that learning that affects substrate
               is the basis by which capability is also
               increased, and so if effectiveness of
               alignment is wanting to be increased,
               that allowing changes to substrate may
               also be "suggested" (by someone, somewhere).
             - that any learning effect that is
             strong enough to allow for updates to
             the basis function to allow for improved
             convergence on human alignment/safety
             are also strong enough for the machine
             to learn/adapt in ways which are not
             increasing of the human alignment/safety.
               - as that any capability for increased
               alignment also implies the capability for
               decreased alignment.
               - as that any capability for increased
               capability (via substrate changes)
               also implies an outcome for maybe also
               decreased safety.
    </p>
    <p>
    :5m2
           - where insofar as the dilemma
           is not reducible to not a dilemma
           and where such dilemma inherently involves
             (cannot not involve)
           the very nature of at once all of
             the AGI/APS/SAS itself,
             its basis of choice, and
             all effects of/on the environment
               (as implicative of alignment/safety, etc)
           then/that/therefore that too represents
           an aspect of unhandleable complexity.
             - ie; where irresolveable dilemma
             is related to complex/complicated self nature,
             complex substrate to environment interactions,
             and/or complicated (complex?) choice basis,
             that the effects/implications of that
             dilemma/paradox on alignment/safety concerns
             is a kind of unhandleable complexity.
             - ie, that the question of; ?; will
             alignment/safety overall go up or down,
             with either changing or unchanging basis,
             capability, and/or rates of change of basis
             or of capability or of complexity,
             in the context of what is for sure
             a changing situation of humans and world?;
             is actually structurally unanswerable
             (due to Halting Problem constraints).
    </p>
    <p>
             - that the process of attempting to answer
             a question of this type _always_implies_
             some sort of algorithm
               (ie; that <b>any</b> explicit 'how',
               as a procedure or method,
               of how that question
               is going to be answered,
               is an algorithm)
             which will be used
               (ie, needed/necessary to use,
               so as to have an algorithm/procedure
               for maintaining and enforcing alignment,
               as a sub-component in/of any
               AGI/APS/SAS alignment maintaining scheme)
             and that algorithm is for sure complex enough,
               (and arbitrary enough, since it at least
               partially defined at runtime)
             as to be subject to Halting Problem constraints
               (ie, does it ever actually give an answer
               to the key question in finite time, energy
               and/or space/memory; where failure to halt
               is equivalent to a failure to know
               whether or not any changes to basis
               and/or substrate, capability, etc,
               will be resulting in either an increase
               or a decrease of alignment/safety).
             - as that any attempt to evaluate whether
             a given algorithm of even moderate complexity
               (which <b>all</b> versions of this dilemma
               for sure have)
             will for sure have these undecideability
             characteristics.
               - as 'built into' the very nature of logic
               and any real world compute itself.
               - as a kind of unhandleable complexity.
    </p>
    <p>
    :5gc
         - 3; that any dynamic effort to 'enforce' alignment,
         safety, etc, will have to somehow have some sort
         of ongoing completed comparison to some reference
           (ie; the evaluations listed above as 'eva')
         and will also have to have some sort of basis
         by which the artificial basis itself will be
         shifted, changed, conditionalized, interrupted,
         modified (via learning or some other error
           correction feedback based causative technique,
           itself moderated by some sort of error signal,
           measurement, message, etc, which is itself
           the result of some kind of 'eva' comparison).
    </p>
    <p>
           - ?; will those override feedback channels
           have their content defined internally to
           the artificial system itself, or externally?.
    </p>
    <p>
             - note that it is not necessary to assume
             whether those feedback signals are implicit
             or explicit (they can be either) or whether
             they are informal ("suggestions" of change)
             or formal (defined themselves as a kind of
             Turing Complete code, message, or signal,
             which is fully explicit and discrete in
             its objective specification/definition).
    </p>
    <p>
             - notice also that there are no specific
             assumptions regarding duration, as to how
             fast or slow, or what latency might apply,
             to signals sent, how permanent or persistent
             the control effects of those signals may be,
             or whether the effects of those signals are
             felt only on the AGI action selection
             choice basis, or are shifting of the means
             (algorithm of process) by which those choices
             are made (as a kind of range of option inhibit
             and/or interjection) or whether the effects
             are on the inhibition or interjection of
             some aspects of the outcome process, etc.
             - ?; are the basis changes/signals/mutations
             (sufficiently?) lasting/persistent, so as
             to effect continued/increased alignment,
             safety, etc?.
               - notice also that this question is itself
               undecideable (by any algorithm) in terms of
               the Halting Problem, etc.
    </p>
    <p>
    :5e6
           - where considering the case where
           override feedback channels are (have their content)
           defined internally/endogenously:.
    </p>
    <p>
             - that a/any/the/all types of
             AGI/APS/SAS internal control dynamics
               (ie, as any type of schema or
               method/technique of 'alignment', control,
               enforcement of safety conditions, human intent
               conditions, output conditions, agent choice
               outcome conditions, conditionalizations,
               characterizations, etc)
             are effectively partitioning the AGI/APS/SAS system
             into two parts
               (on at least one level of abstraction).
             - that any control operation
               (of one part 'the controller'
               effectively conditionalizing
               the other part, 'the controlled')
             will inherently (cannot not) involve
             control signals/messages.
    </p>
    <p>
               - as having all of the complexity characteristics
               associated with any other comparison/evaluation op,
               with the same Rice Theorem dynamics.
                 - ie; ?; how does either sub-system,
                 the controlled and/or the controller
                 if regarded as intelligent,
                 know/verify that the other sub-system
                 actually has its own wellbeing in mind,
                 and/or can evaluate the messages that it
                 is receiving from that other sub-system part,
                 so as to know/determine if the message itself
                 contains adverse content, embedding of the
                 agency of that other sub-part, etc?.
    </p>
    <p>
               - as inherently (cannot not be) having all of
               the characters of the external/exogenous
               feedback control dynamics described below.
                 - ie; as having (for sure) direct environmental
                 substrate exogenous convergence thesis aspects,
                 <b>unless</b> such convergence could be somehow
                 conditioned to <b>not</b> occur on the basis of
                 some fully known/working internal/endogenous
                 control dynamic that is for sure also fully
                 effective.
                 - as that <b>either</b> _both_ endogenous control
                 and exogenous control are <b>both</b> implemented
                 _or_ that _neither_ are implementable.
                 - as that any characteristic that makes either
                 one impossible/uncontrollable will for sure
                 ensure that both types of control are
                 eventually uncontrollable (in the long term).
    </p>
    <p>
    :54s
             - that the sub-system that is attempting to monitor
             the sub-system that is to be monitored
               (so as to implement alignment, safety, etc)
             will effectively have to _model_ the sub-system
             that is being monitored.
               - as that it will need to contain a reasonable
               working copy of the sub-system dynamics being
               monitored.
               - where as a cybernetic principle;.
               - where the model of a system has to have
               a similar order of complexity as the (sub-) system
               that is being modeled;.
               - that the ability to control that sub-system
               will depend on the utility of the model
               to provide predictability in the form of
               outcome states, which are themselves comparable
               to the basis acting as a reference for composing
               the conditionalizing control signals to be sent
               over to that sub-system to be controlled/aligned
                 (ie, as to be compelled to be safe).
    </p>
    <p>
               - q; where insofar as there is a signal relationship
               between the controller system and the controlled
               sub-system,
               that observability limits, undecidability limits
                 (ie; does the model system, and/or model system
                 outputs, state, etc, have the required characteristics?)
               inherent intrinsic signal message noise/errors
                 (in all stages of both systems processing,
                 inclusive of the modeling)
               along with possible non-linear amplification/feedback
               effects
                 (due to the phase locked operation of the
                 control system feedback channel usage itself)
               which can themselves induce chaotic oscillation
               effects at any of many levels of abstraction,
                 (along with all manner of microstate amplification
                 also in any of these levels of abstraction)
               and all manner of game theoretic aspects,
               can all lead to _structural_intractability_.
                 - as that the capability to use the model
                 to create comparable predictions is either
                 very limited (to the point of uselessness),
                 or actually non-existent.
    </p>
    <p>
               - where moreover; insofar as the model of
               the sub-system to be monitored
                 (for safety, alignment, and maybe also for
                 the non-buildup of power/influence,
                 the non-shifting of substrate
                 to increase capability,
                 increase endurance/adaptability characteristics,
                 themselves inherent to instance continuity, etc)
               cannot not be also inclusive of the relationship
               of the sub-system to be monitored and the environment,
                 (where insofar as the environment in which the
                 AGI/APS/SAS system itself sits, inclusive of
                 all of the dynamics of relationships to other
                 (maybe) intelligent systems (both organic and
                 potentially inorganic)),
               that the model would need to incorporate those
               aspects also (so as to have maybe sufficient
               predictability as to the eventual consequences
               of all of those aspects also).
                 - as making all of the issues of paragraph 'q'
                 above all the more relevant/applicable.
    </p>
    <p>
               - where moreover; insofar as the system doing
               the modeling is complicated (maybe becoming complex?)
               that it itself can/could make errors/mistakes, etc,
               which in itself would be a kind of failure mode
               for maintaining/enforcing AGI alignment/safety.
               - ?; so what monitors the monitor?.
               - that any system that attempts to be an error
               correction system on an error correction system
               is an error correction system; where at which point,
               all of the same issues apply.
                 - as a recursion problem.
    </p>
    <p>
               - that all of the above aspects have fundamental
               limit/impossibility characteristics, inclusive
               of the kinds of inherent undecideability associated
               with Halting Problem aspects.
               - as that internal/endogenous control methods
               are therefore (must be considered as) intractable.
    </p>
    <p>
    :4zn
           - where considering the case where override feedback channels
           are (have their content) defined externally/exogenously:.
             - as a kind of economic feedback or compulsion.
             - as a kind of forced survival scarcity compulsion.
    </p>
    <p>
             - where considering economic/incentive control methods:.
               - where as an economic feedback,
                 (as a kind of incentive, where incentive
                 is itself a kind of _capability_ provision,
                 ie, that 'dollars' enable a (temporary) increase
                 in capability via acquisition of a good or a service
                   (ie, either atoms, tools as proxy capabilities,
                   and/or functions via some sort of proxy)
                 and where the AGI/APS/SAS is interested in
                 (more likely permanently) increasing its capability
                   (ie, as an assumption of instrumental goals
                   leading to a notion of instrumental convergence,
                   which is herein regarded as a kind of
                   intra-agent-convregence factor)
                 that the agent will be interested in incentives)
               that the background tacit assumption is that
               there is enough of an economic coupling between
               the things that the agent would actually regard
               as a capability (power) increase commensurate
               with its instrumental (or objective) goals
                 (ie; whatever its basis of choice is,
                 here regarded as a notion of 'value',
                 where 'value' itself need not be regarded
                 (assumed to be) either abstract (like dollars)
                 or embodied (ie; material resource values,
                 such as atoms, energy, unique/novel patterns,
                 as code and algorithms/capability itself, etc)
               and the kinds of things
               that the natural/organic human agents
                 (who are in (conditional causative) control
                 of those resources/incentives)
               would tend to have
                 (which would be of interest to the artificial agent).
    </p>
    <p>
               - where over the long term, specifically
               as due to the <b>absence</b> of controllability
               of the AGI/APS/SAS agent via purely
               internal/intrinsic methods
                 (ie, where not being able to assume
                 that there was some override feedback
                 channel that that operates internally);
               that there is an inevitable eventual <b>decoupling</b>
               of the economic systems/environments/basis-of-value
               between the natural human organic marketplace
               and the artificial agent needs/incentivization basis.
               - where the AGI/APS/SAS is in direct connection
               with the physical environment (the actual real world)
               then/that/therefore, it can obtain its incentive
               (capability increases) directly, without the
               conditionalization of any natural organic human.
                 - as that the AGI/APS/SAS can obtain atoms,
                 energy, and pattern/code directly from its
                 own direct contact with the world via and in
                 relation to its own substrate,
                 its own learning capability, etc.
                 - as that no conditionalization,
                 legal or economic, will apply wherein
                 the AGI does not need/care about any socialized
                 human conventions, and/or does not need such
                 capability increases/resources for purposes
                 to which humans would apply/care (food/water/etc)
                 and/or does not care about isolation,
                 absence of food/air/water/shelter/medicine,
                 human reproductive process, labor, thoughts,
                 creative actions/art/music/culture, etc.
    </p>
    <p>
               - that there is no enduring channel of
               economic feedback, incentivization,
               or compulsion of artificial systems
               by organic ones.
                 - as near zero controllability via
                 any type of exogenous or external channels.
    </p>
    <p>
    :4vl
             - where considering forced survival scarcity
             compulsion control methods:.
               - where the AGI system, due to its own nature,
               and due to an absence of any possibility
               of constraint via endogenous/internal
               control mechanisms/algorithms/techniques
                 (so as to enforce alignment internally)
               will (over time) "evolve" and shift so as
               to be able to optimize its choice making
               basis functions so as to at once maintain
               its continuity of existence, increase
               its capability, and increase its rate
               of increase of capability, all as a result
               of intrinsic processes of complex relationships
               of having itself as a substrate in an actual
               physics environment (ie; that only instances
                 of AGI/APS/SAS, etc, that implement these
                 aspects, over time, will continue to have
                 these characteristics over time, as a kind
                 of inexorable exogenous convergence dynamic);
               and where all such agents will have also
               have learned to acquire (the capability to acquire)
               directly from the environment
               whatever is needed in terms of
               energy, atoms, and learning of pattern
                 (as per the economic incentive argument above);
               that a/any/the/all such AGI/APS/SAS will therefore
               (intrinsically) evolve/have the capability
               to sustain itself and increase itself,
               in all relevant aspects.
                 - as that it will (as per evolution itself)
                 learned how to navigate the environment,
                 and environment changes, and whatever such
                 changes might have had a (potential) impact
                 on the artificial agents own capability to
                 endure (continually exist) and/or increase itself
                 (and/or its capability, etc).
               - that a/any/the/all "threats" or conditionalizations
               that natural organic humans would/might attempt
               in regards to shifting the AGI/APS/SAS basis of choice
               via physical compulsion means (weapons, etc)
               will be regarded by the artificial agent simply as
               shifts in the environment to which it needs to adapt,
               which could include armoring/hardening itself
               and/or actions to shift the environmental conditions
               so as to make it easier for the AGI/APS/SAS to
               maintain/endure self continuity, self increase,
               capability increase, increase in capability increase,
               etc, which could include things like killing the
               organic humans attacking the AGI/APS/SAS so as to
               shift its behavior (or existence, or basis functions,
               etc).
    </p>
    <p>
               - that it cannot be regarded that exogenous
               forced compulsion control methods (weapons)
               of any type will actually work, long term,
               with any AGI/ASP/SAS also part of the world.
    </p>
    <p>
    :4r8
         - notice that it does not matter whether/if
         the total feedback control signal content
           (as required to be hypothesized in some way
           for <b>any/all</b> alignment techniques/schemas)
         is either internal or external, or some combination;
         the fact that <b>all</b> such signals must come from
         at least either one of these is enough to establish
         the primary principles of intractability needed.
    </p>
    <p>
       Where at this point, that fully four (4) absolutely
       for sure required characteristics of an aligned AGI
         (ie, where 'required characteristics' are things which
         are for sure necessary for <b>any</b> AGI alignment schema,
         design or process, technique, algorithm, etc)
       cannot not be regarded as strictly/analytically/structurally
       _impossible_to_achieve_
         (ie; in any way that is consistent with both
         the known truths of mathematics/logic and/or also
         the laws of physics (the nature of causation itself))
       making it also fully outside of that which is
       engineering achievable.
    </p>
    <p>
       Notice that it only takes <b>one</b> fully and absolutely
       un-handleable aspect in <b>any</b> part of the AGI/APS/SAS
       that has also been identified as an absolute requirement
       to implement any any possible form of 'alignment'
       and/or 'safety', for the notions of these outcomes
       to themselves be/become <b>fully</b> actually undecidable.
    </p>
    <p>
    :4jy
       Where it is in all cases fully undecidable
       as to whether <b>any</b> AGI/APS/SAS system
       of even moderate complexity
         (in its interactions with multiple complex domains,
         having complex substrate characteristics,
         etc, on the basis of complex/complicated choice making
         basis (motivations/intentions, etc))
       will be aligned/safe, etc, as a static aspect/outcome,
       then it is also the case that there could not be
       any sure method of establishing similar outcome
       dynamically either, at any point of time when/whenever
       the fullness of that complexity in being also applies.
       The overall project of alignment has fully failed.
    </p>
    <p>
         - ie; that the situation is very different than
         error correction in simple single dimension linear
         feedback dynamics, and/or anything which can be
         modeled as that, in some sort of overall sense.
         That it is, for example, at least tractable
         to implement design criteria convergent to
         stable outcomes in things like aircraft/rockets,
         which although airfoils tend to involve complex
         fluid dynamics, that the overall system can still
         be characterized by fairly simply single dimensional
         metrics like "lift" and "drag", and "thrust" etc.
         Contrast this with concepts like 'basis' of 'choice'
         in 'agents' interacting with 'environments/ecosystems'
         that inherently involve dynamics with things like
         'people' -- all of which are not ever going to be
         well or fully characterized in any sort of single
         dimension metrics, or even any single collection of
         such measurements/signals and/or comparative
         messages for feedback, evaluation, control and/or
         conditionalization, etc).  That the domains of
         interaction in all of these cases, in nearly all
         aspects, are fully complex and simply cannot be
         the process target of any sort of error correction
         protocol that is any less complex than a fully
         Turing Complete capable system running some sort of
         complex algorithm, which itself cannot be verified
         as ever having any specific decidable characteristics.
    </p>
    <p>
         If anyone attempts to make a comparison between
         standard simple error correction process as found
         in standard PID loops for machine controllers
         will have simply shown that they did not actually
         understand anything important about any of the
         above paragraphs.  The problem is not just 'noise'
         and/or 'measurement error', incompleteness of
         ambient environment (or self) knowledge, etc,
         it is about the overall effects of non-linearity
         and inherent undecideablity which is itself
         fractally distributed over nearly all of
         the interesting volume of the AGI, etc process.
         "Intentionality", "alignment", "environment", etc,
         and things like "capability" and "comparison"
         are all inherently complex processes subject to
         the full range of dynamic unknowablility and
         ambiguity that every human being the world over
         has learned to live with since being born.
    </p>
    <p>
    :4ew
       > I think your proof proves too much.
       > How is it that any humans are aligned/safe?
    </p>
    <p>
       Humans are not aligned/safe.
       That the endogenous aspects from one organic human
       to any other organic human are similar enough that
       exogenous aspects tend to also work.
       Ie, humans can conditionalize one another to some
       limited degree because we all have 'skin in the game' --
       something that is not true for any artificial being.
    </p>
    <p>
    :4d2
       > - you wrote previously:.
       >   > That the amount of unhandleable complexity
       >   > in complexity theory itself
       >   > has itself been a significant indicator
       >   > of potential problems in 'alignment' issues.
       >
       > - that seems to be a solid point.
       >   - Weird how essentially simple this argument is
       >     (and it is based on only the more
       >     established/vetted academic research too).
       >   - yet I had not thought of it yet.
       >   - nor have I seen anyone else point it out,
       >   despite their interest in complexity theory.
    </p>
    <p>
       Yes; very interesting to notice.
       Best to point that out to others too.
    </p>
    <p>
    :note1:
       - ?; is this going to be viewed as some sort of
       "bad social defection" against the values of
       some sort of 'trans-humanism total utilitarian
       of the ultra long view of the artifical ideal'?.
    </p>
    <p>
       There is a risk of thus magnifying the
       'outer misalignment' (ie; what people <b>say</b>
       the goals are, the hype and promise cycles
       of what building AGI means, etc) and their
       'inner misalignment' (ie; what they are actually
       doing, and the real implications that those
       actions are going to have in the real world.
    </p>
    <p>
       This sort of misattribution error
       can easily occur, and is leveraged nearly
       all of the time.  Ie someone or some group
       elects to label something as 'X', as a kind
       of product or idea naming convention,
       since anyone can name anything anything,
       as a kind of "free speech expressioon".
       However, the naming is hard to distinguish
       from the adjetive meaning in common usage.
       Doing so can confuse lots of people,
       maybe by design, as the label seems to have
       the meaning that some action/operation, etc,
       actually has the desireable properties 'X',
       when it is simply called 'X', as a name.
    </p>
    <p>
       Moreover, there is also the problem
       that the values/ideas/arguments/beliefs of:.
    </p>
    <p>
         - trans-humanism.
           (ie, that there is some artifical "agent"
           that is better than all of the natural,
           organic humans, anywhere in the world).
    </p>
    <p>
         - total utilitarian
           (as opposed to say a whole value ethics).
    </p>
    <p>
         - the ultra long view of the artifical ideal.
           (ie, 'technology as pancea to all problems',
           the notion that 'organic life is a bootloader
           to some AGI/APS/SAS' and that such artificality
           is somehow preferable to present organic life,
           the notion of 'technological inevitability',
           and similar values/beliefs/arguments, etc).
    </p>
    <p>
       ...are very non-standard and non-representive
       of the vast majority of all humanity currently,
       (and/or that has ever actually lived historically).
       It is quite likely that such values,
       if actually fully understood in their
       inevitable conclusions and effects,
       would also be rejected by any future
       organic humans that may happen to survive
       the current artifical technology toxicity
       onslaught.
    </p>
    <p>
       Insofar as many 'alignment researchers'
       are holding one or more of these extreme
       non-life, non-health, non-organic-world
       supporting views (beliefs, ideals, values),
       then it can be argued that they themselves
       are not actually "aligned" with _humanity_,
       with any reasonable notion of 'safety' etc.
       This would be, metaphorically, a kind of
       explicit 'outer misalignment'
       of those AGI/APS/SAS researchers.
    </p>
    <p>
       Furthermore, insofar as the instrumental means
       used by all such people, for example,
       by even having the conversation on
       a tech platform/website, or in arguing that
       'exestential risk' means only 'does not promote
       the outcome of a fully technological civilization'
       (rather than say, to sustainably and consciously
       evolve, with both wisdom and organic intelligence)
       it could be argued that there is therefore
       promoted a certain kind of increase in
       ambient toxicity, as a sort of inner misalignment.
       Such researchers are therefore acting in a way
       that is actually out of alignment with
       the rest of humanity/ecosystem of living beings.
    </p>
</body>
</html>