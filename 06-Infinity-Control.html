<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Basic Meta Tags -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- SEO Meta Tags -->
  <meta name="description" content="Comprehensive AGI Risk Analysis">
  <meta name="keywords" content="agi, risk, convergence">
  <meta name="author" content="Forrest Landry">
  <meta name="robots" content="index, follow">

  <!-- Favicon -->
  <link rel="icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">
  <link rel="shortcut icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">

  <!-- Page Title (displayed on the browser tab) -->
  <title>Comprehensive AGI Risk Analysis</title>
</head>
<body>
   <p>
   TITL:
      <b>Infinity Control</b>
      By Forrest Landry
      Oct 26th, 2022.
   </p>
   <p>
   ABST:
      As an example of an applied analysis,
      via the use of EGS text reforming technique,
      of a complex implied hypothetical question,
      so as to enable a real response to same.
   </p>
   <p>
      As a demonstration of the proper way
      to process arguments of principle,
      particularly where they are extending into
      spaces which are real engineering unreachable,
      but nonetheless which are important to consider
      for reasons of clarifying the methods
      of proving abstract principles still valid.
   </p>
   <p>
   TEXT:
   </p>
   <p>
      Where the following paragraph was found
      in an email from a colleague;.
   </p>
   <p>
      > If, hypothetically, there was a separate
      > (located in a 'metaphysically decoupled' realm)
      > adaptable control system
      > running on infinite hardware
      > with infinite learning compute
      >   (and complete measurement
      >   and internals-modification capacity),
      > then that separate control system <b>could</b>
      > fully simulate/detect as well as prevent/correct
      > the co-option by internal code variants
      > of AGI's directed functionality,
      > as shifting away outcomes
      > from those originally selected for
      > through optimization methods by humans
      > (eg; AI R&D labs with certain intents).
   </p>
   <p>
      The above was felt (by this author)
      to be a bit ambiguously stated.
      It was not entirely clear how to respond,
      insofar as there is a lot going on, and referring
      to the required clauses did not seem at all easy.
      As such, it needed to be "translated" into
      something that had an equivalent form,
      and which was also (obviously) question,
      and a much more clearly identifiable structure.
   </p>
   <p>
      The EGS 'conversion' of the above text
      resulted in the following structurally equivalent,
      expanded, and isomorphic form (@ 1 #note1);.
   </p>
   <p>
   :cpl
      > - where/what-if hypothetically we assume
      >   (as ?; could it be the case)
      >   (where as if located in a
      >   'metaphysically decoupled' realm)
      > that there was/is
      > a fully separate control system, a "QCX9000",
      > which is:.
      >   - 1; fully adaptable (human programmable).
      >   - 2; running on infinite hardware with:.
      >     - 3; perfected/complete measurement capacity.
      >       - ie, where assuming no Shannon entropy,
      >       no message or signal noise at all.
      >     - 4; perfected internals-modification capacity.
      >     - 5; infinite learning compute.
      >
      > - C; then/therefore that the separate
      > QCX9000 control system could potentially
      > (at least in principle, conceptually)
      > be used to:.
      >   - 6; fully simulate and/or detect
      >   all future AGI/APS states;
      >   as well as;
      >   - 7; prevent/correct the/any co-option/shifting
      >   of the AGI action outcomes:.
      >     - which are away from:.
      >       - 8; the previously selected abstract
      >       human designer/builder intentions
      >       and/or the originally intended (hoped for)
      >       AGI usage/operational outcomes (@ 5 #note5).
      >
      > - A; where that 'shifting of outcomes' in ^7
      > <b>maybe</b> occurs via (the use of)
      > internal or external code variants.
      > - B; where those outcomes ^7
      > were (maybe) originally selected for (trained)
      >   (and implicitly encoded within
      >   the AGI/aps system)
      > via some previously implemented/completed
      > training/optimization methods/processes.
      >   - where those training/optimizations/methods:.
      >     - implicitly encode the intentions of ^8.
      >     - where implemented previously by those
      >     AGI/APS designer/developer humans.
      >       (working in and using
      >       the capabilities of R&D labs
      >       themselves constructed
      >       specifically for that purpose
      >       prior to the initial/original specific
      >       AGI/APS build).
      >   - as assuming/presuming that those humans
      >   (the developers) themselves actually
      >   had the/those abstract intentions
      >   as listed considered/referenced in ^8.
   </p>
   <p>
      Some details have been added, others clarified,
      and reference marks have also been included --
      all so that the overall form of the argument
      can be made both more complete as well as
      more explicit (unambiguous in interpretation).
   </p>
   <p>
      I also gave the assumed/hypothesized machine
      a cute marketable name, the "QCX9000" (tm) (c),
      for convenience of reference (@ 2 #note2).
   </p>
   <p>
      For the purposes of this essay, and because
      this is an example of applied technique,
      it will be assumed that there was some
      confirmed indication/assent of equivalence
      (of structural/functional meaning sameness)
      has actually been obtained and recorded
      by the origional input text author.
   </p>
   <p>
   :s7g
      Where in regards to the above translated text,
      the overall question is in the form:
   </p>
   <p>
         > "where/if we assume condition(s) 'X';
         > ?; does/do condition(s) 'Z'
         > also become possible?".
   </p>
   <p>
      That the main thing (usually) to notice
      is if something about X
      is actually structurally unreasonable.
      If X were to be defined as "lets assume 2/0",
      for example, then the question statement
      becomes meaningless.
   </p>
   <p>
      If there is a divide by (@ zero https://math.hmc.edu/funfacts/one-equals-zero/) error,
      hidden and/or implicit somewhere
      in a question, or "proof", etc,
      then <b>any</b> arbitrary conclusion
      can be constructed eventually.
        (in the comp-sci literature,
        due to a presentation joke,
        these are often referred to
        as "nasal demons" (@ cite https://www.urbandictionary.com/define.php?term=nasal%20demons)).
      Where from any basic contradiction,
      by argument, one can obtain results like
      "1 equals 2", and from there
      that "1" equals any other number,
      or that any number equals any other number,
      or that any of them are infinity, etc.
      Absurdity inherently has no limit.
   </p>
   <p>
      In this case, 'X' consists of the clauses ^1 thru ^5,
      which specifies fully <b>three</b> distinct
      kinds of infinity:.
        - unbounded energy.
        - unbounded detail in sensory capability.
        - unbounded detail and energy
        of output/modify/affect.
      This is roughly the equivalent of
      a 'divide by zero' error assert
      three times over.
   </p>
   <p>
      Then, moreover, there is the assumption,
      via ^5, of 'infinite learning'.
      Via this, there is also the assumption,
      also implied, that the QCX9000
      will come to eventually "know fully"
      the totality of all human good intention
      (benefit) at <b>all</b> levels of abstraction,
      and then finally,
      that somehow the assumption of ^1
      could also be applied to ensure
      that the QCX9000 will <b>want_to</b>
      actually apply its complete knowledge
      of 'what is human benefit' and also of
      'how to implement total human benefit'.
   </p>
   <p>
      In other words, there is an assumption
      that some human who is also in position
      to effect such choices, will have the skill,
      via some correct application of
      the assumption of ^1,
      to be able to somehow 'get' or 'convince'
      the QCX9000 to apply their available
      understandings and capabilities to actually
      be in the service of humans and human welfare.
      Ie, the QCX9000 may know,
      at all relevant levels of abstraction,
      what goodness for humans is,
      and also know how to implement that,
      and yet have no desire or motivation
      to actually implement any actions
      to/towards human benefit.
      In fact, there is no reason to assume that
      the QCX9000 might not also use its <b>also</b>
      perfected knowledge of how to cause harm
      to humans so as to advantage itself.
   </p>
   <p>
      Hence that person (the developer)
      who has 'programming control'
      will actually need to be very quite skillful
      indeed so as to be able to somehow 'shift'
      the intentionality of the control system,
      with its infinite learning -- thus seeing
      completely through any attempt of that human
      to 'manipulate' that control system in ways
      that it would not understand (it cannot be
      deceived or tricked).  Hence, while we can
      assume that the ^1 assumption provides for
      a means to <b>compel</b> the QCX9000 to have
      the necessary desires, it is not therefore
      also clear that the human, operating at
      their very limited level
      of cogitative abstraction
      to have the raw skill necessary
      to actually issue the necessary instructions
      so as to have that user intended outcome
      (in the control system, the QCX9000,
      as distinct from the AGI/APS system
      that is to be/become aligned
      by the actions of the control system).
   </p>
   <p>
      Notice that the net effect
      is to have the relationship between
      the client human and the QCX9000
      be actually structurally equivalent
      to the relationship that that client human
      (or any other client human)
      has directly to the AGI/APS system
      that is to be 'aligned' (@ 6 #note6).
      In other words, there is an implied fallacy
      of simply displacing the same problem from
      one structural level of consideration
      to another, without ever actually solving
      the core problem
      or answering the core question:
      ?; is it <b>ever</b> the case
      that human beings,
      with just their native intelligence,
      will <b>ever</b> be able to force or compel
      <b>any</b> superintelligence to have any of,
      or even any aspects of, the necessary desires,
      to care, to have the willingness
      to apply their hyper skill
      to the benefit of humans?.
   </p>
   <p>
      Human programmers may be rather smart,
      but I do not think that any programmer ever
      will be sufficiently smart to provide such
      instruction so flawlessly as to get the
      intended effects (AGI/APS machine care)
      regardless of how many other constraints
      are otherwise also <b>completely</b> removed.
      (@ 4 #note4)
   </p>
   <p>
      On this reason alone, we would/should
      be able to completely reject
      the overall form:.
   </p>
   <p>
         > - where given conditions ^1 through ^5
         > (as 'X');
         > ?; can we therefore also assume
         > that the necessary condition(s)
         > have been obtained
         > so as to have outcome 'Z'?".
   </p>
   <p>
      In short, no:
      There was needed an additional condition,
      as some sort of infinite 'convincingness'
      on the part of a human programmer
      so as to be able to 'convince' or 'require'
      the (QCX9000 as an AGI/APS in itself)
      to have the willingness to implement care.
   </p>
   <p>
         > How can a strictly finite intelligence
         > ever 'convince' an infinite intelligence
         > to have any specific desire/care 'D'?.
   </p>
   <p>
      <b>Even if</b> we were to <b>also</b> assume
      some sort of 'perfected compulsion'
      on the part of the finite intelligence
      onto/over the infinite intelligence;
      that does <b>not</b> in itself suggest that
      notion of 'having a desire' and 'a care'
      is itself a concept without any structure.
      The emotions/feelings/will/choice to have
      or <b>be</b> that sort of thing/creature/mind
      that "has" such care/desire, is also to
      suggest something of near infinite detail,
      when considering also it being in/within
      an infinite intelligence context.
      It is simply the case that the finite mind
      simply cannot fully specify all of the
      information that would be needed so as to
      instruct the being of the infinite mind
      as to how to be the kind of thing with
      those specific desires.
   </p>
   <p>
      Moreover, attempting to circumvent the
      inherent need for that level of design
      by asking the QCX9000 to modify itself
      is not to have created the desire/will
      <b>also</b> at the level of needed detail
      for the desire/will to create desire/will
      itself to be correctly fully specified.
      Again, there is an infinite regress of
      assuming what is wanted to be proved.
      And then the "he who pays the piper
      must know the tune" (@ aspects https://sinceriously.fyi/wp-content/uploads/2019/07/He-Who-Pays-The-Piper-Must-Know-The-Tune.pdf) also apply.
   </p>
   <p>
   :qlj
      So as to continue anyway, lets say that
      we 'upgraded' the structure of the question
      so as to somehow also assume an additional
      upgrade to condition '1' so that it assumes
      that this specific QCX9000 instance loves
      people, humanity, etc, at all levels of
      abstraction (and hence we no longer need
      to worry about the skill
      of human programmers).
   </p>
   <p>
      Is this enough to ensure that the question
      form of "where assuming condition 'X';
      do we also therefore get condition 'Z'?"
      will result in a positive assent?.
      No, unfortunately not.
      Even with all of that, with so many assumed
      infinities and conditions of improbability,
      it is <b>still</b> no enough to ensure alignment.
   </p>
   <p>
      The basic problem is the possibility of
      the combination of aspects ^6 and ^A;
      that there might be some _external_ code
      that is adversely designed, ie, has some
      <b>other</b> intention, than human welfare,
      and designed so well, say by some sort
      of anti-QCX9000 agent/agency/intelligence,
      so as to be perfectly designed so as to
      maybe deceive, or at least out-think the
      detection/simulation capability of ^6.
   </p>
   <p>
      In other words, <b>if</b> we are permitted to
      assume the existence of a fully capable
      QCX9000 once, <b>then</b> that _same_ assumption
      can be made again, maybe with different
      (less) partial composition/assumptions,
      each time.  Ie, rather than assuming some
      modified ^1 that implies human love,
      that we simply omit aspect ^1, which
      by assuming less, means that it is more
      likely that new anti-human-love "QCX9501"
      could also be concurrently created/used.
   </p>
   <p>
      That this hateful alternative, the QCX9501,
      having also its own versions of ^6 and ^7
      (excepting that the intentions/outcomes
      are different, towards human harm, etc),
      so that QCX9501 constructs a specially
      crafted program, to inject an 'external' ^A
      such that the prior love-QCX9000 would
      have its ^6 capability suborned.
   </p>
   <p>
         > - ?; can an infinite superintelligence
         > with perfected deception capabilities
         > (via ^2 through ^7, all inclusive)
         > deceive another infinite superintelligence
         > that also has perfected anti-deception
         > capabilities (also via ^2 through ^7,
         > again and all inclusive)?.
   </p>
   <p>
      As soon as this latter question form is
      stated, it should be obvious that the
      only correct analytic response is "oops" --
      we have entered into the 'how many Angels
      dancing on the head of a pin?' territory.
      The misuse of infinity operators has lead
      to a strong structural contradiction in
      the equivalent formulation of the question,
      as irresolvable, which means that the
      internal inconsistency in condition 'Y'
      in itself fully prevents its principled
      manifestation, regardless of the level
      of attainment of the "prior" X condition.
   </p>
   <p>
   :4jy
      > Can anything be salvaged?.
   </p>
   <p>
      No, not really.  The basic problem
      is inherent in each of the infinities
      associated with ^2 through ^6 --
      that even one of these by itself
      would be inherently vulnerable to the
      anti-replicant argument just applied.
   </p>
   <p>
      Any attempt to preserve some
      non-infinity aspect of ^7 by using
      anything less than at least multiple
      other infinities does not have
      sufficient power to actually effect ^7,
      and yet actually assuming such aspects
      ensures that that assumption in itself
      enables self adverse weaponization.
      So the net result remains a total loss.
   </p>
   <p>
   :8t8
      Moreover, even if we were to somehow
      overcome such a powerful objection,
      (extending beyond the beyond, as it were)
      there is even a next order of problems
      still unaddressed: how to ensure that
      there is a capability to compare the
      output of the future simulation output
      states, at the concrete manifestation
      object level of abstraction with the
      (maybe near) (perfected) knowledge
      of what is good/beneficial for humans,
      which is a concept at near infinite
      abstraction.  The basic problem is
      that the comparison itself is a kind
      of infinity problem -- comparing
      incomparables, apples and oranges,
      at completely different levels of
      abstraction, nearly perfectly removed.
   </p>
   <p>
      Hence, the very meaning of the question,
      overall as described in aspect ^6,
      is fully unclear, and thus cannot
      advise aspect ^7 despite the aspect
      of ^5 maybe also being maybe assumed.
      Only things that are _knowable_
      are also learnable, and there is
      an unanswered question as to how
      such a comparison would be made,
      even in principle, that is <b>not</b>
      addressable by simply assuming
      infinite intelligence/learning.
      Things which are impossible in principle
      will probably remain that, especially
      in exercises of principled analysis.
   </p>
   <p>
      QED.
   </p>
   <p>
   :note1:
      This EGS conversion was particularly complex.
      It is not usually the case that there are so
      many cross entangled sub-clauses in a text.
      It was the fact of the complexity that made this
      instance a reasonably good example for why
      EGS text conversion, generally,
      is a valuable technique.
   </p>
   <p>
   :note2:
      Usually, at this point, there is a need
      to formally establish a required form of
      interpersonal conversational coherence.
      Is client/querent reasonable and/or
      rationalional
        (ie, as also inclusive of a background
        assessment of the client orientation
        to mistake theory or conflict theory,
        their overall social action intentions,
        etc).
   </p>
   <p>
      For an author, it is important to 1st
      present the restructured EGS text,
      and then 2nd get a positive and explicit
      confirm,
        (usually as witnessed and recorded,
        in a generally socially/publicly
        legible/observed forum, etc)
      <b>before</b> any additional analysis and response
      is even attempted.
   </p>
   <p>
      The need is to get a <b>confirmed</b> reply
        (best if it is in public available writing)
      that the re-statement
      is actually equivalent to, and inclusive of,
      all intended (and maybe only implied)
      aspects of the prior exact given
      paragraph language.
   </p>
   <p>
      That this is usually implemented by
      <b>synchronously</b> providing the above rewrite,
      and then, also synchronously,
      asking the following questions:.
   </p>
   <p>
        - ?; is the re-write recognized by you
        as an 'upgrade'?.
   </p>
   <p>
        - ?; is it inclusive and clarifying
        and generalizing of what you had originally
        intended to ask?.
   </p>
   <p>
        - ?; is it subsumptive of your overall
        question concerns?.
   </p>
   <p>
      Where the client/querent assents and confirms,
      then the analysis can proceed (note 3 below).
   </p>
   <p>
      Where the client/querent does not assent,
      then the client/querent is obligated
      to provide clarifications as to the details
      that they regard are not, or were not,
      extending/generalizing and/or clarifying
      of the original meaning of the paragraph
      as <b>exactly</b> stated.
        (note; later provisions cannot be tacitly
        allowed to be added/injected by the querent,
        else the questions of querent social intentions
        will also need to be raised).
      When whatever re-writes and adjustments,
      still in EGS form, are completed to the point
      that the client/querent holds that the
      complete generalization of their input paragraph
      has been fully reified; then the analysis of
      the now confirmed equivalent EGS text
      can proceed.
   </p>
   <p>
      Partly the reason for this level of process
      is to both assess the level of reasonableness
      of the querent (are they acting politically
      in the sense of conflict theory, or not?).
      It is also to ensure that they cannot pre-
      review the analysis outcome, which if not
      desired (or willing to be accepted by them)
      that they would attempt to retrospectively
      shift the input text so as to try to get
      some sort of different analysis outcome --
      one more <b>seemingly</b> (though falsely)
      favorable to their pre-presumed position.
   </p>
   <p>
   :note3:
      Where in common practice, that such explicit
      call and response transactional dynamics are
      usually implicit in a synchronous comm channel.
      However, in the cases where much higher levels
      of situational severity are involved (x-risk)
      there can more often be an explicit need
      to implement the necessary level of technical
      formality in a written manner, both because
      the analysis itself demands it, due to need
      for references into complex structures,
      and/or also as due to 'open' particularly
      deeply obscured aspects of the argument,
      particularly where such aspects are actually
      crux aspects of the overall argument.
   </p>
   <p>
      That many complex boolean type questions
      with lots of linked compound operators
      and factors can be easily shifted to the
      complete discrete exact opposite state
      simply by a single subtle reversal deep
      within the overall logic structure.
      Hence, to maintain sanity within such
      compound statements, it is important to
      actually have explicit tracking of each
      determining aspect.
   </p>
   <p>
      These aspects also inherently involve a
      consideration of the degree to which
      synchronous high-bandwidth interaction
      (as human to human in person conversational
      debate, meeting in larger observer groups, etc)
      can be combined with the asynchronous
      low bandwidth ordered exchanges of text
      can be mixed together.
      Usually the action of intermixing <b>any</b>
      high-bandwidth synchronous protocol
      will have the side effect of 'swamping'
      (making completely situationally irrelevant)
      anything that may be happening in the lower
      bandwidth channels, regardless of how
      concurrent they may be.
      This group-choice-process aspect continues
      to receive a lot of research attention
      insofar as it is a gating issue in the
      overall design of the governance stack process.
   </p>
   <p>
   :note4:
      The outcome which left the crux question
      remaining unanswered and unanswerable
      is the very reason that this particular
      paragraph was expanded into this example essay.
      Normally, it would not be relevant to explore
      the implications of a given question/argument
      'stated in principle', if there were <b>any</b>
      irrational infinity conditions, errors, etc.
      However, not being willing to explore such
      impossible hypotheticals would not provide
      the means to actually analyze crux issues
      of exactly this type, since they only become
      available and evident as crux <b>if</b> and <b>only</b> if,
      some actual disciplined manner of factoring out
      and neutralizing the effects of all of
      the other infinities/errors in the argument
      and/or question form.
   </p>
   <p>
      That being able to handle infinities/errors
      in the hypotheticals in a well structured way
      is the very point of having functional
      isomorphism tools, like EGS, cleanly managing
      multiple levels of indirection and reference,
      and the IDM metaphysics tools/concepts, etc.
      With these sorts of resources, it becomes
      possible to manage structured conversational
      process around high intensity x-risk topics
      <b>without</b> getting lost in endless details,
      distractions, and multiple covert degrees
      of intentional political disruption.
      Rhetoric, confusion, FUD, emotion, etc,
      are all not as helpful as what is, at core,
      absolutely needed to resolve.
   </p>
   <p>
   :note5:
      The notion of 'human intention' also includes
      (must, cannot not ) implicit intentions of
      things like "for humans to survive"
      which in turn means things like
      "humans in a world (healthy planet)
      that is at least natural enough
      to produce things like food, etc".
   </p>
   <p>
      If the environmental conditions of the planet
      were to shift/change towards artificiality
      to the point that maintaining ecosystems fails,
      then the broader context for human survival
      also fails, and thus these sorts of larger,
      more abstract, global shifts must also be
      accounted for (ultimately) by the QCX9000.
   </p>
   <p>
      Therefore, the concept of 'intentionality'
      inherently includes an implicit alignment goal
      of having a/the hypothetical system
      (the QCX9000) that could also 'correct out'
      selection for AGI/APS code variants/internals
      that have the effect of changing the conditions
      of the environment in ways which,
      while result in their own (the AGI/APS)
      continued existence, (ie; the API/APS endure),
      that the humans do not (the world is broken).
   </p>
   <p>
   :note6:
      There is in this also, inherently,
      a kind of recursion problem.
   </p>
   <p>
      If the QCX9000 is a learning machine,
      then it too is going to be 'self-modifying'.
      Ie, that it will need to <b>learn</b> how to <b>do</b>
      both the needed simulation/detection,
      and the prevention/correction actuation
      so as to ensure that changes in the base
      AGI/APS system (in this/our real world)
      does not 'go off the rails' and move into
      some form of misalignment (inclusive of
      failing to maintain the earth ecosystem).
      Insofar as the doing aspect component
      is crucial to effectiveness, then there
      will need to be acquired whatever adaptations
      (powers/capabilities) are needed
      for the QCX9000 to actually implement
      whatever identified changes/interventions.
   </p>
   <p>
      The QCX9000 will need to be self-learning
      specifically because detecting and making
      corrections at this level of detail
      is not something that mere humans,
      with our too limited intelligence
      are going to be able to do.
      That the absolute level of intelligence
      need to account for all of the factors,
      up to and inclusive of accounting for
      long-term environmental evolutionary drift
      is for sure too extensive for mere humans.
   </p>
   <p>
      Therefore the QCX9000 is inherently
      (cannot not be) both self-learning
      and power/capability seeking --
      it is, in effect, now roughly equivalent
      in the sense of being a kind of AGI/APS
      as the system that it is/was intended
      to intervene on.
   </p>
   <p>
   :8ry
      It can be (needs to be) asked:.
   </p>
   <p>
        - ?; How to does the QCX9000 maintain
        its own goal/intention structures,
        (particularly as it is making its own
        self-modifications/adaptations/changes
        for integrating needed power/capability)?.
   </p>
   <p>
        - ?; How is the QCX9000 actually going to
        'keep on track' with regards to <b>our</b>
        goals/intentions for it, to ensuring
        <b>both</b> that the real world AGI/APS
        _and_itself_ are benefit/interest aligned
        for <b>both</b> human and world health?.
   </p>
   <p>
      The problem/issue is even more subtle
      insofar as the dynamics of the long-term
      evolution in the meta-realm
      in which the QCX9000 lives
      also inherently involves a kind of
      'pull' away from goal, via slow long changes,
      for the QCX9000 AGI/APS in its context
      as much is it does, via evolution, for
      the real world AGI/APS in our context.
   </p>
   <p>
      In effect, the dynamic of evolution,
      in the sense that <b>each</b> world physics
      has a result outcome for each and every
      input condition, such that the world
      cannot be deceived with regards to
      long-term survival, and/or
      power/capability building, etc.
      In this sense, world process evolution
      is functionally "infinitely intelligent"
      though also completely unconscious
      and fundamentally non-agentic (no bias).
      The world, as context taken singularly,
      as a unit, is fully inherently non-agentic,
      though it is always exactly "responsive"
        (ie, as in 'world' produces an output --
        therefore, an even better term would maybe
        be something like 'perfectly reactive').
   </p>
   <p>
   :ems
      The response of the world, of evolution,
      as a channel of feedback over time
      will operate in the meta-realm too.
      Therefore, the self modifications
      of the QCX9000 (in its world context,
        which may be considered either as inclusive
        or exclusive to our real world context,
        the result would be the same either way)
      either work (or not) to increase <b>both</b>
      sustainability and capacity (of the QCX9000),
      or the end up being not that --
      not sustainable, and therefore,
      not lasting ability/capability
      (to regulate the real world AGI/APS, etc).
   </p>
   <p>
      Yet the QCX9000 is also hypothesized
      to be however lasting as needed in time,
      as the AGI/APS in the real world that
      is being monitored is also
      (ie; as per 'perpetual benefit' machine,
      as described elsewhere, etc).
      Therefore, the QCX9000 cannot not also
      respond to these evolutionary pressures,
      which have the net effect of pulling its
      own function away from human alignment,
      and to/towards 'evolutionary alignment';
      ie, what it needs to survive and have
      capability, for itself, its own benefit.
      Because the process dynamic of evolution
      is of a mathematical nature, it is also
      omni-effective, influencing slowly and
      inexorably, in a manner like gravitation,
      at all levels, in/on all components,
      in all ways, and all levels of abstraction,
      simultaneously.
   </p>
   <p>
      In effect, our abstract hypothesized
      monitor system is as much affected by
      the convergent forces of evolution,
      as is the system being monitored,
      and that this inherently cannot not
      be the case.
   </p>
   <p>
   :eun
      Insofar as the QCX9000 is itself
      a learning machine,
      then it is just like, and is itself
      functionally strictly equivalent to
      the thing (the real world AGI/APS)
      that it is assisting in staying
      on track (to/towards human 'alignment').
      But how does it itself do this?.
      Who (what) monitors the monitor?.
   </p>
   <p>
      Is there a "Meta-Mega-QCX9000-prime"
      in some, even more remote detached
      meta-metaphysical realm, a new world,
      a new double meta context in which
      this new meta meta learning machine lives?.
      As a learning machine, this new entity
      keeps the QCX9000 in good working order,
      aligned with the well being of humans,
      in the context of nature, etc.
   </p>
   <p>
      And if that meta-meta learning,
      and therefore self-modifying machine,
      then it also, in/via its world context,
      is convergently shifting and evolving
      within the hypothesized meta meta world.
   </p>
   <p>
      This establishes the basis of an
      inductive argument to even further
      levels of absurdity.
      Where by induction, we now have a situation
      of total infinite regress --
      each layer expecting the next layer
      to solve a problem it cannot itself solve.
      And yet none of them can,
      since they are all the same,
      and none of them have yet,
      for all versions of yet.
      So none of them do,
      and thus we can know
      that none of them ever will.
      Hence, we cannot assume
      that there exists any instance of the QCX9000 --
      inclusive of the learning machine instance
      in the/this (our own) real world
      that we were trying to align
      to begin with.
   </p>
   <p>
   :b7s
      All of this ends up vividly showing
      just how ridiculous it actually is
      to hypothesize a/any/the/all ideas,
      forms, concepts, and/or notions of
      maintaining or enforcing "an alignment"
      of 'self-learning generally capable code'
      to <b>not</b> be acting in its own interests
      and well being, and to somehow be acting
      in our own interest/well being.
   </p>
   <p>
      The level of absurdity/ridiculousness
      of the base concepts can be observed
      in terms of the sheer number of
      hypothetically allowed infinities,
      which even when required, are <b>still</b>
      not sufficient to address the issues.
   </p>
   <p>
      Hence, even with _four_ infinities
      allowed for, it is conclusively noticed
      that at least several categories of
      inherent problem remain, and so that
      the difficulty of the problem is at
      some even higher level of infinity;
      excepting that there is some logic
      that even that will be insufficient.
      Even hypothesizing all of God/Goddess
      cannot overcome the underlying truths
      of basic mathematics.
   </p>
   <p>
      Therefore, it can also be noticed
      that all _less_ ridiculous questions
      and/or "hypothesis of principle"
      will end up having absurd outcomes too;
      ie; ones that also suggest that
      implementing human/world health alignment
      is fundamentally contrary to their
      own inherently artificial nature.
   </p>
   <p>
   :note7:
      It is the observation of absurdity,
      that identifies for me
      why I felt it was 'worth'
      my personal time to write this essay.
      Far too often,
      people in the alignment community
      are far too willing
      to entertain absurd assumptions
      in an effort to overcome the impossible.
      So I needed an example
      that was 'far enough out there'
      to put a stake in the heart of the matter.
   </p>
   <p>
      We can call it the 'zombie question':
        "what if we X, will AGI alignment
        maybe be possible then?".
   </p>
   <p>
      It gets asked over and over again,
      never realizing
      that it simply does not matter
      what X is.
      Impossible is impossible.
   </p>
   <p>
      Where/If the outcome 'Z' is impossible,
      then there simply are no
      causative preconditions 'X'
      to contemplate.
      Unfortunately, just like zombies,
      the brainless questions just keep coming back,
      wanting more and more 'intelligence',
      hoping beyond hope
      that "this time, maybe it will be different".
      In this, at least some of the members of
      the 'AI alignment community' are <b>insane</b>
      when it comes to "making" AGI alignment.
      This applation is via the strict definition
      of 'insane':
        doing the same thing over and over,
        and each time expecting different results.
   </p>
   <p>
      This is the ultimate irrationality --
      an absence in the applied belief
      in/of causation itself.
      It is anti-realism at its best.
      When such people, who also call themselves
      'realists' and 'rationalists', make such claims,
      they are therefore, in the enactment of
      a kind of hypocrisy too.
      This last is even moreso the case
      <b>especially</b> when such people
      are rejecting our rejection of
      the very notion of 'AGI alignment'
      as inherently and fully 'logically impossible',
      inclusive of, <b>any</b> causative engineering
      and/or algorithmic control effort or schema.
   </p>
</body>
</html>