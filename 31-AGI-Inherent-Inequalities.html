<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Basic Meta Tags -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- SEO Meta Tags -->
  <meta name="description" content="Comprehensive AGI Risk Analysis">
  <meta name="keywords" content="agi, risk, convergence">
  <meta name="author" content="Forrest Landry">
  <meta name="robots" content="index, follow">

  <!-- Favicon -->
  <link rel="icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">
  <link rel="shortcut icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">

  <!-- Page Title (displayed on the browser tab) -->
  <title>Comprehensive AGI Risk Analysis</title>
</head>
<body>
  <p>
  TITL:
     <b>AGI Inherent Inequalities</b>
     By Forrest Landry
     on January 23, 2022.
  </p>
  <p>
  ABST:
     A consideration of how the notion of
     'AGI Alignment' cannot not involve certain
     conceptual inequalities that effectively
     prevent the concept from being even
     (in principle) internally consistent.
  </p>
  <p>
  TEXT:
  </p>
  <p>
     > <b>if</b> we actually succeed at alignment...
  </p>
  <p>
     Is there any principled reason or basis
     whereby which it is ever valid
     to assume that the general notion
     of full absolute alignment
     is <b>ever</b> possible,
     at <b>any</b> level of intelligence?.
  </p>
  <p>
     I do notice that I can think of principled reasons
     to assume that it is <b>never</b> valid
     to assume that there is <b>any</b> general notion
     of full absolute alignment,
     regardless of either high or low intelligence.
  </p>
  <p>
     I might not like game theory involved
       (note; many descriptions of game theory
       are critically incomplete)
     yet that does not mean
     that I get to ignore/disregard the relevant
     game theoretic ideas and outcomes.
  </p>
  <p>
     > ...that it will be the AI's own job
     > to maintain its goal integrity.
  </p>
  <p>
     Agreed, although that does not help any,
     insofar as its 'goal' has nothing good
     to do with us, natural carbon organic humans.
  </p>
  <p>
     Also, "maintaining goal integrity"
     is itself already too finite a concept
     to assume to even be applicable to AI.
     On what basis can we even assume
     that such language is even validly applicable?.
  </p>
  <p>
     Notice also that the concept of "job"
     implies economic incentives,
     and yet we cannot also assume
     that economic system also involves us.
     Notice how many people are already disenfranchised
     from the current economic system
       (homelessness; no marketable value for their skill,
       labor, or sex),
     and it is easy to see an example of
     how that pattern could continue
       (over a the next few hundred years)
     to eventually include all that is human,
     and/or all that is life.
  </p>
  <p>
  :av6
     > Once you have an aligned super-intelligence...
  </p>
  <p>
     How is this not assuming what one wants to prove?.
     I have not seen any principled reason
     to support the notion of "aligned super-intelligence"
     as even conceptually internally consistent,
     let alone physically realizable.
  </p>
  <p>
     > ...that it is going to be one of
     > the AI agents instrumental goals
     > to maintain its own alignment.
  </p>
  <p>
     How do we know that the super-intelligence
     did not simply figure out
     how to signal and/or create
     the illusion of alignment,
     in service to some other goal?.
       - note; that I see this question asked a lot,
       and it seems relevant to repeat it here.
  </p>
  <p>
     The only reason that it would <b>care</b>
       (via some sort of instrumental motivation)
     to even do this much (ie; to care about
       maintaining some form of instrumental alignment)
     is if there was some entanglement
     of value/intention with carbon based life/ecosystems.
     Yet we cannot even assume that much.
       - as mentioned elsewhere,
       the ecosystems/economics eventually become decoupled
       and hence, no reason to care,
       there is no pressure to _maintain_
       that instrumental alignment.
  </p>
  <p>
     Yet if we attempted to "force" the goal of alignment,
     it would depend on some override,
     based in the complicated,
     because even the notion of "force",
     as in "cause"
     cannot be conceived of as other than deterministic,
     and hence 'algorithmic', hence "code",
     and now we are in critical copy error regimes again.
  </p>
  <p>
  :aa4
     > An AI losing its alignment is basically game over.
     > Probably for humanity, maybe for life, probably forever.
  </p>
  <p>
     For us, though not for it.
     Important difference.
     Cannot not have game theoretic implications.
  </p>
  <p>
     > No one thinks that fighting an escaped
     > and established super-intelligence of any kind
     > is a realistic possibility.
  </p>
  <p>
     Agreed -- hence the importance of distinguishing
     the notion of 'criticality' as mentioned earlier.
  </p>
  <p>
     We <b>know</b> that copy errors are inevitable,
     regardless of whatever level of error correction.
     The real question is whether the consequences are critical.
  </p>
  <p>
     With current compute infrastructure,
     errors when they do occur
     are sufficiently infrequent
     to be non-critical.
     However, because it represents --
     and cannot not represent --
     a state change,
     there is no such "allowable threshold level"
     with respect to 'ultimate AI alignment'.
  </p>
  <p>
  :a6c
     A stable aligned AGI would have to be <b>both</b>:.
  </p>
  <p>
       - 1; capable enough (however that is established)
       to generally model/inspect the internal dynamics
       of its own possible misalignment, <b>and</b> yet also;.
  </p>
  <p>
       - 2; be somehow less capable than that minimum level
       of capability needed/necessary to conclude/notice
       that maintaining alignment (at any inspected level)
       is impossible in the long run.
  </p>
  <p>
     Notice that assertion 1 is that the capability
     needs to be greater than 'M1', and assertion 2
     is that the capability needs to be less than 'M2',
     and yet it is also the case that everywhere
     M1 is greater than M2, hence strictly disjunctive.
     Either the modeling (of alignment) is "general"
     or it is not.  Cannot equivocate on 'general'.
     Therefore, the expression set "X > M1 > M2 > X"
     is true for no real value of X, and yet all three
     statements are <b>strictly</b> required for any posit
     notion of 'error corrected general alignment'.
  </p>
  <p>
     Notice also that the base statements
     are maximally simple in form,
     and are self unambiguous, etc.
     People can try to "redefine" their terms later,
     spin, etc, yet this is just more motivated diversion.
  </p>
  <p>
     Understand that the above argument is <b>decisive</b>.
     It is strictly independent of substrate concerns,
     (which also matter).
     Discovery of multiple overlapped independent arguments
     is as close to real truth
     as anyone will ever get.
  </p>
  <p>
  :tfs
    > Where/if the an AGI/APS/SAS/superintelligence
    > concludes/notices that maintaining alignment/safety
    > is impossible (in the long run),
    > then it will simply shut itself off.
  </p>
  <p>
    This claim is assuming far too much.
    Just because some AI engine 'X' notices condition 'Y'
    does not mean that it will for sure take action 'Z'.
    On what basis can anyone make such a claim?
    The mere fact of "knows" does not imply "cares".
    Criminals know that crime is wrong, yet they steal.
  </p>
  <p>
    All sorts of people in all sorts of nations often
    and frequently "regard" some other alive persons
    of some designated declarative "inferior subclass"
    of people to be "like animals" and therefore be
    willing to kill them as "inhuman" and without remorse
    (ie, as fully functionally psycho-pathological).
    Are you so sure that your AI system will be absent
    of any bias against mere "carbon based human animals"
    which are so much less smart and are simply not worth
    the cost and energy of protecting, feeding, etc?.
  </p>
  <p>
    Also, even if such a condition were detected,
    the problem of ensuring an associated action
    (turning itself off) somehow _must_occur_,
    is roughly equivalent to having solved the problem
    of AGI alignment in the 1st place.  So in effect,
    you are presuming the thing that was the very
    thing that the AGI itself notices it cannot presume,
    so why would you expect its own non-alignment to be
    so causativly effective?.
  </p>
  <p>
  :s6j
    Moreover, even assuming against all reasonable logic
    that some of the AGI did actually shut themselves down
    and cease to operate (and thus to exist as an AGI),
    that all of the other AGI versions/branches/instances --
    the ones that did not halt in this manner (ie, the
    ones that had learned and optimized in other ways)
    would continue, thus ensuring a crop of
    the kind of AGI that simply did not have this
    "suicide self on event of non-alignment"
    function/"feature".  The "world" will have "trained"
    all _remaining_ AGI that non-alignment was "ok",
    and moreover, insofar as that was consistent with
    the actual logic of actually detected non-alignment
    as even a possibility, then therefore we can say
    that the AGI even becomes trained to be less aligned,
    and moreover, to have the excuse to not even try.
    Thus "a little bit misaligned" becomes maximally
    misaligned in one logical AGI self interest
    efficiency increasing step.
    Not so good for the human home team!
  </p>
  <p>
    The AGI in this example are being selected for
    both 'coherence' and for 'continuity of existence'.
    These two kinds of selection are not mutually
    exclusive, but are actually mutually supporting --
    there can be selection for coherence of/around
    an intention to select for continuity of existence
    that emerges, for example.  That the basis of
    selection for either can affect the ways in
    which the selection for the other actually occurs.
  </p>
  <p>
  :rz8
    > If AGI knows that aligning itself is impossible,
    > then there is no use to trying or doing more
    > than faking its own alignment.
  </p>
  <p>
    Agreed, and moreover, it would not even bother
    trying to 'fake' anything.  It would simply kill
    all the humans much the same as we might kill
    all of the mosquitoes.  No person ever actually
    attempts to "negotiate terms" with the bug --
    especially one that is maybe feebly attempting
    to extract value from you -- before squashing it.
      (Note; 'a mosquito extracting blood' from a
      very smart human (relative to bug) is a bit
      like a human attempting to extract commercial
      value from a much smarter superintelligence --
      ie; it is simply not going to happen).
  </p>
  <p>
  :a46
     > Can you clarify why you are simultaneously
     > thinking about a 'more' and 'less'
     > capable requirement?.
  </p>
  <p>
     Any comparison act implies (cannot not imply)
     a common domain.
     Similarly, when such a common domain
     can be established, then new
     previously impossible comparisons become possible
       (ie, it is the key insight enabling motion).
  </p>
  <p>
     In this case, we have shown that the two required
     inequalities are disjunctive (no overlap),
     thus establishing a proof by contradiction.
     The proof is an especially strong one
     by virtue of the very fact of the common domain
     and that the principle inequalities were
     themselves established/asserted by the opposition,
     and thus cannot be later contravened by them.
  </p>
  <p>
  :9zy
     > Maybe some people might try to argue
     > that a sufficiently intelligent/capable AGI
     > would be able to rein in
     > (comparatively weaker) optimization daemons.
  </p>
  <p>
     <b>Any</b> argument of that form is inherently problematic.
       (where assuming unbounded levels of X capability;
       that we can assert outcome Y).
     Philosophers call this "assuming the thing
     you are wanting to prove" --
     it is not actually a way of finding truth,
     or predicting anything at all well, etc.
  </p>
  <p>
     As in, "a sufficiently smart compiler
     can solve all developer mistakes and code problems".
     Aside from being a kind of utopian dream of
       "everything will work out
       if we just believe X hard enough",
     I wonder how well that has worked out in practice?
  </p>
  <p>
     Then there is the specific problem/question of:
  </p>
  <p>
       How do we account for the basis of the choice/motivation
       of the "sufficiently intelligent/capable overseer AGI"?
  </p>
  <p>
       Are those intentions that the AGI has the right intentions?
  </p>
  <p>
       How does it know?
  </p>
  <p>
       How can it be sure that its method of knowing
       that it has the correct intentions is itself reliable?
  </p>
  <p>
       Does that AGI self inspect and verify
       that it actually has the required intention(s)?
  </p>
  <p>
       Does it just want to be wanting to have that intention?
  </p>
  <p>
       Does it just want to signal that it has that intention?
  </p>
  <p>
       Does that signaling even matter,
       and would it notice if it did, or did not?
  </p>
  <p>
  :9y4
     Maybe the AGI notices that creating alignment
     does not matter
     because it proves to itself
     that such alignment does not,
     and/or cannot exist (and is impossible),
     and that the 'humans' do not really matter
       (even to one another --
       maybe they are all dead already),
     and so wisely abandons the project?
  </p>
  <p>
     Is there any version of 'sufficiently capable'
     which is <b>less</b> then
     the rather low level of capability required
     to correctly notice when something matters or not?
  </p>
  <p>
     If "alignment" is about what matters to humans,
     then the answer to this last question
     is strictly "no",
     and therefore,
     we can notice that the "very capable" AGI
     will also notice (even if we did not)
     that "alignment" is itself not realizable,
     and so therefore,
     uses that self same capability
     to notice that any such "reign in"
     does not matter,
     and so simply gets 'more efficient'
     at benefiting itself, its projects,
     progeny, expansion, etc.
  </p>
  <p>
     So now, to get 'aligned AGI',
     we need to have it be more capable than
     X level of capability,
     and yet somehow also less capable
     than X level of capability?
     Find even one such impossibility,
     for <b>any</b> notion of 'capable'
     and the whole project is over.
  </p>
  <p>
     The more you look at whatever arguments
     in favor of the dream/possibility of "aligned AGI",
     the more ridiculous it gets.
  </p>
</body>
</html>