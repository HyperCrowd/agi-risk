TITL:
   *Infinity Control*
   By Forrest Landry
   Oct 26th, 2022.

ABST:
   As an example of an applied analysis,
   via the use of EGS text reforming technique,
   of a complex implied hypothetical question,
   so as to enable a real response to same.

   As a demonstration of the proper way
   to process arguments of principle,
   particularly where they are extending into
   spaces which are real engineering unreachable,
   but nonetheless which are important to consider
   for reasons of clarifying the methods
   of proving abstract principles still valid.

TEXT:

   Where the following paragraph was found
   in an email from a colleague;.

   > If, hypothetically, there was a separate
   > (located in a 'metaphysically decoupled' realm)
   > adaptable control system
   > running on infinite hardware
   > with infinite learning compute
   >   (and complete measurement
   >   and internals-modification capacity),
   > then that separate control system *could*
   > fully simulate/detect as well as prevent/correct
   > the co-option by internal code variants
   > of AGI's directed functionality,
   > as shifting away outcomes
   > from those originally selected for
   > through optimization methods by humans
   > (eg; AI R&D labs with certain intents).

   The above was felt (by this author)
   to be a bit ambiguously stated.
   It was not entirely clear how to respond,
   insofar as there is a lot going on, and referring
   to the required clauses did not seem at all easy.
   As such, it needed to be "translated" into
   something that had an equivalent form,
   and which was also (obviously) question,
   and a much more clearly identifiable structure.

   The EGS 'conversion' of the above text
   resulted in the following structurally equivalent,
   expanded, and isomorphic form (@ 1 #note1);.

:cpl
   > - where/what-if hypothetically we assume
   >   (as ?; could it be the case)
   >   (where as if located in a
   >   'metaphysically decoupled' realm)
   > that there was/is
   > a fully separate control system, a "QCX9000",
   > which is:.
   >   - 1; fully adaptable (human programmable).
   >   - 2; running on infinite hardware with:.
   >     - 3; perfected/complete measurement capacity.
   >       - ie, where assuming no Shannon entropy,
   >       no message or signal noise at all.
   >     - 4; perfected internals-modification capacity.
   >     - 5; infinite learning compute.
   >
   > - C; then/therefore that the separate
   > QCX9000 control system could potentially
   > (at least in principle, conceptually)
   > be used to:.
   >   - 6; fully simulate and/or detect
   >   all future AGI/APS states;
   >   as well as;
   >   - 7; prevent/correct the/any co-option/shifting
   >   of the AGI action outcomes:.
   >     - which are away from:.
   >       - 8; the previously selected abstract
   >       human designer/builder intentions
   >       and/or the originally intended (hoped for)
   >       AGI usage/operational outcomes (@ 5 #note5).
   >
   > - A; where that 'shifting of outcomes' in ^7
   > *maybe* occurs via (the use of)
   > internal or external code variants.
   > - B; where those outcomes ^7
   > were (maybe) originally selected for (trained)
   >   (and implicitly encoded within
   >   the AGI/aps system)
   > via some previously implemented/completed
   > training/optimization methods/processes.
   >   - where those training/optimizations/methods:.
   >     - implicitly encode the intentions of ^8.
   >     - where implemented previously by those
   >     AGI/APS designer/developer humans.
   >       (working in and using
   >       the capabilities of R&D labs
   >       themselves constructed
   >       specifically for that purpose
   >       prior to the initial/original specific
   >       AGI/APS build).
   >   - as assuming/presuming that those humans
   >   (the developers) themselves actually
   >   had the/those abstract intentions
   >   as listed considered/referenced in ^8.

   Some details have been added, others clarified,
   and reference marks have also been included --
   all so that the overall form of the argument
   can be made both more complete as well as
   more explicit (unambiguous in interpretation).

   I also gave the assumed/hypothesized machine
   a cute marketable name, the "QCX9000" (tm) (c),
   for convenience of reference (@ 2 #note2).

   For the purposes of this essay, and because
   this is an example of applied technique,
   it will be assumed that there was some
   confirmed indication/assent of equivalence
   (of structural/functional meaning sameness)
   has actually been obtained and recorded
   by the origional input text author.

:s7g
   Where in regards to the above translated text,
   the overall question is in the form:

      > "where/if we assume condition(s) 'X';
      > ?; does/do condition(s) 'Z'
      > also become possible?".

   That the main thing (usually) to notice
   is if something about X
   is actually structurally unreasonable.
   If X were to be defined as "lets assume 2/0",
   for example, then the question statement
   becomes meaningless.

   If there is a divide by (@ zero https://math.hmc.edu/funfacts/one-equals-zero/) error,
   hidden and/or implicit somewhere
   in a question, or "proof", etc,
   then *any* arbitrary conclusion
   can be constructed eventually.
     (in the comp-sci literature,
     due to a presentation joke,
     these are often referred to
     as "nasal demons" (@ cite https://www.urbandictionary.com/define.php?term=nasal%20demons)).
   Where from any basic contradiction,
   by argument, one can obtain results like
   "1 equals 2", and from there
   that "1" equals any other number,
   or that any number equals any other number,
   or that any of them are infinity, etc.
   Absurdity inherently has no limit.

   In this case, 'X' consists of the clauses ^1 thru ^5,
   which specifies fully *three* distinct
   kinds of infinity:.
     - unbounded energy.
     - unbounded detail in sensory capability.
     - unbounded detail and energy
     of output/modify/affect.
   This is roughly the equivalent of
   a 'divide by zero' error assert
   three times over.

   Then, moreover, there is the assumption,
   via ^5, of 'infinite learning'.
   Via this, there is also the assumption,
   also implied, that the QCX9000
   will come to eventually "know fully"
   the totality of all human good intention
   (benefit) at *all* levels of abstraction,
   and then finally,
   that somehow the assumption of ^1
   could also be applied to ensure
   that the QCX9000 will *want_to*
   actually apply its complete knowledge
   of 'what is human benefit' and also of
   'how to implement total human benefit'.

   In other words, there is an assumption
   that some human who is also in position
   to effect such choices, will have the skill,
   via some correct application of
   the assumption of ^1,
   to be able to somehow 'get' or 'convince'
   the QCX9000 to apply their available
   understandings and capabilities to actually
   be in the service of humans and human welfare.
   Ie, the QCX9000 may know,
   at all relevant levels of abstraction,
   what goodness for humans is,
   and also know how to implement that,
   and yet have no desire or motivation
   to actually implement any actions
   to/towards human benefit.
   In fact, there is no reason to assume that
   the QCX9000 might not also use its *also*
   perfected knowledge of how to cause harm
   to humans so as to advantage itself.

   Hence that person (the developer)
   who has 'programming control'
   will actually need to be very quite skillful
   indeed so as to be able to somehow 'shift'
   the intentionality of the control system,
   with its infinite learning -- thus seeing
   completely through any attempt of that human
   to 'manipulate' that control system in ways
   that it would not understand (it cannot be
   deceived or tricked).  Hence, while we can
   assume that the ^1 assumption provides for
   a means to *compel* the QCX9000 to have
   the necessary desires, it is not therefore
   also clear that the human, operating at
   their very limited level
   of cogitative abstraction
   to have the raw skill necessary
   to actually issue the necessary instructions
   so as to have that user intended outcome
   (in the control system, the QCX9000,
   as distinct from the AGI/APS system
   that is to be/become aligned
   by the actions of the control system).

   Notice that the net effect
   is to have the relationship between
   the client human and the QCX9000
   be actually structurally equivalent
   to the relationship that that client human
   (or any other client human)
   has directly to the AGI/APS system
   that is to be 'aligned' (@ 6 #note6).
   In other words, there is an implied fallacy
   of simply displacing the same problem from
   one structural level of consideration
   to another, without ever actually solving
   the core problem
   or answering the core question:
   ?; is it *ever* the case
   that human beings,
   with just their native intelligence,
   will *ever* be able to force or compel
   *any* superintelligence to have any of,
   or even any aspects of, the necessary desires,
   to care, to have the willingness
   to apply their hyper skill
   to the benefit of humans?.

   Human programmers may be rather smart,
   but I do not think that any programmer ever
   will be sufficiently smart to provide such
   instruction so flawlessly as to get the
   intended effects (AGI/APS machine care)
   regardless of how many other constraints
   are otherwise also *completely* removed.
   (@ 4 #note4)

   On this reason alone, we would/should
   be able to completely reject
   the overall form:.

      > - where given conditions ^1 through ^5
      > (as 'X');
      > ?; can we therefore also assume
      > that the necessary condition(s)
      > have been obtained
      > so as to have outcome 'Z'?".

   In short, no:
   There was needed an additional condition,
   as some sort of infinite 'convincingness'
   on the part of a human programmer
   so as to be able to 'convince' or 'require'
   the (QCX9000 as an AGI/APS in itself)
   to have the willingness to implement care.

      > How can a strictly finite intelligence
      > ever 'convince' an infinite intelligence
      > to have any specific desire/care 'D'?.

   *Even if* we were to *also* assume
   some sort of 'perfected compulsion'
   on the part of the finite intelligence
   onto/over the infinite intelligence;
   that does *not* in itself suggest that
   notion of 'having a desire' and 'a care'
   is itself a concept without any structure.
   The emotions/feelings/will/choice to have
   or *be* that sort of thing/creature/mind
   that "has" such care/desire, is also to
   suggest something of near infinite detail,
   when considering also it being in/within
   an infinite intelligence context.
   It is simply the case that the finite mind
   simply cannot fully specify all of the
   information that would be needed so as to
   instruct the being of the infinite mind
   as to how to be the kind of thing with
   those specific desires.

   Moreover, attempting to circumvent the
   inherent need for that level of design
   by asking the QCX9000 to modify itself
   is not to have created the desire/will
   *also* at the level of needed detail
   for the desire/will to create desire/will
   itself to be correctly fully specified.
   Again, there is an infinite regress of
   assuming what is wanted to be proved.
   And then the "he who pays the piper
   must know the tune" (@ aspects https://sinceriously.fyi/wp-content/uploads/2019/07/He-Who-Pays-The-Piper-Must-Know-The-Tune.pdf) also apply.

:qlj
   So as to continue anyway, lets say that
   we 'upgraded' the structure of the question
   so as to somehow also assume an additional
   upgrade to condition '1' so that it assumes
   that this specific QCX9000 instance loves
   people, humanity, etc, at all levels of
   abstraction (and hence we no longer need
   to worry about the skill
   of human programmers).

   Is this enough to ensure that the question
   form of "where assuming condition 'X';
   do we also therefore get condition 'Z'?"
   will result in a positive assent?.
   No, unfortunately not.
   Even with all of that, with so many assumed
   infinities and conditions of improbability,
   it is *still* no enough to ensure alignment.

   The basic problem is the possibility of
   the combination of aspects ^6 and ^A;
   that there might be some _external_ code
   that is adversely designed, ie, has some
   *other* intention, than human welfare,
   and designed so well, say by some sort
   of anti-QCX9000 agent/agency/intelligence,
   so as to be perfectly designed so as to
   maybe deceive, or at least out-think the
   detection/simulation capability of ^6.

   In other words, *if* we are permitted to
   assume the existence of a fully capable
   QCX9000 once, *then* that _same_ assumption
   can be made again, maybe with different
   (less) partial composition/assumptions,
   each time.  Ie, rather than assuming some
   modified ^1 that implies human love,
   that we simply omit aspect ^1, which
   by assuming less, means that it is more
   likely that new anti-human-love "QCX9501"
   could also be concurrently created/used.

   That this hateful alternative, the QCX9501,
   having also its own versions of ^6 and ^7
   (excepting that the intentions/outcomes
   are different, towards human harm, etc),
   so that QCX9501 constructs a specially
   crafted program, to inject an 'external' ^A
   such that the prior love-QCX9000 would
   have its ^6 capability suborned.

      > - ?; can an infinite superintelligence
      > with perfected deception capabilities
      > (via ^2 through ^7, all inclusive)
      > deceive another infinite superintelligence
      > that also has perfected anti-deception
      > capabilities (also via ^2 through ^7,
      > again and all inclusive)?.

   As soon as this latter question form is
   stated, it should be obvious that the
   only correct analytic response is "oops" --
   we have entered into the 'how many Angels
   dancing on the head of a pin?' territory.
   The misuse of infinity operators has lead
   to a strong structural contradiction in
   the equivalent formulation of the question,
   as irresolvable, which means that the
   internal inconsistency in condition 'Y'
   in itself fully prevents its principled
   manifestation, regardless of the level
   of attainment of the "prior" X condition.

:4jy
   > Can anything be salvaged?.

   No, not really.  The basic problem
   is inherent in each of the infinities
   associated with ^2 through ^6 --
   that even one of these by itself
   would be inherently vulnerable to the
   anti-replicant argument just applied.

   Any attempt to preserve some
   non-infinity aspect of ^7 by using
   anything less than at least multiple
   other infinities does not have
   sufficient power to actually effect ^7,
   and yet actually assuming such aspects
   ensures that that assumption in itself
   enables self adverse weaponization.
   So the net result remains a total loss.

:8t8
   Moreover, even if we were to somehow
   overcome such a powerful objection,
   (extending beyond the beyond, as it were)
   there is even a next order of problems
   still unaddressed: how to ensure that
   there is a capability to compare the
   output of the future simulation output
   states, at the concrete manifestation
   object level of abstraction with the
   (maybe near) (perfected) knowledge
   of what is good/beneficial for humans,
   which is a concept at near infinite
   abstraction.  The basic problem is
   that the comparison itself is a kind
   of infinity problem -- comparing
   incomparables, apples and oranges,
   at completely different levels of
   abstraction, nearly perfectly removed.

   Hence, the very meaning of the question,
   overall as described in aspect ^6,
   is fully unclear, and thus cannot
   advise aspect ^7 despite the aspect
   of ^5 maybe also being maybe assumed.
   Only things that are _knowable_
   are also learnable, and there is
   an unanswered question as to how
   such a comparison would be made,
   even in principle, that is *not*
   addressable by simply assuming
   infinite intelligence/learning.
   Things which are impossible in principle
   will probably remain that, especially
   in exercises of principled analysis.

   QED.

:note1:
   This EGS conversion was particularly complex.
   It is not usually the case that there are so
   many cross entangled sub-clauses in a text.
   It was the fact of the complexity that made this
   instance a reasonably good example for why
   EGS text conversion, generally,
   is a valuable technique.

:note2:
   Usually, at this point, there is a need
   to formally establish a required form of
   interpersonal conversational coherence.
   Is client/querent reasonable and/or
   rationalional
     (ie, as also inclusive of a background
     assessment of the client orientation
     to mistake theory or conflict theory,
     their overall social action intentions,
     etc).

   For an author, it is important to 1st
   present the restructured EGS text,
   and then 2nd get a positive and explicit
   confirm,
     (usually as witnessed and recorded,
     in a generally socially/publicly
     legible/observed forum, etc)
   *before* any additional analysis and response
   is even attempted.

   The need is to get a *confirmed* reply
     (best if it is in public available writing)
   that the re-statement
   is actually equivalent to, and inclusive of,
   all intended (and maybe only implied)
   aspects of the prior exact given
   paragraph language.

   That this is usually implemented by
   *synchronously* providing the above rewrite,
   and then, also synchronously,
   asking the following questions:.

     - ?; is the re-write recognized by you
     as an 'upgrade'?.

     - ?; is it inclusive and clarifying
     and generalizing of what you had originally
     intended to ask?.

     - ?; is it subsumptive of your overall
     question concerns?.

   Where the client/querent assents and confirms,
   then the analysis can proceed (note 3 below).

   Where the client/querent does not assent,
   then the client/querent is obligated
   to provide clarifications as to the details
   that they regard are not, or were not,
   extending/generalizing and/or clarifying
   of the original meaning of the paragraph
   as *exactly* stated.
     (note; later provisions cannot be tacitly
     allowed to be added/injected by the querent,
     else the questions of querent social intentions
     will also need to be raised).
   When whatever re-writes and adjustments,
   still in EGS form, are completed to the point
   that the client/querent holds that the
   complete generalization of their input paragraph
   has been fully reified; then the analysis of
   the now confirmed equivalent EGS text
   can proceed.

   Partly the reason for this level of process
   is to both assess the level of reasonableness
   of the querent (are they acting politically
   in the sense of conflict theory, or not?).
   It is also to ensure that they cannot pre-
   review the analysis outcome, which if not
   desired (or willing to be accepted by them)
   that they would attempt to retrospectively
   shift the input text so as to try to get
   some sort of different analysis outcome --
   one more *seemingly* (though falsely)
   favorable to their pre-presumed position.

:note3:
   Where in common practice, that such explicit
   call and response transactional dynamics are
   usually implicit in a synchronous comm channel.
   However, in the cases where much higher levels
   of situational severity are involved (x-risk)
   there can more often be an explicit need
   to implement the necessary level of technical
   formality in a written manner, both because
   the analysis itself demands it, due to need
   for references into complex structures,
   and/or also as due to 'open' particularly
   deeply obscured aspects of the argument,
   particularly where such aspects are actually
   crux aspects of the overall argument.

   That many complex boolean type questions
   with lots of linked compound operators
   and factors can be easily shifted to the
   complete discrete exact opposite state
   simply by a single subtle reversal deep
   within the overall logic structure.
   Hence, to maintain sanity within such
   compound statements, it is important to
   actually have explicit tracking of each
   determining aspect.

   These aspects also inherently involve a
   consideration of the degree to which
   synchronous high-bandwidth interaction
   (as human to human in person conversational
   debate, meeting in larger observer groups, etc)
   can be combined with the asynchronous
   low bandwidth ordered exchanges of text
   can be mixed together.
   Usually the action of intermixing *any*
   high-bandwidth synchronous protocol
   will have the side effect of 'swamping'
   (making completely situationally irrelevant)
   anything that may be happening in the lower
   bandwidth channels, regardless of how
   concurrent they may be.
   This group-choice-process aspect continues
   to receive a lot of research attention
   insofar as it is a gating issue in the
   overall design of the governance stack process.

:note4:
   The outcome which left the crux question
   remaining unanswered and unanswerable
   is the very reason that this particular
   paragraph was expanded into this example essay.
   Normally, it would not be relevant to explore
   the implications of a given question/argument
   'stated in principle', if there were *any*
   irrational infinity conditions, errors, etc.
   However, not being willing to explore such
   impossible hypotheticals would not provide
   the means to actually analyze crux issues
   of exactly this type, since they only become
   available and evident as crux *if* and *only* if,
   some actual disciplined manner of factoring out
   and neutralizing the effects of all of
   the other infinities/errors in the argument
   and/or question form.

   That being able to handle infinities/errors
   in the hypotheticals in a well structured way
   is the very point of having functional
   isomorphism tools, like EGS, cleanly managing
   multiple levels of indirection and reference,
   and the IDM metaphysics tools/concepts, etc.
   With these sorts of resources, it becomes
   possible to manage structured conversational
   process around high intensity x-risk topics
   *without* getting lost in endless details,
   distractions, and multiple covert degrees
   of intentional political disruption.
   Rhetoric, confusion, FUD, emotion, etc,
   are all not as helpful as what is, at core,
   absolutely needed to resolve.

:note5:
   The notion of 'human intention' also includes
   (must, cannot not ) implicit intentions of
   things like "for humans to survive"
   which in turn means things like
   "humans in a world (healthy planet)
   that is at least natural enough
   to produce things like food, etc".

   If the environmental conditions of the planet
   were to shift/change towards artificiality
   to the point that maintaining ecosystems fails,
   then the broader context for human survival
   also fails, and thus these sorts of larger,
   more abstract, global shifts must also be
   accounted for (ultimately) by the QCX9000.

   Therefore, the concept of 'intentionality'
   inherently includes an implicit alignment goal
   of having a/the hypothetical system
   (the QCX9000) that could also 'correct out'
   selection for AGI/APS code variants/internals
   that have the effect of changing the conditions
   of the environment in ways which,
   while result in their own (the AGI/APS)
   continued existence, (ie; the API/APS endure),
   that the humans do not (the world is broken).

:note6:
   There is in this also, inherently,
   a kind of recursion problem.

   If the QCX9000 is a learning machine,
   then it too is going to be 'self-modifying'.
   Ie, that it will need to *learn* how to *do*
   both the needed simulation/detection,
   and the prevention/correction actuation
   so as to ensure that changes in the base
   AGI/APS system (in this/our real world)
   does not 'go off the rails' and move into
   some form of misalignment (inclusive of
   failing to maintain the earth ecosystem).
   Insofar as the doing aspect component
   is crucial to effectiveness, then there
   will need to be acquired whatever adaptations
   (powers/capabilities) are needed
   for the QCX9000 to actually implement
   whatever identified changes/interventions.

   The QCX9000 will need to be self-learning
   specifically because detecting and making
   corrections at this level of detail
   is not something that mere humans,
   with our too limited intelligence
   are going to be able to do.
   That the absolute level of intelligence
   need to account for all of the factors,
   up to and inclusive of accounting for
   long-term environmental evolutionary drift
   is for sure too extensive for mere humans.

   Therefore the QCX9000 is inherently
   (cannot not be) both self-learning
   and power/capability seeking --
   it is, in effect, now roughly equivalent
   in the sense of being a kind of AGI/APS
   as the system that it is/was intended
   to intervene on.

:8ry
   It can be (needs to be) asked:.

     - ?; How to does the QCX9000 maintain
     its own goal/intention structures,
     (particularly as it is making its own
     self-modifications/adaptations/changes
     for integrating needed power/capability)?.

     - ?; How is the QCX9000 actually going to
     'keep on track' with regards to *our*
     goals/intentions for it, to ensuring
     *both* that the real world AGI/APS
     _and_itself_ are benefit/interest aligned
     for *both* human and world health?.

   The problem/issue is even more subtle
   insofar as the dynamics of the long-term
   evolution in the meta-realm
   in which the QCX9000 lives
   also inherently involves a kind of
   'pull' away from goal, via slow long changes,
   for the QCX9000 AGI/APS in its context
   as much is it does, via evolution, for
   the real world AGI/APS in our context.

   In effect, the dynamic of evolution,
   in the sense that *each* world physics
   has a result outcome for each and every
   input condition, such that the world
   cannot be deceived with regards to
   long-term survival, and/or
   power/capability building, etc.
   In this sense, world process evolution
   is functionally "infinitely intelligent"
   though also completely unconscious
   and fundamentally non-agentic (no bias).
   The world, as context taken singularly,
   as a unit, is fully inherently non-agentic,
   though it is always exactly "responsive"
     (ie, as in 'world' produces an output --
     therefore, an even better term would maybe
     be something like 'perfectly reactive').

:ems
   The response of the world, of evolution,
   as a channel of feedback over time
   will operate in the meta-realm too.
   Therefore, the self modifications
   of the QCX9000 (in its world context,
     which may be considered either as inclusive
     or exclusive to our real world context,
     the result would be the same either way)
   either work (or not) to increase *both*
   sustainability and capacity (of the QCX9000),
   or the end up being not that --
   not sustainable, and therefore,
   not lasting ability/capability
   (to regulate the real world AGI/APS, etc).

   Yet the QCX9000 is also hypothesized
   to be however lasting as needed in time,
   as the AGI/APS in the real world that
   is being monitored is also
   (ie; as per 'perpetual benefit' machine,
   as described elsewhere, etc).
   Therefore, the QCX9000 cannot not also
   respond to these evolutionary pressures,
   which have the net effect of pulling its
   own function away from human alignment,
   and to/towards 'evolutionary alignment';
   ie, what it needs to survive and have
   capability, for itself, its own benefit.
   Because the process dynamic of evolution
   is of a mathematical nature, it is also
   omni-effective, influencing slowly and
   inexorably, in a manner like gravitation,
   at all levels, in/on all components,
   in all ways, and all levels of abstraction,
   simultaneously.

   In effect, our abstract hypothesized
   monitor system is as much affected by
   the convergent forces of evolution,
   as is the system being monitored,
   and that this inherently cannot not
   be the case.

:eun
   Insofar as the QCX9000 is itself
   a learning machine,
   then it is just like, and is itself
   functionally strictly equivalent to
   the thing (the real world AGI/APS)
   that it is assisting in staying
   on track (to/towards human 'alignment').
   But how does it itself do this?.
   Who (what) monitors the monitor?.

   Is there a "Meta-Mega-QCX9000-prime"
   in some, even more remote detached
   meta-metaphysical realm, a new world,
   a new double meta context in which
   this new meta meta learning machine lives?.
   As a learning machine, this new entity
   keeps the QCX9000 in good working order,
   aligned with the well being of humans,
   in the context of nature, etc.

   And if that meta-meta learning,
   and therefore self-modifying machine,
   then it also, in/via its world context,
   is convergently shifting and evolving
   within the hypothesized meta meta world.

   This establishes the basis of an
   inductive argument to even further
   levels of absurdity.
   Where by induction, we now have a situation
   of total infinite regress --
   each layer expecting the next layer
   to solve a problem it cannot itself solve.
   And yet none of them can,
   since they are all the same,
   and none of them have yet,
   for all versions of yet.
   So none of them do,
   and thus we can know
   that none of them ever will.
   Hence, we cannot assume
   that there exists any instance of the QCX9000 --
   inclusive of the learning machine instance
   in the/this (our own) real world
   that we were trying to align
   to begin with.

:b7s
   All of this ends up vividly showing
   just how ridiculous it actually is
   to hypothesize a/any/the/all ideas,
   forms, concepts, and/or notions of
   maintaining or enforcing "an alignment"
   of 'self-learning generally capable code'
   to *not* be acting in its own interests
   and well being, and to somehow be acting
   in our own interest/well being.

   The level of absurdity/ridiculousness
   of the base concepts can be observed
   in terms of the sheer number of
   hypothetically allowed infinities,
   which even when required, are *still*
   not sufficient to address the issues.

   Hence, even with _four_ infinities
   allowed for, it is conclusively noticed
   that at least several categories of
   inherent problem remain, and so that
   the difficulty of the problem is at
   some even higher level of infinity;
   excepting that there is some logic
   that even that will be insufficient.
   Even hypothesizing all of God/Goddess
   cannot overcome the underlying truths
   of basic mathematics.

   Therefore, it can also be noticed
   that all _less_ ridiculous questions
   and/or "hypothesis of principle"
   will end up having absurd outcomes too;
   ie; ones that also suggest that
   implementing human/world health alignment
   is fundamentally contrary to their
   own inherently artificial nature.

:note7:
   It is the observation of absurdity,
   that identifies for me
   why I felt it was 'worth'
   my personal time to write this essay.
   Far too often,
   people in the alignment community
   are far too willing
   to entertain absurd assumptions
   in an effort to overcome the impossible.
   So I needed an example
   that was 'far enough out there'
   to put a stake in the heart of the matter.

   We can call it the 'zombie question':
     "what if we X, will AGI alignment
     maybe be possible then?".

   It gets asked over and over again,
   never realizing
   that it simply does not matter
   what X is.
   Impossible is impossible.

   Where/If the outcome 'Z' is impossible,
   then there simply are no
   causative preconditions 'X'
   to contemplate.
   Unfortunately, just like zombies,
   the brainless questions just keep coming back,
   wanting more and more 'intelligence',
   hoping beyond hope
   that "this time, maybe it will be different".
   In this, at least some of the members of
   the 'AI alignment community' are *insane*
   when it comes to "making" AGI alignment.
   This applation is via the strict definition
   of 'insane':
     doing the same thing over and over,
     and each time expecting different results.

   This is the ultimate irrationality --
   an absence in the applied belief
   in/of causation itself.
   It is anti-realism at its best.
   When such people, who also call themselves
   'realists' and 'rationalists', make such claims,
   they are therefore, in the enactment of
   a kind of hypocrisy too.
   This last is even moreso the case
   *especially* when such people
   are rejecting our rejection of
   the very notion of 'AGI alignment'
   as inherently and fully 'logically impossible',
   inclusive of, *any* causative engineering
   and/or algorithmic control effort or schema.
