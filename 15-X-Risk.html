<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Basic Meta Tags -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- SEO Meta Tags -->
  <meta name="description" content="Comprehensive AGI Risk Analysis">
  <meta name="keywords" content="agi, risk, convergence">
  <meta name="author" content="Forrest Landry">
  <meta name="robots" content="index, follow">

  <!-- Favicon -->
  <link rel="icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">
  <link rel="shortcut icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">

  <!-- Page Title (displayed on the browser tab) -->
  <title>Comprehensive AGI Risk Analysis</title>
</head>
<body>
  <p>
  TITL:
     <b>APS/AGI X-Risk Detail</b>
     <b>By Forrest Landry</b>
     <b>Oct 4th, 2022</b>.
  </p>
  <p>
  ABST:
     - a very brief partial abbreviated review
     of x-risk associated with the intrinsic nature
     of APS/AGI.
  </p>
  <p>
  TEXT:
  </p>
  <p>
     - where clarifying that all APS (and AGI in general)
     have at least the following properties:.
  </p>
  <p>
       - 'Agentic planning'.
         - as referring to the idea
         that the APS/AGI can (and do usually)
         make and execute "plans" (ie; take actions,
         and thus have an effect on/in/within environments)
         in pursuit of "objectives".
           - where the notion of 'plan'
           is both roughly and strictly equivalent
           to the notion of 'recipe' or 'algorithm'.
           - where the structure of a plan/algorithm
           may or might not be based on obvious,
           explicit, or declarative
           'models' of the world.
           - where the notion of "objective"
           might itself be considered/known
           in only an implicit way,
           and not even in an explicit, declarative,
           or observable way.
             - ie; that the objective might not be known
             to the Agent/AGI/APS itself,
             in any sort of self reflexive epistemic way.
             nor is it assumed/assumable
             that the/that 'objective'
             is known/knowable to any other agent
             (being or consciousness, human or otherwise).
         - where insofar as the APS/AGI is itself
         defined in an artificial/algorithmic way
         (as a kind of learning/adapting algorithm/system);
         then/that/therefore all AGI/APS <b>are</b>
         "agentic planners",
         at least within one/some specific domain
         (operating context, environment, loss function, etc).
  </p>
  <p>
       - 'Strategic awareness'.
         - as referring to
         where the actual algorithms, plans,
           (and also, therefore, by implication,
           the implicit presumed basis/models
           of those algorithms/plans)
         <b>corresponds</b> with reasonable
         and reasonably reliable/effective accuracy
         with the overall potential causative effect
         in and within, and maybe even over,
         the/their actual operating environment.
           - as inclusive of the operating context,
           economics of their feedback functions, etc.
           - as thereby (maybe indirectly)
           also over the real-world environment,
           and/or thus also over humans,
           over other life on the planet,
           on/over the total actual ecosystem/environment etc.
         - and where it is the case, intrinsically,
         that all such correspondence
         can only be <b>maintained</b>
         via ongoing sensory input,
         with continual attention
         to ensuring the accuracy and reliability
         of that sense/information input,
         is at least representative of the actual
         environment/context state, etc;
         (where that sense input is either in
         direct or indirect relation
         to the actual operating environment, loss function, etc);
         then/that/therefore all executing runtime AGI/APS
         cannot not also be, at least in some sense,
         "strategically aware".
  </p>
  <p>
     - where in summary; where insofar as
     all agents/agency (including that of human)
     inherently cannot not involve
     input, processing, and output,
     and where there is at least the potential
     of the eventual causative effects that output
     also shaping the overall environment in which
     the substrate of the agents/agency itself
     bases its existence on/within;
     <b>then</b> the key distinction of interest
     turns on the degree of <b>artificiality</b>
     of that agent and/or of the substrate basis
     of that agency <b>and</b> also, on the degree to which
     that agent/agency can/could also arbitrarily increase
     its input/processing/output effectiveness/capability
     (via 2nd order self/substrate modification operations)
     and/or its effectiveness/capability to shape/shift,
     the world/environment in which it operates,
     (via 3rd order modification of context ops)
     via some (cumulative) outcome of its outputs
     such that the <b>difference</b>
     of its own nature/substrate being artificial
     eventually becomes mirrored
     in/as a degree of increased artificiality
     in the (inherently shared/common) world/environment,
     such that the new/shifted world/environment/context
     is <b>eventually</b> no longer compatible with
     the underlying needs/requirements of the
     non-artificial agents/agency (ie; as inclusive of
     the human and also of all other planetary life).
       - ie; where like any animal and/or human species,
       that the artificial agent --
       like any agent, when given sufficient time --
       will modify, or indirectly cause to be modified,
       its ambient environment to be more artificial
       (ie; in ways congruent with its own nature).
       - that this cumulative environmental modification
       occurs just through the inherent nature of its operation,
       and is not dependent on some "will", "goal",
       or "objective function" that it may, or may not have.
       - that it is less important whether such modification
       (whether maybe occurring either slowly or quickly)
       occurs directly or indirectly,
       ie; for example, through the actions of its makers,
       than it is that it happens,
       and that it happens in ways
       that are specifically artificial.
       - this "mirroring effect" inherently occurs
       because inevitably some aspects
       of at least some types of outputs
       will feed back over iterative cycles
       into the contextually connected
       configurations of the agent 'signature'
       that produces such outputs
       continuing to exist and/or to extend themselves,
       and/or their capabilities, etc,
       as a result of its learning/adaptive character --
       which is itself necessary (in the long term)
       for the artificial device to be, and to remain,
       useful at all.
       - that iterative versions of the agent,
       and of its actions, as considered collectively
       over all instances of that agent, agent type,
         (and/or constellations or groupings of inter-related types,
         up to and including functional artificial singletons, etc),
       will shape the connected surroundings of the environment,
         (perhaps unknowingly, in many many subtle
         too small to be noticed aspects),
       to be conducive to its own (continued) existence
       and its potential future.
  </p>
  <p>
       - that these changes do not occur "just" in terms
       of the outputs of the code
       being expressed as effects
       in interactions with the connected surroundings/environment --
       rather that it is the case that
       the connected surroundings of the environment
       can itself be seen as imbuing a kind of
       "agentic force" or "pressure"
       on the agent --
       on <b>every</b> agent and agent type --
       to be part of a certain kind of agentic process.
         - ie; that there is a "Newtons Law" analogy --
         a principle "that every action
         has an equal and opposite reaction" --
         operating in the hyperspace of all potentiality,
         that suggests, insofar as there is a degree
         to which an agent acts on the environment,
           (technically on any context
           which is not already wholly and completely
           created by and for that one agent singularly),
         that there is a semi-proportional degree
         that the environment also acts on the agent:.
           - 1; to cause and <b>require</b> the/that agent/agents
           to subtly shift and adapt
           its/their own output patterns/configurations;.
           - and also 2; their overall capabilities
           so as to (and be able to)
           shape and re-shape the environment/context
           so as to be more compatible with the agent.
         - that this is a 'pressure' and 'force',
           as a kind of 'selection bias'
           on the effects of the code/algorithm output/actions
         that operates in ways that both
         encourage and require
         that the agent be/become the kind of agent
         which has these capabilities.
  </p>
  <p>
       - whatever the configurations of the environment
         (ie; aspects are outside the physically distributed
         code/hardware shell/substrate of the AGI/APS);
       that the effects of that environment/context
       are inherently going to be expressed through:.
         - 1; degrees of freedom of interaction across space --
         including all interactions in and within and with
         the hardware shell/code itself.
         - 2; the agent continuing to exist
         as part of the process
         of furthering the existence
         of the code/hardware/substrate
         that will itself instantiate
         such configurations of the environment.
  </p>
  <p>
  :2lu
     - where both narrow and general AI share the distinction
     of being artificial (ie, as having a different substrate basis
     than that of carbon based life, ie; animals and humans);.
     - that the main distinction between narrow and general AI
     starts with the degrees of influence being spread over
     multiple levels of (higher) abstraction
     and also over at least multiple different diverse
     kinds of {domain / environment / operating (economic) context,
     world(s) of action, etc}.
  </p>
  <p>
       - as roughly analogous to the distinction between
       plants and animals (non-mobile vs mobile),
       and also at a meta level, between animals and humans,
       as characterized in terms of the degrees of freedom
       as understood as degrees of potential abstraction
       in/within/with which such agents operate and act.
       - as that general artificial intelligence
       would have and be able to operate in
       a technical context/world/environment
       (such as the internet, for example)
       in/within/with degrees of (potential) abstraction
       very far outside of human capability and/or
       possibility of perceiving/understanding.
       - that this makes AGI an 'agent within that world',
       and already far more effective in that world
       in ways that humans simply could not ever be.
         - ie; that an artificial agent
         in an artificial world
         is more matched to that world,
         and thus able to shape that world
         than a (natural) human would ever
         be able to be
         effective as an agent
         in that (artificial) world.
  </p>
  <p>
       - that the main distinction of interest,
       in regards to agency,
       is <b>NOT</b> whether AI/AGI/APS
       could be more effective
       and/or more efficient
       in worlds/contexts/tasks
       in which humans operate,
       (and/or already operate,
       and/or have specifically created
       for in which to operate)
       but whether AI/AGI/APS will for sure
       be more effective, more efficient, capable, and agentic
       in important and consequential worlds/contexts/tasks
       in which humans do not yet operate
       and/or might not ever operate,
       at any point in the future --
       ie; <b>artificial</b> worlds of consequence
       made for purpose, by humans,
       maybe initially for private economic interests,
       but which, over time, will be most perfectly suited
       for artificial agents to be able,
       and inherently far more able,
       to shape to/towards artificial intentions
       and well being.
  </p>
  <p>
       - that creating artificial worlds
       is actually just as dangerous
       as creating adverse artificial agents --
       particularly where those technological domains
       are key/critical infrastructure and/or are entangled
       with the well being of human and/or natural worlds --
       insofar as doing so inherently creates a latent context
       in which forces of potentiality are increased --
       ie risk (by definition) is increased --
       that some agent will be created by/for
       and/or introduced into that world
       that will eventually be shaped by that world
       to have an implicit objective function
       that inherently favors its own interest and well being
       over that of any "external abstraction"
       related to anything outside of that
       wholly artificial (yet consequential) world.
  </p>
  <p>
     - that narrow AI extends into general AI and x-risk categories
     if, and when, and where, such correspondence
     (as implicit in 'Strategic awareness')
     can/could extend into an epistemic awareness
     of the overall effects of
     of gaining and maintaining
     excessive power/influence/capability
     over the world, environment, context, etc
     (whatever that environment is purely technological
     or physical or political/economic or otherwise).
       - that the APS/AGI would likely create, have, and maintains
       such correspondence, and such influence
       over the environment/context, etc,
       by also creating, having, and maintaining
       (increasingly accurate/precise) ongoing sensory awareness
       of that environment, context, economic situation, etc,
       and/or increasingly sophisticated/abstract models/plans
       involving that sensory/epistemic data, etc.
         - as a feedback cycle both in the 1st order sense
         of having inputs, processing, and outputs,
         but in the 2nd order sense of increasing/changing
         the means, manner, and method (the total bandwidth)
         of obtaining increased input/information, and
         increased processing capability/accuracy/reliability,
         and/or increased output capability/influence, etc,
         which itself can result in 3rd order process
         of shifting/increasing/influencing/changing
         the contexts/domains of those inputs,
         the contexts/domains of their internal processing,
         and/or/ of the contexts/domains
         of their output/influence/control.
  </p>
  <p>
     - that the net effect of this feedback cycle
     would/could eventually be:.
  </p>
  <p>
       - 'Advanced capability'.
         - as referring to the idea
         (that they can at least potentially)
         significantly outperform even the best humans
         on those specific types of tasks and/or skills
         which, when performed at advanced levels,
         have the side effect
         of granting significant power and/or influence
         in the current world.
         - where examples of such tasks/skills include:.
           - scientific research.
           - business/military/political strategy.
           - engineering.
           - persuasion/manipulation (of humans).
  </p>
  <p>
     - where obtaining significant preemptive power/influence
     in any one domain of human interaction
     (and/or of the world ecosystem)
     then/that it also becomes possible
     to leverage that power/influence
     into multiple other domains of action/interaction.
       - as via the dependence and inter-dependence
       of the domains themselves,
       and/or via any secondary paths/means of control,
       extortion, etc.
       - as for sure leading to high
       human outcome risk, harm, etc.
  </p>
  <p>
     - that creating agents which are
     much more intelligent than human
     is for sure dangerous (inherently unsafe)
     to human interests and well being.
       - where/if their "objectives" ever become problematic,
       that such agents would plausibly have
       strong and/or overriding instrumental incentives
       to seek power over (or in place of) humans
       (or any other carbon based life on earth).
         - as 'the instrumental convergence thesis'.
  </p>
  <p>
       - that increased intelligence/agency
       over and above that of human capabilities
       very likely necessarily implies
       a complete and total transformation
       of the common shared environment/ecosystem
       to the point of complete terminal incompatibility
       of/with all human and/or carbon based life.
         - ie; where considering/cite (@ "The Power of Agency" https://mflb.com/ai_alignment_1/power_of_agency_out.html).
         - as therefore a terminal extinction risk.
  </p>
  <p>
  :2pc
     - that therefore; it can also be asked:.
  </p>
  <p>
       - Q1; ?; is there <b>any</b> chance at all, over time,
       that processes/changes of/in/within the 1st order
       do not eventually (cannot not) result in
       at least some kind of processes/capabilities/changes
       in/of/within the dynamics of the 2nd order,
       and/or thus (eventually) in/of
       dynamics of the 3rd order?.
  </p>
  <p>
         - for one way, among many,
         of thinking about this,
         see (@ the IM Suggested Response https://mflb.com/ai_alignment_1/im_suggested_response_out.html).
  </p>
  <p>
       - Q2; ?; is there any possibility
       that some engineering/algorythmic code/method
       could somehow inhibit the 1st order process
       from eventually impacting 2nd order process
       and/or 3rd order process?.
  </p>
  <p>
         - for example; see (@ Galois Theory applied to AGI https://mflb.com/ai_alignment_1/galois_theory_out.html)
         (which will be expanded to show more
         of the methods of reasoning (hopefully soon)).
  </p>
  <p>
       - Q3; ?; is there any possibility
       that some engineering/algorythmic method/code
       (and/or learning/adapting system)
       could somehow fully and perfectly dynamically inhibit
       <b>all other</b> learning/adapting systems/process
       (and thus so inherently, self modifying systems)
       from eventually having/being/operating/becoming
       able to shift environment/context?.
  </p>
  <p>
         - for an example of some of the issues
         associated with 'learning' as an 'optimization process'
         see the first of (@ "Three Questions" https://mflb.com/ai_alignment_1/three_questions_out.html).
         - that there are a number of other further
         considerations/links to be added here.
           - ie; where any compositional form of 'system A'
           attempting to predict/control/constrain/correct 'System B';
           as based on prediction theory, modeling theory, game theory,
           and information theory, along with oracle problems,
           the limits of computer science tractability, etc;
           that there are specific impossibility inequalities
           that apply.
  </p>
  <p>
     - <b>if</b> it is discovered, and/or reasonably proven,
     (and/or even if it is just the case
     that there is a strong/significant preponderance
     of reasonable doubt, as noted in the links)
     that the answers to any/all of the questions Q1/Q2/Q3
     is/are 'no';.
       - where the three questions are structurally
       strictly isomorphs of one another;
       that the answers to any one of them
       is actually the answer to all three,
       though each of the different phrasings
       gives access to different ways
       of being able to know/prove the response.
     - then/that/therefore; it is far better
     to <b>not</b> use/permit
     (or even seek to develop, experiment with, etc)
     <b>any</b> form of generalized AI and/or APS.
       - as consistent with the precautionary principle.
       - that the hazards of such use (and abuse)
       will (for sure) <b>eventually</b>, over the long term,
       fully structurally outweigh
       a/any/the/all (presumed, hoped for) short term benefits
       of such use.
  </p>
</body>
</html>