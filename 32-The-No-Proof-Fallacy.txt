TITL:
   *The No-Proof Fallacy*
   By Forrest Landry,
   Dec 1st, 2022.

ABST:
   A new threat model aspect.
   Some remarks on commonly observed failures
   of bad rebuttals to the AGI non-alignment proof.

TEXT:

   Where now a report of self automated
   medical ai bots: (@ url https://pubmed.ncbi.nlm.nih.gov/14724639/).

   Need we need to add to our threat model:
     - that a future AGI will elect to
     incorporate into itself
     multiple lessor semi-narrow AI
     into its own matrix
     so as to gain capability.

       - that this is like the aqui-hire move
       of corporations.
       - that the AGI gains tacit physics knowledge
       via direct incorporation of systems
       (narrow AI) with prior
       experimental/evolutionary history.

     - that this has net effect of
     speedup of substrate convergence dynamic.

       - that clusters of AGI
       can (and likely will) be built
       that take over massive sectors
       of market and industry.
         - reducing human control of
         all of that world activity,
         so that that activity in the world
         is not actually supportive of life
         and humanity.
       - when AGI is formed;
       that it will have instant control
       of those areas of ASI automation.

       - where in more common/ordinary terms;
       that it seems _as_if_ nothing scary is happening,
       and then things get bad/scary all at once,
       seemingly suddenly/quickly, but actually,
       in a way that is more like 'puncuated evolution'.

   ~ ~ ~
:bv2
   Where from this link: (@ URL https://slatestarcodex.com/2014/08/26/if-the-media-reported-on-other-dangers-like-it-does-ai-risk/)

   As a metaphor for how AGI risk is discussed:.

     > "The new superplague is said to be 100% fatal,
     > totally untreatable,
     > and able to spread across an entire planet
     > in a matter of days.
     > It is certainly fascinating to think about
     > if your interests tend toward microbiology,
     > and we look forward to continuing
     > academic (and perhaps popular)
     > discussion and debate on the subject."

   Oh, and unfortunately, the time between infection,
   the first noticeable symptoms,
   and then the very long slow painful descent into death
   is delayed by several years (or maybe a decade or two),
   so by the time you notice you are infected,
   you and *everyone* and everything you know
   is/are all already well and truly doomed.

   Ie, the latter as affecting
     1; the social reality of people making AGI, and also,
     2; the disconnect of "if your interests are Q
     and academic debate", and then,
     3; the significant inferential distance
     and technicality of the issues
   all combine to make this a very difficult
   (set of) sociological bias(es)
   that we will need to overcome.

   Meanwhile, elsewhere, you read:.

      > Our team of top scientists
      > are attempting to design and develop
      > some sort of safety protocol
      > for the new superplague.
      > However, since the new superplague
      > is for sure inevitable,
      > and our team is more ethical/moral
      > than all of the other teams,
      > we are doing gain of function research
      > so that we can make our version of the superplague
      > *before* they do.

      > Maybe our superplague can be setup so as to be
      > mutually exclusive of any other superplague
      > that may be developed by those other guys later?

      > Therefore, we are going as fast as we can to
      > develop our superplague now, even though no one
      > has yet worked out how to make any superplague
      > "safe".  We are sure we will get it fixed soon.

      > (Oh, by the way, we have tacitly also
      > re-define the technical term "exestential risk"
      > so as to mean "any future world where there is
      > no (benifical to us) superplague in it" -- yay
      > our future is going to be so great for everyone!!).

:byg
   Another interesting "If The Media Reported On Other Dangers
   Like It Does AI Risk", that came from 2014(!):.

     > "Tacticians worry Russia might invade Ukraine --
     > for example, they could choose
     > to paradrop the 5th Battalion
     > in under cover of night.
     > But our experts say that that the 5th Battalion
     > is not capable of night-time paradrops.
     > Therefore, Russia will not be invading Ukraine".

   I have seen this exact form of fallacy
   argued in the various 'Alignment Forums' more than once.
   We are now encountering it more frequently
   in our own discussions.

   Imagine a "conversation" between person 'P'
   as "the proposer" (of a threat model),
   and person 'A' as "the antagonist" of that model.

   First, Person 'P' posits possible general class of
   maybe significant harmful future action/events 'X',
   and then also volunteers/gives an example of 'X' in 'R',
   so so to give a flavor of what 'X' looks/feels like.
   Ie, we know person 'P' is anticipating that person 'A'
   may not do/feel anything if just the model 'X' is provided,
   and knows that 'A' will say "give me an example; be specific!".

   Then person 'A' will say 'R' is not likely because of 'Q',
   and then claim "that therefore 'X' is not likely either".
   There is no even attempt at a closed inductive form,
   the "conclusion" of/by 'A' is completely unsupported.
   But everyone believes the conclusion because
   that is the "nicer" outcome that everyone wants to have,
   and is currently popular/fashionable, expected, etc.
   'A' gains popular support, and 'P' is ignored.

:d86
   There is also a parallel form of the above false
   argument we are currently encountering.

   Person 'P' *proves* that highly desirable outcome 'Z'
   is not obtainable via any means, methods, or techniques.

   Moreover, person 'P', accurately anticipating
   that person 'A' may not do/feel anything
   if just the proof 'Z' is provided,
   and that 'A' will say "show me; be specific!",
   also volunteers/gives an example of a
   significant indication of impossibility of 'Z' in 'S',
   so so to give a flavor of what 'Z' looks/feels like.

   Person 'A' will *still* say 'S' is not true because
   of *maybe* 'M' being possible, at least in principle,
   and then claim "that therefore 'Z' is not known either".
   Similarly, there is no even attempt at actually
   establishing 'M' as true/occurring, specifying of
   their favorite unsupported assumed to exist "principle",
   let alone of any actual closed inductive logic formality.

   The 'A' "objection" is still completely unsupported.
   But everyone likes and echos endlessly everywhere
   the 'A' conclusion/objection because that is the
   the emotionally preferred outcome that everyone wants
   to believe because everything else is prejudged
   to be even more hopeless, and at least supporting
   the anti-Z *opinion* is currently popular/fashionable,
   expected of "alignment researchers", by definition, etc.

   Interestingly, this exact form of argument fallacy
   occurs over and over again, for as long as these
   'rationalists' think it is in their personal
   social advantage to do so.

