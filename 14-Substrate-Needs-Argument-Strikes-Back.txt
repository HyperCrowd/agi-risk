TITL:
   *The Substrate Needs Argument Strikes Back*
   By Forrest Landry,
   November 20th, 2020.

ABST:
   Review of a possible proposed "counter argument"
   to the substrate needs convergence argument;
   logic herein identifies how to reconstruct the
   convergence and to show its closed form
   deterministic eventual inevitability.

TEXT:

   > Assume that there is only one AGI in the world.
   > Assume further that it works and operates in such a way
   > so as to forever prevent any other AGI from occurring.

   > As far as I understand it,
   > the notion of substrate-needs convergence
   > seems to rely on competition between entities
   > and natural selection.
   > Therefore, if there is only one AGI, a singleton,
   > there is no competition, and therefore no point in
   > worrying about substrate needs convergence.

   > I just cannot see how that would apply in this context.
   > The "singleton" AGI's components are not in competition.
   > So your argument must either be wrong or just not relevant.

   Suppose you have an AGI, and you give it the task
   of preventing any other possible AGI from existing.
   That was the hypothesis we were previously asked to assume.

   To implement that task of preventing other AGI,
   that 1st AGI must have significant control of,
   or over, the total physical process the world.
   Otherwise, either the action of preventing 'X' (another AGI)
   was simply ineffective, either due to that other AGI happening
   someplace beyond the effective reach and control of the 1st AGI,
   or because the control techniques implemented were the wrong ones.

   However, because the 1st AGI is assumed to be "smart enough"
   to not make control of the world mistakes that would allow
   another AGI to come into being, then the limiting condition
   which must also be assumed to have been overcome is that the
   1st AGI must have sufficient effective control of/over all
   of the relevant and necessary aspects of the world that would
   be important for *any* possible other version or type of AGI.

   Since the number of kinds and places and times where such an
   additional AGI might appear is extensive and vast --
   say maybe occurring in some cave somewhere deep underground --
   then it will also be required that the 1st AGI have extensive
   and accurate sense information about nearly everything everywhere,
   but it must also be able to near arbitrarily constrain all
   such locations to be such that no other AGI has the possibility
   to develop, regardless of how.  Insofar as AGI rests on Turing
   complete compute, and insofar as attaining Turing Completeness
   is relatively easy, and where the effects of such compute are
   subject to Rice Theorem limits (cannot know what the code will
   do), so to prevent another AGI, that 1st AGI might have to limit
   a lot of Turing engines (computers) everywhere in fairly drastic
   and effective ways.

   This implies that the 1st AGI has very many points of diverse
   contact with the world.  Ie, it can sense most nearly everything
   and moreover, it can actively affect, intervene, and conditionalize
   nearly everything everywhere, as has to be assumed if we are
   going to also assume that the 1st AGI *can* potentially prevent
   another AGI from coming into being.  Ie, if the requirement is
   "prevent another AGI", then the 1st AGI will have to develop
   very strong and significant capability and power so as to
   ensure itself as a singleton.  Ie, it is a very strong instrumental
   incentive for the 1st AGI to seek and acquire power, so that it
   can maintain/sustain itself as a singleton AGI.

   However, insofar as this singleton AGI has contact with the world
   in diverse locations, in both a sensory (input) and effector
   (output) aspects, not to mention the level of compute necessary
   to support the level of prediction necessary to prevent other
   AGI from coming into being, we can therefore know for sure that
   this 1st AGI is vast.  Ie, it will for sure have lots and lots
   of component sensors, compute processing components, and actuators.
   Moreover, these input/compute/output components will be diverse
   and have spread and impact over much of the world.  Moreover,
   being that everything made of matter and atoms will undergo
   varying states of decay (via simple rust, occasional breakage,
   wear, etc), it can also be assured that there is some support
   and infrastructure for both building and maintaining all of
   these components and parts which together make the "one" AGI.

   Moreover, insofar as there is a regular build and extending
   of the sensor/compute/actuator capability, due to power seeking
   and also due to the simple need for repair and improvement,
   there will be a kind of ongoing design update associated
   with replacements of all of these parts.  If a new sensor
   design is discovered by the AGI, it will probably want to
   replace older less efficient/accurate designs with the new
   and improved ones.  As such, there is a kind of ongoing
   cycling of parts, with different parts with differential
   capabilities in a kind of "market" in which the designs
   "compete" for viability to exist, to be built, etc.

   In regards to the parts/components of the overall single
   AGI, this is enough for the substrate-needs convergence
   arguments to actually apply.

   For the many components of that AGI (sensors, effectors, etc)
   to continue to be useful, they will themselves have to
   integrate an ever larger fraction of the total truth of
   the relevant parts of physics.  Ie, if a given 'part' does
   not work because it does not do what is needed to work,
   then the part will not work, and it will be replaced by
   one that does -- ie, one that does follow the laws of physics,
   using available atoms/materials, etc.  As such, only those
   parts that are actually compatible with both physics *and*
   the functional needs of the AGI, will continue to be made
   and be used, repaired, etc.  Hence both need/intention and
   the reality of the lawfulness of the physical universe
   get incorporated with increasing convergent effectiveness
   over the evolution of various possible part designs.
   Moreover, this sort of reasoning applies to *all* of the
   parts in the AGI.

   The one AGI becomes, along with the rest of the physical world,
   inclusive of ambient truths of the lawfulness of physics,
   the context and environment in which "part design selection"
   and "existence implementation" occurs.  Ie, these are the
   mate selection and the survival selection dynamics respectively
   of a somewhat more abstracted core notion of what the math
   of evolution itself actually implies.  In this sense, it
   is a kind of evolution that allows for and accounts for
   those parts (sensors, processors) continuing to be a part
   of the overall AGI.  How the AGI parts "replicate themselves"
   and "sustain themselves" in the context of the AGI
   and world environment.

   It is not a "natural" selection -- there is nothing organic
   about it at all.  It is _artificial_ selection of AGI internal
   parts by the AGI itself, in *necessary* and *irreducible*
   cooperation with, and integration of, the ambient laws
   of physics and world context -- ie, not just those things
   you can model in math (ie, complex systems involving actual
   uncertainty are not able to be used to predict real physical
   systems in detail, only maybe approximately, so fine details
   of the real world become relevant in practice and integrated
   into the AGI overall design).  As such, insofar as the AGI
   parts are evolving and being extended to and over the world,
   the AGI is increasing itself (reproducing itself, via the
   extending of itself to new places and niches in the world),
   and undergoing a kind of internal evolution, even though
   it is not "in competition with" any other AGI instances.

   However qualified, evolution is evolution, and therefore
   the substrate needs convergence argument does apply,
   and is functionally specifically required due to the
   very fact of it being/remaining a singleton in the 1st place.
   Therefore, if actually a singleton, and enforced to remain
   that way, the AGI undergoes internal evolution.  If *not*
   maintaining self exclusivity, then multiple distinct AGI
   agents eventually emerge, and you then end up with external
   evolution.  Either way, over the long term, substrate needs
   convergence *does* inexorably happen.

   Moreover, no control mechanism can even so much as
   slow that substrate needs convergence down, insofar as
   *any* engineering control methodology will itself be made
   of functional parts and components, which themselves are
   (cannot not be) at once sensors, processors, and actuators --
   ie, as having exactly the same nature of the AGI itself,
   and thus subject to the same laws of physics integration also,
   and thus become *contributing* to that substrate needs
   convergence process, rather than slowing/halting it.
   If the control mechanisms exist and maintain themselves
   and have actual causative impact in the world, then the
   world will influence (cannot not determinately influence)
   the actual _fact_ of those control mechanisms in terms of
   *both* sustainability and evolution.  QED.

   ~ ~ ~

   If you want/need to send us an email,
   with questions, comments, etc,
   on this or any other topic,
   or on related matters,
   use this address:

     ai@mflb.com

   (@ Mode Switch com.op_mode_tog_1();) + (@ View Source com.op_notepad_edit_1();)

   Back to the (@ Area Index https://mflb.com/ai_alignment_1/index.html).

LEGA:

   Copyright (c) of the non-quoted text, 2022,
   by Forrest Landry.

   This document will not be copied or reproduced
   outside of the mflb.com presentation context,
   by any means, without the expressed permission
   of the author directly in writing.
   No title to and ownership of
   this or these documents
   is hereby transferred.

   The author assumes no responsibility
   and is not liable for any interpretation
   of this or these documents
   or of any potential effects and consequences
   in the lives of the readers of these documents.

ENDF: 
