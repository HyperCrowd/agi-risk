<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Basic Meta Tags -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- SEO Meta Tags -->
  <meta name="description" content="Comprehensive AGI Risk Analysis">
  <meta name="keywords" content="agi, risk, convergence">
  <meta name="author" content="Forrest Landry">
  <meta name="robots" content="index, follow">

  <!-- Favicon -->
  <link rel="icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">
  <link rel="shortcut icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">

  <!-- Page Title (displayed on the browser tab) -->
  <title>Comprehensive AGI Risk Analysis</title>
</head>
<body>
  <p>
  TITL:
     <b>What about Gradient Descent?</b>
     <b>By Forrest Landry</b>
     <b>Oct 7th and 11th, 2022</b>.
  </p>
  <p>
  ABST:
     Some more question and answer dialogue,
     mostly about technical aspects related to
     learning and choice making.
  </p>
  <p>
     Also considers the meaning and nature
     of what would be required regulations.
  </p>
  <p>
  TEXT:
  </p>
  <p>
     - where listing acronyms:.
       - SGD; as Stochastic Gradient Descent.
       - AGI; as Artificial General Intelligence.
       - APS; as Advanced Planning and Strategy Systems.
  </p>
  <p>
  :ggu
     > In your essays, you do not particularly address
     > the selection effects produced by SGD,
     > which are <b>vastly</b> more powerful
     > than the selection effects produced by evolution.
  </p>
  <p>
       - that the fact that some effects are stronger
       in one sense, one domain, over one scope of time,
       does <b>not</b> mean that other effects
       are irrelevant in other scopes of time,
       other domains, or in other relevant abstractions.
  </p>
  <p>
  :gk2
     > SGD deserves more attention
     > than simply speculating
     > that SGD might select for "code variants"
     > that are somehow "bad".
  </p>
  <p>
       - ?; who said anything about the mechanism
       of selection of such code variants
       being in any way defined by SGD at all?.
  </p>
  <p>
       - we may want to stop short of something like
       a presumption, assumption, or declaration
       of <b>both</b> of:.
         1; "everything is SGD", and then.
         2; "SGD does not do X".
       - that it ends up being something of a
       "presume what you want to prove" problem.
         - ie; has no relevance to our argument/claims.
  </p>
  <p>
  :mrk1
     > That the odds of
     > a randomly sampled configuration of weights
     > being competent at self-reproduction
     > and thus of some kind of human subversion
     > are tiny --
     > far too unlikely to matter.
  </p>
  <p>
       - the replication process, at least initially,
       has absolutely nothing to do with such weights.
       - where at first; that <b>humans</b> are selecting
       which code AGI/APS variants get reproduced.
       - that they (the humans) mostly select
       on the basis of what succeeds in an environment,
       (ie; what is practical/profitable in a marketplace).
  </p>
  <p>
       - that we are simply noticing
       that the overall motion
       is one of, gradually,
       a shift in the environment
       in the favor of that AGI/APS replication capability.
         - ie; to make more, better, more automated factories,
         and then to notice what sort of conditions
         are needed within those factories
         as required to make more AGI/APS robots.
  </p>
  <p>
       - where/if we sight along the trend-lines
       just a little bit more;
       then, at some point,
       that the systems are selected
       for self repeatability --
       fully automated self production/reproduction,
       maybe even inclusive of self design/optimization
       to better both be profitable
       within the existing environment/context,
       and then; over time, to maybe shift
       the environment/context/marketplace
       to be one more allowing of profit extraction.
  </p>
  <p>
       - that we each notice, in this,
       that there is in that last aspect --
       a kind of _decoupling_,
       one that occurs in the overall economics
         (ie; in the operating context/environment)
       such that humans are more and more
       no longer a part of the economy.
         - that this is a trend that is important
         and has larger ramifications later.
  </p>
  <p>
       - where in the interim, however,
       we also notice that the AGI/APS
       design optimization process also
       has become fully automated,
       at which point, both direct environment feedback
       and maybe something of the SGD weights process
       might matter,
       if that is still relevant
       in the AGI design space --
       something we cannot now assume.
         - ?; does it become relevant
         that some neural network with SGD
         might have some implicit feedback
         to/towards artificiality benefit?.
         - as not obvious at all,
         at least at first (ie; today).
  </p>
  <p>
  :gs8
     > - ?; are you assuming away value alignment
     > by characterizing the "true" goal
     > of the AGI internals
     > as self-replication?.
  </p>
  <p>
       - no; not affecting of overt goals,
       nor even implying it is a single "true goal",
       that would be displacing all other concepts;
       rather that it is an implicit intrinsic motivator,
       whether objectively stated/known/knowable or not.
  </p>
  <p>
  :mrk2
     > Are you suggesting that we cannot, even in principle,
     > exert any kind of overriding selection pressure?
     > That argument does not make any sense at all.
     > People breed animals in all sorts of specific ways,
     > all of which end up 'overriding evolution'.
  </p>
  <p>
       Yes, we can/do implement 'selection pressure'
       in/with/within evolution process among animals
       by fully controlling/forcing their mate selections.
         - ie; by fully <b>denying</b> them (via full control)
         their own natural mate selection choices thereby.
         - as a kind of non-consensual slavery, etc (@ note 1 #note1).
  </p>
  <p>
       The total strength of 'the selection pressure'
       that we can exert (even under ideal circumstances)
       is actually only a very small fraction
       of what is actually going on --
       nature does nearly all of the work for us
         (ie; one forced macroscopic change is not
         equivalent to the billions of microscopic changes
         that occur in each moment in all of the trillions
         of cells that the creature is made of).
           - ie; that we can force mating patterns,
           yet we cannot create the animal children
           through some sort of manual process --
           we still need the animal code to do the
           animal process, in its own way.
         - that any genetic manipulation and
         "test tube baby process" we do
         is still very largely based on
         the code that nature had figured out
           (the hard way, by exhaustive 'try everything'
           and 'have absolutely zero subjective bias' --
           what nature/evolution automatically does,
           over the last billion years or so).
           - that we are good at making simple tweaks
           here and there, sometimes, in some ways,
           more or less like a hacker
           changing a few select lines of code,
           in an overall million line code base.
           - we can hardly call the hacker
           "the author" and/or the "developer"
           of "the whole system"
           when their total "contribution"
           is significantly less than 10^-10%.
  </p>
  <p>
  :cl1
     > So, what is the overall claim?.
  </p>
  <p>
       - <b>if</b> (and only if/where) AGI gets developed;
       that there is no possible/implementable technique
       that could both detect and limit
       more than a tiny subset of
       all of the possible locally selected-for
       code variants
       that would (eventually) cause/compel
       accumulative changes to the environment --
       ones that are inherently out of line
       with conditions that humans need to survive.
         - that such variants
         are inherently more likely
         to be made and selected for,
         simply on the basis of
         practical existential necessity.
  </p>
  <p>
       Where/if self-learning, self-defining/making
       machine architectures are made to exist,
       and if/where/when they continue to exist;
       that it is categorically impossible
       to prevent/block all (but a tiny subset of) variants
       of the computed (appx) digital 'code'
       from also being selected (inherently)
       to be causing of outputs/outside effects
       which in interaction with
       the rest of the physical environment/context
       will lead those selected variants to replicate
       at a more frequent and sustained rate
       across the (global) environment."
  </p>
  <p>
         Ie; neither humans, nor even some <b>other</b> AGI,
         could account for and/or ultimately
         counteract/limit all the outcomes
         of the (random) fluctuations/relations
         in/of underlying physical substrate,
         along with all the chaotic dynamics
         (as entangled with all of planet Earth)
         in a way that would limit such selection.
  </p>
  <p>
  :gww
     > Building AIs that can exchange code --
     > like bacteria exchange DNA --
     > would be Very Bad.
  </p>
  <p>
       Yes, and elsewhere we show
       that such code sharing
       is not, even in principle, preventable,
       if the notion of AGI means anything like
       "learning machine" or "adaptive".
         - cite (@ SuperIntelligence Safety https://mflb.com/ai_alignment_1/si_safety_qanda_out.html) for example.
  </p>
  <p>
  :gy6
     > Maybe this could be made into a good outcome?
     > An AGI could store multiple copies of its past self,
     > which then can be used to veto any decisions
     > that are 'too contrary to their values' --
     > and/or intervene if the current AGI
     > seems to be malfunctioning somehow.
  </p>
  <p>
       - ?; is this somehow a proposal
       for an "error correcting protocol"?.
  </p>
  <p>
       - where if so; then it seems to completely
       conflate simple boolean fixed choice "voting"
       with <b>all</b> even slightly more complex/abstract
       choice making process --
       as if all "decision making processes" are strictly,
       or can somehow be made, strictly isomorphic with
       (ie; somehow equivalent to, in actual practice)
       all other abstract <b>objective</b> goal selection processes,
       inclusive of selection of basis of choice process (values),
       inclusive of "alignment with human interests"
       and/or "safe" with respect to carbon based life.
  </p>
  <p>
         - oh, wait --
         ?; are those deeper value/alignment choices
         somehow to be treated as immutable, fixed,
         and yet abstract, totally coherent, relevant,
         whereas every other level/process
         of choice/selection/decision
         is to be treated as "optimizeable",
         and therefore as "mutable"?.
  </p>
  <p>
         - where given the assumption that all choice
         processes are isomorphic with voting;
         and therefore scale invariant --
         equivalent at all levels of abstraction;.
         - ?; how is this assumption of
         "that we can achieve AGI alignment"
         /not/ a complete contradiction?.
  </p>
  <p>
  :st1
     > Is the assumption you are disproving
     > maybe somehow also
     > an assumption that you introduced?.
     > If so, you defeating a strawman.
  </p>
  <p>
       We are actually making a specific claim --
       a kind of positively specified statement
       about a negative probability.
       What we are (here/elsewhere) claiming
       is that there are clear principles
       that show that it is <b>impossible</b>
       for any engineer, corporation, etc,
       to develop an AGI/APS system
       that in practice can <b>guarantee</b>
       that the actual likelihood of
       'a very bad thing happening'
       is at least strictly below
       some reasonable maximum allowable
       probability threshold --
       ie, one that reasonably reflects
       the consensus value of the combined
       result of the associated
       cost, benefit, risk tradeoff.
  </p>
  <p>
       The tacit assumption that we are disproving --
       the false hope that we are dispelling --
       is the idea that someone could
       maybe potentially eventually make
       and/or use some (<b>any</b>) type of AGI/APS system --
       even if done fully secretly and privately --
       and also have that action be,
       in any way at all, somehow "safe" for humanity,
       long term.
  </p>
  <p>
  :h22
     > Even if you somehow 'disprove X',
     > how well does this reduce risks?.
  </p>
  <p>
       In the sense of people/researchers/corporations
       no longer trying to themselves do an impossible X,
       and/or not allowing others to implement a false X,
       then maybe a lot.
  </p>
  <p>
  :h3l
     > Is there any way at all to do AGI/APS
     > that is not an x-risk, something that
     > at least moderates the problem a bit?.
     > Surely, there must be some exception.
  </p>
  <p>
       No, not that we can see.
  </p>
  <p>
       Either we actually fully account for
       the fact of the effects of artificiality
       over the long term,
       or we fail to do so,
       at our peril.
  </p>
  <p>
  :h4u
     > What about all of the other work of
     > research into how to do AGI/APS?
     > Do you see any value and/or benefit
     > in our (failed) attempts to make AGI safe,
     > at least seemingly in the short term,
     > even if we cannot do so over the long term?.
  </p>
  <p>
       To the extent that these results,
       such as improved interpretability, clarity, etc,
       can be applied to <b>Narrow</b> single domain AI,
       then at the moment, aside from the inherent
       inequality issues, I see no problems with that.
  </p>
  <p>
       There is no question that NAI is useful.
       It just happens that it is also the case
       that AGI <b>cannot</b> be _actually_ useful.
       Distinguishing these two, in practice,
       is therefore paramount to our species.
  </p>
  <p>
       Even when regarding full NAI deployments,
       we should be careful, as engineers/developers,
       and keep the precautionary principle in mind.
       Ie, when working adjacent to an x-risk area,
       it is far better to move <b>extra</b> slowly,
       and be very sure to get the right answers,
       then it is to make some (silly) mistake
       and accidentally kill everything/everyone.
  </p>
  <p>
  :h6q
     > So we must implement a perfect hard firewall
     > against a/any/the/all AGI/APS development?.
     > Inclusive of mass social commons sanctions
     > on anyone who even suggests slightly trying?.
  </p>
  <p>
       Basically, yes.
  </p>
  <p>
  :safe1
     > Is there is an actionable
     > a solution to the problem
     > you have identified?
  </p>
  <p>
     > Maybe we should have the AI industry --
     > as with any new tech industry
     > which is developing new technology
     > that is increasingly closer
     > to uncontrollable --
     > to regulate itself
     > to the same extent
     > that the automotive industry
     > regulates itself
     > to so as to ensure
     > that it does not cross the line
     > of designing uncontrollable products?
  </p>
  <p>
     > Maybe we could require research
     > in even the NAI space to produce
     > automotive levels of safety docs?
  </p>
  <p>
       The auto industry standards,
       stiff as they are,
       are way too low.
       It is a bit more like regulating a bio lab
       to not do gain of function research
       (making <b>worse</b> pandemic bio weapons)
       without systemic safety protocols.
  </p>
  <p>
       Yet even that is not at all enough.
       For example, even really bad bio agents
       simply are not potentially permanently fatal
       to the entire planet, <b>forever</b>.
  </p>
  <p>
       When the severity indicators
       are fully orders of magnitude worse
       than even full all out WWW III nuclear exchange,
       any sort of proxy legal limits
       simply do not seem very realistic.
       Even a very low 0.00001% chance of death,
       in this case, of everyone,
       of total (all species) extinction,
       of everything that is wild and beautiful --
       of all life and aliveness lost for ever --
       is just not acceptable.
  </p>
  <p>
       Asking the industry to regulate itself,
       has historically been a non-start,
       even for relatively benign industries
       such as with aircraft safety.
       Consider, for example, what the FAA did
       with Boeing big-corp, and the result
       of its '737 MAX' product/system.
  </p>
  <p>
       As such, bio-labs simply should
       just not ever do <b>any</b> sort of
       'gain of function' research.
       Just not at all, not for any reason.
       Not ever, not even with the very best
       "systemic safety controls".
       They simply do not have the right
       to make those kinds of choices,
       and take those kinds of risks,
       ostensibly on behalf of everyone else.
       Even the highest security labs
       still fail in unacceptable simple ways,
       at unacceptably high rates,
       far too often,
       and we <b>all</b> pay the price.
  </p>
  <p>
       When considering it internationally,
       that asking biotech to self regulate
       seems like a clear false start.
       There are far too many smart idiots,
       paid for by naive govs, doing things
       that ought to never be done at all.
  </p>
  <p>
       It is not even clear if international
       nuclear treaty regulation levels
       are sufficient in this case.
       We might need something stronger still.
  </p>
  <p>
       We would need to prevent the existence
       of supercomputers, of <b>any</b> accumulation
       of total compute beyond certain limits,
       much the same way we carefully regulate
       the total amount of plutonium
       that can ever be in one place, at one time,
       in any/every country in the world.
  </p>
  <p>
       In the same way that any/every atomic pile
       is required to be internationally inspectable,
       we will need to somehow verifiably ensure
       that any 'oversized compute pile'
       is also inspectable, and/or simply
       have it be prevented from existing at all,
       beyond certain ranges --
       ie, say anything within even 10%
       of human equivalent brain capability.
  </p>
  <p>
       As we are starting to see
       with some actual anti-trust movement,
       that Big-Co Data-centers
       simply are not in the public interest.
       Especially as now, very publicly,
       we are starting to recognize
       that basically any form of
       total compute inequality
       really represents a total inequality
       in choice agency, of real power --
       via the very nature of code itself.
  </p>
  <p>
  :note1
  </p>
  <p>
    - where remembering the 'people as pets' scenario,
    where some superintelligence decides to keep us,
    it is possible to consider what if the situation
    was reversed; as if something was breeding farmers:.
  </p>
  <p>
      - ?; I wonder if any farmer would be "ok" with
      some other alien mind 'forcing their choices'
      regarding whom they could (and/or could not) sleep with,
      fully including denying/depriving them
      ever having any lover(s) altogether, for life,
      if perhaps the AGI machine/system in some opaque way
      judged that their innermost code (their DNA, etc)
      was somehow "unfit"?.
  </p>
</body>
</html>