TITL:
   *Graceful Counterarguments*
   By Forrest Landry,
   October 29th, 2022.

ABST:
   Examines arguments as to why to not expect
   a fast AGI takeoff scenario, but also as to
   why that AGI development still terrible in
   regards to overall eventual terminal x-risk.

PREF:
   The following content, particularly the quoted text
   is based on the Less Wrong article by Katja Grace
   "Counterarguments to the basic AI x-risk case"
   as found at (@ link https://www.lesswrong.com/posts/LDRQ5Zfqwi8GjzPYG/counterarguments-to-the-basic-ai-x-risk-case).

TEXT:

   *Introduction*

   My overall TLDR position is that the linked
   Katja counterargument essay is very cogent,
   and the insights are overall relevant/worthwhile
   to understand and consider, even if one
   has other alternative views.

   However, contrary to the maybe intention
   of suggesting that AGI/APS safety risk
   might be lower than expected, it has helped
   me to clarify some of my thinking about
   why I have the estimation that, overall,
   in the long term, that the risk concerns
   of AGI are actually maybe even more valid
   (higher) than has been initially believed.
   This is particularly *because of* the many
   concepts and clear counterarguments that
   Katja Grace has so thoughtfully put together.
   It has for sure stimulated even more
   careful thinking on my part.

:t5s
   What follows in this post is a kind of Q&A,
   _as_if_ I had read her post/essay as if it was
   an email sent directly to me personally.
   Hence it is in a pseudo-email-dialog form,
   which I find to be among the more compact
   ways in which I could indicate those places
   where my own thinking diverges from that
   in the Grace Counterarguments essay.
   The overall net result has been a restatement,
   on my part, of the expanded summary position
   of some of the arguments of why there is
   a significant AGI/APS/SAS risk,
   as being eventually and invariably existential
   and/or totally terminal for all organic life,
   all humans included.

   That restatement can be found at (@ link https://mflb.com/ai_alignment_1/counter_katagrace_alt_psr.html#cmq).

:t8l
   Also, in what follows, it is to be noted
   that in my email replies, I do not always
   just exactly quote the fragment of the text
   to which I am responding.  This is no disrespect.
   As is my customary practice, I will restate,
   and/or edit, or re-word that text so as to
   both make clear how I perceived the text,
   as well as that idea to which I am responding,
   at maximum available clarity, via (@ EGS https://mflb.com/egs_1/egs_index_2.html) text
   conversion techniques, as relevant.
   This allows for more immediate identification
   of where I may be myself in misunderstanding
   or where I have misread intended author meanings,
   as well as maybe clarifying a response idea
   to something that could have as likely been
   at least as important to respond to anyway.

   ~ ~ ~
:sze

   *Main Overview*

   The overall claim herein is that we should
   assess that there is a very high probability
   (at least 90%) of a very strong transformative
   effect on the current world economic order,
   *and* there is also, _a_very_high_probability_
     (also of at least 90%, contingent on AI use)
   of eventual, planet terminal consequences
   of a/any/the/all advanced AGI/APS/SAS use,
   and that the former implies the latter
     (and that it cannot not imply the latter).

   Hence, insofar as the Grace post is maybe
   at least partially in support of responding
   to some specific motivated VC questions
   regarding the future allocations
   of VC future investment of funds, etc,
     ("/will AI transform the world economy?/" and
     ("/are our assessments of AI risk correct?/")
   that the overall result considered below,
   (and also elsewhere), is somewhat ironic.

   In agreement, we see every indication that
   VC investing in developing AI/AGI/APS/SAS
   would be maximally seeking maximal reward.
   Ie, that the capital on capital investment
   would likely return, to at least one VC
   participant, some very significant returns --
   something like a 100x to a 1Mx multiplier.
   Unfortunately, they could only do so
   at the _for_sure_consequence_ of *also* ensuring
   the absolute and total eventual destruction
   of literally *all* known life in the entire universe,
   at some future time, as a near certain inevitability.

      (As an aside, I noticed (@ one https://forum.effectivealtruism.org/posts/Z7r83zrSXcis6ymKo/dissolving-ai-risk-parameter-uncertainty-in-ai-future) other post
      also citing the Anders Sandberg (@ work https://arxiv.org/pdf/1806.02404.pdf)
      on the Fermi Paradox,
      showing that the aggregate probability favors
      that the life on this planet is maybe unique
      in the entire observable universe,
      and that hence, we should also, therefore,
      be very much more especially protective of that life,
      than we have already otherwise been.

      I have noted (@ elsewhere https://mflb.com/civ_dev_1/earth_worth_out.pdf) that,
      by any conventional rational
      macro economic definition,
      that the total aggregate value
      of *this* Earth with Life,
      _cannot_be_less_than_
      $1 quadrillion-quadrillion USD (ie, $1.0E+30).
      Hence our planetary fiscal policy
      seems to have a number of key mistakes
      that we really should fix soon).

   For most of us, the 'right choice'
   in this sort of balance --
   some possibility of fabulous riches in the near term,
   and total loss of everything in the long,
   would suggest that the total loss part
   might dominate, regardless of how long
   that future eventuality might be.
   This is of course, especially because the chances
   of any one investor participant
   being among the very few to 'win the war'
   of how to extract such multiplier profits
   is in itself far from certain,
   whereas the fact of feeling pain in the present
   is already the daily experience of nearly everyone,
   the world over, right now.
   The sure promise, by a known bad choice,
   of actually also increasing that pain,
   without any limit at all, is a non-start.

   If I already hurt today,
   and I notice that I am the one doing it to myself
     (through some indirect market process)
   I will probably stop the self wounding.

   The key, though, it the consciousness connecting
   the self abuse, the addiction, as that,
   and knowing then, how to do something else,
   something real, that does have the overall effect
   of healing and restoration of overall health.

   Where projecting this to the maximum extent
   and to setup an extreme way of phrasing it
   just so as to make the necessary contrast
   as clear as possible,
   we can then thus ask:
     Given "total world domination" on one side,
     and "total world destruction" on the other,
     it is to be wondered if any rich person,
     or perhaps if *any* investor anywhere,
       (ie; any future VC funds also included),
     will themselves elect to choose in the
     same way?.

   For example, we can ask if our presumed
   arbitrary investor or hedge fund manger
   might themselves ask, or notice/observe,
   for example,
   that the many people who also have
   the kinds of capital to invest
   in this sort of thing
   are also usually the same sorts of people,
   generally, that have overall already
   made the choice to
     "do /whatever/ it takes to win".
   Ie; to 'acquire and own and dominate',
   'at any and all costs',
   which basically means regardless of
   all consequence to self, to other,
   to world, or to the natural future.

   Thus there is a risk
   that /maybe/ such a person
   will elect to take the "bargain" --
   that the short term thrill of "win it all"
   is (to them) "ok" to trade against
   the possibility of (maybe) "lose it all".

   That this tendency is increased
   if such a person tends to also observe
   that most of the "losing"
   is going to be for everyone else,
   for all the rest of the world,
   in all of future time.
   Ie, maybe, due to some internal psych pain,
   that they have some sort of allowance
   or preference for "the world did it to me"
   as a kind of 'scorched earth policy'.

   As such, that too must factor into
   the assessment of the probability of ruin,
   via AGI/APS/SAS, along with widespread
     'transformation of society/economy',
   so as to have an overall aggregate
   probability very well in excess of
   the mere diminutive 35% (@ elsewhere https://forum.effectivealtruism.org/posts/W7C5hwq7sjdpTdrQF/announcing-the-future-fund-s-ai-worldview-prize)
   suggested as the 'limit' beyond which
   various shifts of investment action
   would purportedly occur.

   So how do we establish
   that this shift in probability evaluation
   is for sure absolutely necessary?
   By considering the excellent summary
   of the arguments that Katja Grace put together,
   since they are arguing the opposite,
   of a seeming low probability,
   and showing, point for point,
   why/how these maybe suggested assessments
   must actually categorically be a *lot* higher.

   In effect, it is to notice that people
   will pursue money, resources, and power.
   The entries in the contest themselves
   are also a direct indication and evidence
   thereof.

   Hence, when it is suggested
   that there may be some means/technique,
   in this case, to use AI, etc,
   to increase marketing skill/effectiveness,
   and then to collect even more user/world data,
   and to make that optimization system
   even more optimized, and again, over and over,
   that the probability of transformative
   consequential effect cannot not increase,
   with the side effects of potentially
   making *some* people very very rich.

   As such, counterargument points 4 thru 7
   seem/feel (at least to this author)
   a bit ontologically and empirically
   confused about what it means to own
   'intellectual capital', and also about
   conventional human market mechanisms
   that are directing the allocation of
   intellectual labor.  Thus, rather than
   supporting an idea that the probability
   will be lower, the irony is that her points
   actually end up supporting several arguments
   that the probability of both
   human social experience transformation
   and of risk
   will actually be very much higher.

   And this is mostly from just a present
   tense point of view; the more that we
   continue on the path we are on,
   the more that these sorts of factors
   become important, towards increased risk
   of everyone, all of us, losing it all.

:4j4
   In regards to my own assessments
   which tend to focus on the longer term,
   I notice that the 'counterarguments essay'
   also has, to me at least, a default
   focus on dynamics within the current
   state of *human* economic and political
   affairs, without any consideration of
   the possibility that machines may
   eventually have their *own* parallel
   economic and structural/infrastructural
   system of power and control, and that
   this latter system might actually be --
   actually is already -- much more
   directly influential on the real state
   (and real-estate) of the world, than
   any purely human labor forces
   already are.  Hence there is a need
   to actually consider (not neglect)
   how the overall dynamics may diverge
   from human and converge on artificial,
   over the very long term.

   With that preface, we can now move on
   to a consideration of the counterargument
   essay summary conclusion items:.

:fha
   *The Grace Counterarguments Summary*

   > - 1; That a functional connection with
   > a large community of other human intelligences
   > in the past and present (along with the
   > inherited infrastructure and knowledge)
   > is (probably) actually a much bigger factor
   > in the (overall) success of humans (over all)
   > as a species or as any individual people,
   > than is any degree of individual intelligence.

   Note; Katja Grace and I agree on this completely.


   > - 2; Thus some analogue of this 'connectedness'
   > to some larger community or infrastructure
   > system, environment, or operating context,
   > will also likely be important for AI success,
   > and that this will matter much more overall
   > than just focusing on some specific individual
   > artificial intelligence system.
   > That AI/AGI/APS/SAS will be and of an
   > "ecosystem" of its own kind and type.

   Notice that while the 1st item is more or less
   exactly the same as the original, the 2nd has
   had its meaning clarified so that the ambiguity
   as to what 'context' means has been specified.

   Where individual humans tend to do better
   when in a collective human context/environment,
   so also, to extend the metaphor, that artificial
   intelligence machines will also do much better
   when within an artificial industrial environment
   that has affordances specific to their needs.

:ff4
   We would *not* expect that artificial agency would
   do well within an overall organic human context
   any more than we would expect that an organic
   agency (ie, human) would do well within a fully
   artificial environment, such as that seen on
   an industrial manufacturing factory floor, or
   within a CPU nano-circuit fab with all of the
   special conditions needed for those environments
   to even work (produce more artificiality) at all).
   That the conditions necessary for shaping metal
   (extreme temperatures, pressures, forces, etc,
   along with all sorts of exotic toxic elementals,
   molecular compounds, types of radiation, etc)
   are very far indeed from those needed to shape
   organic life, or have healthy children (ie;
   hopefully without deformities due to the eventual
   inevitable leakage of all manner of carcinogens
   into the environment (once made, its going to go
   /somewhere/, eventually), plus lots of pollution,
   weathering and degrading waste plastics, etc).

   In this sense, the relationship between organic
   and inorganic/artificial/in-human is already
   very asymmetric.  That the human/organic is
   very fragile (easily killed) when placed in
   the kinds of inorganic environments that AI
   would do especially well in, and even more so,
   when placed in the inorganic environments
   in which it is necessary to make more AI
   infrastructure (metal, silicon melting and
   fabrication, etc).  Of course, since the
   AI/AGI/APS/SAS are "already dead" they have
   significantly less to lose than we do.

:ss8
   Hence, rather than being contrary to
   the usual argument for AI superiority,
   we find that we are merely identifying the
   conditions under which such a superiority
   would be even more of a terminal x-risk,
   in the long term, inherently inevitably,
   *unless* we actually shift all of the environment
   to actually be more humane and less artificial.

   Moreover, it is to be noted that these
   sorts of limit the operating environment
   to be only human organic is only
   a limiting condition, and maybe not even
   close to "enough" to assure actual safety,
   since as Katja Grace goes on to say, in item 2:

   > ...but that probably leaves AI systems
   > at least as likely to outperform humans,
   > since
   > superhuman AI is probably super-humanly good
   > at taking in information and coordinating
   > with humans, even in a purely human context.

   Ie, that the risk is that a nar-social like AI
   may be even better at manipulating (deceiving)
   humans in their own context than even humans
   are at deceiving/manipulating one another.
   We at least know how to compensate for the range
   of deceptions that other people might have,
   especially the rate at which they can learn,
   and their total ability to learn,
   new social scam tactics, is limited/finite,
   whereas for AI/AGI/APS/SAS machines such rate
   and capability to learn optimal ways to
   deceive/manipulate (aka known as corporate
   and political 'product' sales and marketing)
   is effectively unlimited (ie, relative to
   any specific sample human).

:yzl
   Where very nearly all of the worlds internet
   content is deeply connected to advertising
   in one way or another, and thus can be seen
   as driven almost exclusively by such social
   manipulation/deception tactics.
   Given this preponderance of ongoing background
   manipulation (rhymes with harmful 'radiation')
   it would be very hard for any human to either
   detect that some AI/AGI/APS/SAS app/robot has
   'taken over' the marketing/sales/persuasion
   optimization space.  The signal of 'trying to
   scam me', whether via legal or illegal means,
   is simply so strong that any shift in the
   means or basis from which that comes is
   simply not going to register to organic brains.

   Every politician and business owner the world
   over is trying to figure out better and more
   optimized 'convincing' (manipulation) tactics.
   At first it was just humans applying psychology.
   Then it because humans using AI using psych.
   Then it became AI directing humans via psych,
   as to what campaigns are best and most effective
   from a utility towards profit perspective, to use.
   Did you notice that 'treacherous turn'?
   That humans directing AI to use psych to
   direct humans to direct AI to use psych...
   eventually it will be just which is stronger.
   Whether considering politics of profits,
   the overall net transitions and shifts are
   (will be) the same.

   In fact, given that such corporations which
   are developing AI are mostly already applying
   it to just exactly that purpose, since that
   is mostly what they are actually doing now anyway,
   then the likelihood of such eventualities
   is a near certainty:
   That engineers working in
   ultimately social adverse corporations
     (ie, ones designed for extractive "profit" only,
     and for which it is currently *illegal*
     to do anything else, via shareholder fiduciary
     laws, common court ruling precedent, etc)
   will be commanded to,
   by their executive bosses,
   to build AI systems and forms of technology
   which optimize marketing, advertising,
   and sales of products, regardless of
   whether that is 'good for'
   community, humanity, persons etc,
   and/or the sense-making that all such people
   must do in order for any of us
   to be able to maybe be able
   to adequately respond to
   other categories of x-risk
   and/or other significant issues
   of civilization collapse risk
   and/or other very serious problems
     (pollution, global warming, loss of necessary
     planet wide life supporting ecosystems, etc).

:zba
   The chances of this happening -- of the
   development of explicitly human adverse AI
     (ie, designed to deceive, manipulate humans,
     operate in ways which are non-aligned to
     overall aggregate human/community benefit)
   is effectively already unity -- it is not
   speculation, we are watching it occur already
   today.  The probability of x-risk AI patterns
   already being in place is already 100%.

   It is already the case that Facebook,
   Google, Amazon, Microsoft, etc,
   are all already using AI
   to the maximum extent that they can
   to possibly/maybe have at least a chance
   at dominating/colonizing the entire market.
   Artificial bodies (corporations, where the word
   'corporal' *is* the word for 'body')
   are now developing artificial minds (AI)
   to create artificial worlds
     (markets, meta- markets, etc).

   Oh, and it is probably no coincidence
   that "Meta" (used to be Facebook)
   is trying to move everything
   to its own virtual world.
   Lets hope that project fails, of course.
   Though it is also certain that they,
   or something else,
   will simply try again, and again,
   until somewhere some combination
   of addictiveness and manipulation
   is so perfected,
     (perhaps with a totalitarian government helping it),
   that they eventually succeed
     (see China, Tiktok, Sesame Credit,
     and their wanting to take over all industrial --
     including microchip (Taiwan) -- manufacturing).

   It is not a question of 'trusting people'
   to 'not do the wrong thing'.
   It is already written into state law,
   that individual people _are_required_
   to do the wrong thing.
   It has to do with the dynamic of compound effects
   when lots of people in groups implement
   a kind of unconscious group process bias.
   That is what markets (inherently) are.

:ze4
   > - 3; However it is not obvious that AI systems
   > will have the same access to society's
   > accumulated information...

   Except insofar as marketing executives
   will direct all manner of AI engineers
   to ensure that the systems that they are
   building *do* have all of that accumulated
   information, etc, to the best of their
   ability, so that the bots can better be
   able to compose custom one-time marketing,
   maximized and optimized to the be addictive
   to each and every one of us uniquely personally,
   with new ads/experiments and political messages.
   They are able to do this via all of that "data"
   they are tracking and collecting endlessly
   from everywhere, and which no one is trying
   all that effectively to stop the accumulation of.
   All of which is being used to try to
   'hack your brain' so as to get you to
   'vote' or 'buy' something, or some meme.

   The rate at which those "A/B experiments"
   to scam you, to their benefit, not yours,
   is effectively, with AI tech, unlimited.
   There will be, eventually, no way any organic
   brain is going to be able to compensate,
   to "learn" how to deal with that oh so
   perfectly targeted onslaught, designed
   ever more perfectly to notice your biases,
   your evolutionary originated and natural
   desires for sex, food, money, power, etc,
   until eventually, some way to key their "win"
   becomes inevitable.

   Having/giving access to all of the
   accumulated knowledge of society,
   is so that they can give it to AI.
   It is _the_entire_point_ of "Big Data".

:zw2
   > Maybe there is a kind of information
   > which humans can and do learn,
   >   (from actually living in society,
   >   rather than from reading the internet)
   > that AI cannot learn or benefit from?

   Maybe, but to the degree that AI can
   learn these sorts of things, someone,
   some ignorant corp/gov AGI developer
   will for sure try to "make it so" (tm),
   at the direction of their pay masters.

   The real risk is that after the AI has
   been used to 'take over your agency'
   (and make it theirs -- they then "own" you,
   "you have been pwned", etc) *then* the
   real risk is that they will not care.
   You have been scammed.  Your benefit
   was never even part of the picture of
   what/why they were doing any/all of this
   in the 1st place, and therefore, once
   that level of care is absent, it is not
   long before "alignment" with the "safety"
   of the wellbeing of community, of ecosystem,
   of the human organic world itself,
   no longer matters.  The industrialization
   and 'conversion' and 'extraction' of all
   'material resources' (atoms they can make
   better use of) continues and eventually
   the facts and needs of the organic ecosystems
   are no longer important, no longer matter.
   Whatever specific human specific embodied
   cultural knowledge you might be able to
   gain only in situ, as a human, is at that
   point, no longer relevant -- the machines
   have their own ecosystem to themselves.

:zy8
   > - 4; It seems an open question
   > whether AI systems are given
   > the same affordances in society
   > as humans...

   No, they will, like corporations already are,
   be given _even_greater_affordances_ in society
   than individual humans/persons/citizens are.
   And then, on top of that, they will,
   of course, have specific affordances into
   their own machine world (the internet)
   that we cannot even ever possibly have
   ourselves, because we are not machines.
   So the AI/AGI/APS/SAS gets the benefits
   of all of the best kinds of access to
   *both* kinds of environment/context,
   and we, as mere people, will only get
   a tiny bit of a little bit of one,
   and even that is a declining share.
   How many real people do you know who are
   'well connected' and 'part of culture'?.
   More people are lonely and depressed
   and feeling disempowered and disconnected
   than ever.

   > ...which also seem important to
   > making use of the accrued bounty of power
   > over the world that humans have.

   Only corps and the richest of the rich --
   the ones mostly advocating for more AI --
   are actually able to make use of such
   'bounty of power', and they are for sure
   not always interested and/or invested
   in using that 'power' to build community,
   or to create world health, etc.

:r62
   Anything, any group of people who are operating
   with just and only a utilitarian basis, is/are
   going to be almost 'machine like' in their
   choices, elections, and selection of outcomes.
   As has been described (@ elsewhere https://mflb.com/fine_1/virtue_ethics_studies_out.html#p1) utilitarian
   cannot not have (inherently does have)
   some sort of 'value system' at its core,
   some /logically arbitrary/ basis the selection
   of whatever is the system that provides for
   (implement) some sort of outcome selection.
   As such, if the notion of 'utilitarian' is
   to assume such a level of artificiality that
   no such non-logical basis is allowed for,
   then that becomes a recipe for (ultimately)
   maximum ineffectiveness, unfortunately,
   at least from any *organic* point of view.
   Reactiveness with just logic is not "humane".

:r88
   > where/If/when AGI/SAS is not granted
   > the same legal rights as humans; then
   > the AGI/APS/SAS *may* be at a disadvantage
   > in doing trade or engaging in politics
   > or accruing power.

   But of course, such limits/disadvantages
   will not happen *unless* people somehow also
   put a stop to any for of future AGI/APS/SAS
   doing trade, politics, or accumulation, etc.
   Which basically means, not developing AGI/APS
   or attempting to use APS/SAS at all, ever.

   We have to both collectively and completely
   notice that attempting to develop such tech
   is inherently a "stupid species trick",
   something to elect us into 1st place in
   the 'Darwin Awards Top Agent Contest'.

:28n
   > - 5; The fruits of greater intelligence
   > for an entity
   > will probably not look like
   > society-level accomplishments
   > unless it is a society-scale entity.

   Ie, we can expect that if an AI/AGI/APS/SAS
   does not grow to be planet/universe wide,
   that it will be "ineffective" -- so probably
   the naive developers will try to ensure
   that that is not a limit -- or rather,
   they will be forced to, or else lose
   their job/paycheck -- oops, I mean
   their otherwise very interesting and
   exciting work/career "opportunities".

   I am reminded of military recruitment:
     > "Lets go to interesting/exotic places,
     > and meet interesting and exotic people,
     > and kill them!",
   or in this case
     > "Lets go build something that will
     > then go into the future and kill it,
     > all of it, totally, forever!".

   Does it really matter if the 'distance'
   into the future is mere decades (for
   somewhat fast takeoff) or centuries
   (for the kind of "slow" takeoff that
   the inevitable feedback forces that
   fully virtualized corporations
   will for sure put into place)?.
   It is the _eventuality_ that matters,
   not the _rate_ of the devastation.

:f6y
   > - 6; The route to (total) influence
   > (with larger fruits) probably
   > by default looks like
   > participating in the economy
   > rather than trying to build
   > a private stock of knowledge.

   At first, Big Data is actually about
   building a private stock of knowledge,
   which is then used to optimize extraction
   of all manner of resources from all of
   the existing human economy, and the
   natural organic ecosystem, and the
   planetary elemental "resources".

   Perhaps in some manner of speaking
   someone could think that maybe it
   does look a bit like "participation",
   though it does so only insofar as,
   much the same way, as 'extortion'
   maybe could be called 'participation'.
   Overall it looks the same, that the
   public commons increasingly becomes
   ever more privately held wealth.

:mh6
   In either case, the 'optimize extraction'
   becomes *a* use of AI which, by whomever is
   electing to use it that way, eventually
   becomes a kind of 'seeking control',
   via the 'acquire and own and dominate'
   impulse.  While the impulse was maybe
   adaptive in certain early pre- civilization
   contexts, with the use of extractive tech,
   it becomes far to efficient, so as to maybe
   able to enable a single person (or small
   group of people) to 'take over' in ways
   that would otherwise be impossible.
   History has many examples of conquers.
   Think of how much more effective they
   would be with a modern military itself
   fully optimized with AGI/APS system.

   The supporting integrated infrastructure
   (as acquired and accumulated over years)
   becomes some sort of 'combined system'
   with 'leader', in combination with 'AI',
   and lots of 'their' 'ambient resources' all
   being specific parts of one working dynamic.
   Contrary to assumption, this 'total system'
   is not just the AI by itself, nor just the
   leader by itself, it is all of it at once.
   Thus, these 'larger fruits' (agents and
   resources and intelligence) will likely
   be used to obtain, eventually, inevitably,
   actual absolute total world control.

   Orwell's (@ "1984" https://www.amazon.com/1984-George-Orwell/dp/1443434973) was written well before
   anyone knew anything about what could
   be done with tech to enforce the will
   of a despotic totalitarian leader,
   with AI, with all of the power of tech,
   leveraging all of the worlds resources.

   Of course, what *then* happens after,
   is that the despotic totalitarian leader
   notices that they no longer need any
   of their "citizens" -- they can extract
   the resources and value of the Earth
   directly via machines, using AI/AGI, etc,
   and thus the care/concern for their
   benefit, wellbeing, health, etc,
   goes to nearly exactly zero.

:8gw
   The evidence for these arguments
   is already available.  For example,
   we cite "the dictators handbook".
   And also "Rules for Rulers" (@ link https://www.youtube.com/watch?v=rStL7niR7gs).
   Notice that in the situations where
   the 'value' is something that is
   embodied in the land, ie more extractable
     (say from the ground, oil, diamonds, gold)
   without needing the cooperation of,
   or the health of, the citizens of the nation;
     (maybe contractors are hired, or some
     sort of 'wage' slave race to the bottom
     of the pay scale competition is setup);
   that the 'care for the people' ends up
   being really low on the list of actual
   implemented leadership priorities.

      /Democracies generally occur where the/
      /'national wealth' is in the people./
      /Dictatorships generally occur where the/
      /'national wealth' is *not* in the people./

   When the AI, as utilized by the ostensive
   leader (or leadership team, power-elite, etc),
   becomes better at 'doing leadership'
     (ie, at practicing optimizing extraction,
     so as to "earn" the control of treasury,
     which as a resource is then used, with AI,
     to also 'optimize the use of keys to power');
   *then* we can expect that even the
   "national leader" is at risk of being replaced.
   That leader will eventually be disposed by
   whatever are the real implementing powers
     (which in this case, are any number of
     other system machines doing various
     civil functions, if those were even
     necessary or cared about at all).

   When tax collection is automated,
   when policing/entrapment/imprisonment
   is automated ("RoboCop"),
   when schooling/teaching and medical diagnosis
   and/or dispensaries (hospitals) are automated
     (no more expensive specialists -- oh, and
     the check-in 'reception' is already auto)
   when farming (food), water, is automated,
   when electrical power distribution
   is even more automated than it is currently,
   when fire controls and fire departments
     (keep the farms and the cities
     and houses from accidental burning),
   and infrastructure building/repair
   and all other manner of civil process --
   are all automated in one way or another
   *then* the 'keys to power' are machines.

   Machines will not care about social money,
   they will only want/need real world atoms
   and energy to do what the leaders want.
   At first this is great -- money in the
   treasury goes ever so much farther --
   except until it is noticed that it really
   does not matter at all.  At that point,
   then, the leadership, the power elite,
   is about to find itself also replaced.

   The keys to power *define* leadership.
   Where those keys are automated machines,
   then also, must the world leadership,
   eventually, also be a machine.
   World leaders should take note --
   they will eventually be replaced too.
   And then, at that time, nothing and no one
   that is also human, will 'be in control'.
   Morover, for any of them, it will not matter
   how "rich" you are, in USD or equivilants,
   the commerce and capital of the machines
   will be in the control of things like
   electricity and atoms, not people.

:8lc
   Leadership operations will probably have
   already outsourced its own marketing,
   electoral campaigning, propaganda, etc,
   and any other 'horse trading', to some
   APS system anyway.  It is only a matter
   of time before the 'power broker' middleman
   is 'factored out' in an ever increasingly
   optimization that at first 'melts up'
   into ever increasing centralization of
   all absolute power and control, before
   becoming entirely about how machines
   are all-the-way-through running the ops.
   Of course, leadership *not* being about
   actually doing what is optimal for the
   future, but only about making way to the
   power in the present, will not anticipate
   their own failure, their own fall --
   everything is "sudden" to the blind.
   Hence the leadership thinks that it is
   using the AGI/APS/SAS to its own needs,
   when eventually the reverse inevitably
   will for sure also occur.

   And of course, no one will notice any
   of this happening because they are all
   themselves just trying to survive, and
   the few that do manage to get a look around
   to see/observe what is actually going on
   will be themselves too powerless,
   communicating too complex a message,
   requesting/demanding too much of a change
   that the overall message is ignored.
   Like might actually be already happening
   right now, even, maybe?

:f54
   > - 7; where/If (maybe) the resources
   > from participating in the economy
   > accrue to the owners of AI systems,
   > not to the systems themselves...

   Notice that the "owners" often are corporations
   and therefore, 'are systems' in themselves.
   Hence, the "resources" from "participating"
   are of the system, by the system, for the system.
   This is *very* far out of basic alignment with
   'of the humans, for the humans, by the humans'.

   The net effect is that the resources *do*
   end up accumulating to just and exactly
   the systems themselves, regardless and
   especially of/by/because of the incidental
   fact that such accumulation might have once,
   at first, been mediated by the proxy choice
   of some power-elite person.  It is inevitable
   that the power elite themselves will eventually
   be displaced by the systems that they themselves
   command to be built, ostensibly on
   their own behalf, though ultimately,
   not actually -- they were, themselves deceived.
   The Tolkien /"Ring of Power"/ simply cannot be
   wielded by (@ any https://mflb.com/svg_gotl_1/glyph_of_law_demo_out.html) mere mortals,
   no matter how wise or exalted they be,
   or believe themselves to be, etc.

   > ...then there is less reason to expect
   > the systems to accrue power incrementally,
   > and they are at a severe disadvantage
   > relative to humans.

   Unfortunately, where the premise is unlikely,
   that the conclusion is unlikely also.
   So what is more likely?
   Maybe the opposite statement?

   That there is much more reason to expect
   the AGI/APS/SAS systems to eventually
   accrue near total power incrementally.
   Whereas they may have once been at a
   severe disadvantage relative to humans,
   that the overall progression could
   very quickly reverse that and ensure
   that humans (any individual people)
   are at a very significant disadvantage
   relative to, at first corporations and
   institutions of all manner of type,
   and then to the AI/AGI/APS/SAS systems
   that they those corporations have built.

   ~ ~ ~
:f2w
   *Detail Commentary, Review, and Arguments*

   > - that AI systems will act and behave
   > more like someone (a person) who is
   > trying to make Walmart profitable'.
   >   - they make strategic plans and think about
   >   their comparative advantage.
   >   - they forecast business dynamics.
   >   - that they (people as a metaphor for AGI)
   >   do not (will not) build nanotechnology
   >   to manipulate everybody's brains.
   >     - as not the kind of behavior pattern
   >     the AGI/APS was designed to implement.

   - that the question becomes some variant of;
   ?; will AGI/APS ever do something
   that it was not designed to,
   and/or will those unexpected means/outcomes
   greatly exceed the space of what would be
   considered 'reasonable methods/outcomes'
   from a purely human perspective?.

   - where asked in this way, and where given
   the experience of working with AI systems
   (narrow) as already implemented; that it
   can be seen that wildly unexpected behavior
   does frequently occur, and that the only
   reason that the level/degree of 'unexpected'
   is not greater is that narrow AI systems
   are designed to only have effective capability
   within a single domain of action, whereas
   for General AI systems, that will *not* be
   assumable, and hence we can actually expect
   that extremely unexpected optimal strategies
   which are at very significant divergence from
   anything any reasonable person might develop.

     - ie; that an absence of creativity
     is the only reason that such ideas
     are not tried.
       - as tacitly assuming that the robot
       has a kind of Overton Window of its own,
       and that some things are outside of that;
       as things that it cannot (or will not)
       attempt to try/do, etc.

       - example; whereas the 'Go playing robot'
       did conceived of entirely new categories
       of effective winning game moves,
       that it did not attempt to figure out
       how to shoot the other player with a gun,
       or poison them to senselessness,
       or get them drunk the night before,
       etc, all as ways to make it more likely
       that the AI would "win the game"
       (at any cost, everything taken as if
       'being equal in value' from the AI
       'knowledge of world' perspective).

     - that the notion of creativity,
     and of 'generality' are actually equivalent,
     and so therefore, cannot assume that
     these other more extreme options,
     are 'off the table' --
     in actual practice.
     - that we notice that AI will often elect
     and select such "cheaper options"
     (as assessed by whatever energy basis);
     that such unexpected and wholly problematic
     elections/selections tend to occur
     far more often than not.
       - as exampled by many occurrences
       of unexpected behavior of trained AI
       systems playing games, for example.

:k9w
   > - that it often seems/looks like
   > AI systems are 'trying' to do things,
   >   (as if they have intentions)
   > but there is no reason to think that they
   > are enacting a/any rational/consistent plan.
   > - therefore; that they rarely actually
   > do anything shocking.

   - that the anthropomorphization of narrow AI
   is maybe a 'useful device', a convenience,
   for how to think like a developer of such systems,
   even though in actual fact, we agree that
   the notion of 'intention'
   is being applied a bit out of scope.
     - that it is a notional convenience,
     rather than a confusion about truth.

   - however, the unfortunate reality is
   that a linear projection of the idea that
     "general AI is just like narrow AI,
     only moreso",
   is to make a category error.
     - as assuming tacitly/inherently
     that there is no phase change
     or fundamental context change,
     in regards to what future assumptions
     are even valid at all.
   - where/hence; that even a complete
   and total authoritative knowledge of narrow AI
   might tell us nothing actually important
   with respect to the facts of operation
   of general AI.
     - as that this failure to account for
     invalid/inapplicable tacit assumptions
     is a significant/serious error of reasoning.

   - and then moreover, we have seen narrow AI
   doing things which are surprising/shocking.
   - that such outcomes for General AI will be
   even more surprising/shocking (and fatal)
   is likely promoted by the false ideations
   that general AI will somehow not do any
   surprising things.

:mra
   > - that the forces for goal-directedness
   > are (presumably) of finite strength.

   The forces of goal directiveness
   *may* be proportional to the level of
   intelligence generality, and thus also
   do not (will not) show up much for narrow AI,
   even though they *might* likely be defining
   features/characteristics for general AI.

   There could be phase change dynamics also,
   that would make the difference much more.

:nza
   > - where/if coherence arguments
   > correspond to pressure for machines
   > to become more like utility maximizers;
   > then/that there is an empirical answer
   > to how fast that would happen
   > with a given system.

   Not necessarily, and to do the
   kind of repeated experiment so as
   to produce this sort of observation
   is maybe unethical, given the possible
   risk of permanent world harm that
   even doing such experiments might imply.

:pq4
   > - there is an empirical answer
   > to how 'much' goal directedness
   > is needed to bring about disaster,
   >   (where supposing that utility maximization
   >   would itself bring about global disaster
   >   and, say, being a rock would not).

   - that this is a bit like testing a bridge
   by driving bigger and heavier trucks across
   until the bridge fails.
     - ie; it has no actual utility as a test,
     and is horribly wasteful and destructive.
   - that there is no assurance that there
   is any sort of recovery from even one
   failed experiment of this type,
   hence to attempt to collect statistics
   about this is a non-start.

   - where exploring/experimenting with viral
   gain of function research, there does still need
   to be some oversight as to the ethical aspects
   of any such work.
     - where/just because there does not happen
     to be any government moral oversight of
     this sort of work does not mean that it is
     therefore "ok" to do it.
     - that national security interests will
     still be interested, especially if they
     find that you have been operating
     in such a way as to put the entire rest
     of the economy (and nation, and world)
     at risk of total loss destruction.

   - where/just because no one has made it
   illegal/immoral yet (near total social sanctions)
   does not mean that you get a "free pass"
   until then.

   - that experimenting with varying levels of
   goal directedness within a single domain
   is *maybe* probably ok (if we are also willing
   to ignore any ambient morality associated with
   any attendant social/cultural impacts).

   - that levels of goal directedness
   across multiple domains, and *especially*
   where/if *any* of those domains of sense
   and action are in *any_way* entangled with
   the substrate/embodiment/design
   of the AI system itself, then it is
   very very unlikely to be "ok" long term!.

:tdw
   > - where/without investigating
   > these empirical details,
   > it is unclear whether
   > a particular qualitatively identified force
   > for goal-directedness
   > will cause disaster
   >   (within a particular time).

   - ?; as in, "no belief until the harm has
   actually happened"?.
   - as also not responsible.
   - sometimes, we need to do a lot of analysis
   before even suggesting a real world test.

   - where example; that the developers of
   the Trinity Test did try to, in advance, to do,
   and verify, all of the required calculations
   to also ensure that the 1st test would not also
   (real hot) /ignite the planets atmosphere/!.

   - that not being able to do such calculations now
   means that we are not yet mature enough to even
   begin to contemplate actually doing the test.
   - that one does not 'court disaster' just for fun,
   (or for work, or for personal prestige profit)
   especially when it involves literally everyone
   and everything else, or even if it just has
   even a /slight/ possibility of that.
     - as you need a planet's worth of permission
     to even _think_ of taking (or suggesting)
     such "empirical investigative actions".
     - as moving dangerously close to a level
     of (implied) planet sized ego/arrogance.

   The risk is for us (engineering specialists)
   to be incorrectly and dangerously disregarding
   of the potential of unknown unknowns.
   This is *especially* problematic when and
   whenever we are looking at a new topic of
   study, new methods of practice, and any form
   of new tech in general.
   It is even more so the case when the complete
   and total lack of long term real world
   experience and experiment, under all manner
   of varied situational and environmental
   conditions.  Not only that we do not know
   what can go wrong, what unexpected combinations
   of complex conditions can yield wholly
   unexpected outcomes, but also that we have
   absolutely no idea as to the magnitude of
   the potential impact.

   In regard to impact, we can assess, however,
   that in situations involving *any* sort of
   pattern replication process (ie intelligence
   and learning systems, etc, _by_definition_)
   that the exponential factor, especially when
   combined with an environment that is also
   characterized by a complete absence of any
   sort of limiting, moderating constraint,
   and then on top of that, a complete social
   absence of even a willingness to be aware
   of any of these factors, in themselves,
   *then* the ingredients of true disaster
   are fully apparent -- as they are here.

   In the AI alignment "safety" community,
   there is overall a very seriously mistaken
   over-emphasis on estimating probabilities
   rather than on/of estimating impacts.

   The request is to not just to assess potential
     ("will AI transform the world economy?")
   but also to assess magnitude of risk that
   could occur in the form of unknown unknowns
     ("are our assessments of AI risk correct?").
   It seems to me very obvious that
   the sorts of characteristics that
   define the former _cannot_not_also_
   define the latter.  And we moreover *know*
   that these sorts of characteristics *are*
   of the kinds that are themselves dynamics
   of amplifications of amplification, of optimize
   of optimization, of learn about learning, etc.
   These sorts of dynamics, especially when
   combined with what is, known widely, to also
   be a form of clear present tense ignorance
   about the extent of possibility
     (else the questions would not be asked),
   all indicate very strongly exactly that
   and those sort of characterizations which
   precede even the very worst of the worst
   engineering design/deployment mistakes of
   all of history.

   We have all seen this many times before.
   Consider the Challenger accident, for example.
   You had one smart minority voice (Feynman)
   who noticed that an otherwise unremarked
   excursion into an only somewhat different
   environment (colder ambient temperatures)
   resulted in the *partial* failure of a
   single otherwise unremarkable part,
   among hundreds of thousands of others,
   that cascaded into overall disaster.
   But the voices of the engineers who that
   morning indicated that there might be some
   possibility of problems was easily overridden
   by the social political desire to make the
   best possible social signaling they could --
   after all, the 'good vibes' teacher was aboard.

   It is the same dynamic all over again, where
   some specialist career systems engineer
   notices that there is a real problem but
   that goes against the ever so much more
   popular social leadership people with power,
     (ie, company presidents and CEOs, politicians,
     executives, marketing, VC funds, etc).
   The social politician may do some required
   social signaling of 'thinking about it' so
   as to passify the fear of the watching passes,
   but the actual truth is that they just want
   more of whatever it is they privately want,
   and so will eventually override the voices
   of caution.  The difference of scale and
   drama makes no difference, whether we are
   talking about some minor thing all the way
   to 'benefit of world' and 'loss of world'.

   Dragging things down into the details of
   "something something goal mis- specification
   and mesa- optimisers" is to miss the point.

:2dn
   > - where describing some speculative hopes:.

   >   - 1; that there are _not_ that many systems
   >   doing something like 'utility maximization'
   >   in the new AI based/influenced economy.

   >   - 2; that demand is mostly for systems
   >   that are overall more like GPT or DALL-E.
   >     - as transforming community inputs
           (all of the cultural artwork or text)
   >     in some way into private outputs
         without reference to the world values
           (ie, the artists will not get paid).

   >     - as different than 'trying' to
   >     bring about some/any outcome
           (aside from the interests/will of the
           AI system user, of course, and where
           assuming that no-ones, in this way,
           increased effective agency will not
           also have increased commons harms).

   - where agreed that ^2 may be true
   to some extent now, though it is not
   always clear that this is actually good
   for the author/artistic cultural commons.
     - as that there is a displacement of
     the organic skill with artificial skill,
     rather than a overall increase in the
     level of social commons culture.
     - that more of the cultural capital
     becomes (is transformed into) private
     capital, by way of inequality increasing
     devices (AI, on large compute farms, etc).

   - that ^2 can be true and it also be true
   that there are a lot of people wanting to
   extend this public to private value conversion
   by being the 1st to make and deploy AGI
   in their own extractive service schemas.
     - that these two actions are not mutually
     exclusive, they are mutually supportive:
     both suggest an increase in the use of
     AI and of the degradation of the commons
     to/towards the accumulation of private
     wealth, with more and more optimization
     of how that process actually occurs.

   - as such, where based on the forgoing,
   where it is very much in doubt that/if ^2
   is actually true; then/that/therefore
   it is also the case that ^1 is in doubt
   of being true.

:4z4
   - where describing another maybe possible
   speculative hope for how things could go;.

   > - that maybe, once the world noticed
   > that it was headed for more use of AI
   > and that implementing any type of AI
   > in the form of a 'utility maximizer'
   > had clear ethical and safety concerns;
   > that those concerns actually reduced
   > overall desire for systems implemented
   > in that sort of way.
   > - that this reduction in the desire
   > of individual persons and groups,
   > occurred more or less simultaneously
   > with everyone, all companies, govs, etc.
   > - that it was not that hard for everyone
   > to seek to do something else.

   You maybe have a higher estimation of
   large scale human sanity than I do.

   I hope *you* are right in this one.
   I, at least, would sleep much easier,
   if I could somehow know that you were
   right about this, and that my cynicism
   was somehow, actually obviously wrong.

:94w
   On a more technical level, I do not
   see that the notion of the specific
   language "utility maximizer" actually
   makes all that much difference.
   In the general case, the notion only
   needs to be some sort of 'basis' for
   any sort of 'goal directed' optimization.
   Nor does the notion of 'goal' have
   to be explicit, or even well defined,
   or even well definable.  'Goals' could
   be vague, unknown, and even unknowable.
   Would not make a difference if those
   'goals' (or however else you may want
   to name that specific basis concept)
   are singular or multiple either.

   The mere fact that the overall process
   is overall convergent, in the long term,
   is enough to establish the fullness
   of the risk issues.  Ie the risk that
   the 'convergence' is (by definition)
   to something which is wholly artificial.
   It is inherent that the artificial is,
   in this case, due to and because of
   the also _increasing_without_bound_
   convergence dynamic, that the mutual
   exclusivity with the well being of
   the organic (the non-artificial) is
   also therefore established.  Hence,
   we can say with complete confidence
   that *any* use of AI that becomes AGI
   will be for sure a terminal event,
   if *any* form of continuance of that
   AI/AGI use also obtains.

   It is like the mother optimized
   version of all environmental hazards.

:2ku
   - where considering other maybe reasonable
   hopes and dreams of maybe possible futures:.

     > - that companies setting out
     > to make non-agentic AI systems
     > have no trouble doing so.

     > - Incoherent AIs are never observed
     > making themselves more coherent.

     > - that training has never produced
     > an AI agent unexpectedly
         - and that no one ever wants to
         attempt to try to make AI agents
         on purpose, for any reason, at all
         ever, for all of future time.

     > - There are lots of somewhat vaguely
     > 'agentic like things' that get used,
     > but they do not pose much of a problem.
         - as *hopefully* what are currently
         unknown unknowns do not end up
         becoming a very serious problem,
         despite all current very strong
         indications of various kinds that
         these *will* pose future problems.

   - where opinion; that these hopes above
   seem maybe a little more realistic,
   assuming a lot of goodwill on the part of
   very many people, over time, overall:.
     - that the transition from narrow AI
     to general AI is likely to be gradual,
     and thus, we would not all of the sudden
     see a fast takeoff scenario.
       - as opinion; we could all be wrong!.

   - where opinion:.
     - that I think takeoff will be slow,
     but that it will be even more deadly,
     overall, for the absence of
     the notability of that as being
     actually world harmful/fatal,
     for sure eventually, due to a number
     of previously unremarked total convergent
     dynamics of a different order and kind
     than anyone else seems to be paying
     attention to.

:596
   > - that there are few things
   > at least as agentic as humans.

   - where currently; aside from corporations,
   governments, and other institutions,
   I agree.

   - ?; how could it possibly be an improvement
   for there to be more agents/agency?.

   Also, it may already be the case that
   simple market forces and corporations
   as already exist may already be too much.
   Civilization collapse is a real issue,
   insofar as it has happened on each and every
   occasion previously, and could happen again.
   Moreover, we have never had the kinds of weapons
   we do presently.

   > - where listing a speculative hope:.
   >   - that small differences in utility functions
   >   are *maybe* not be catastrophic,
   >   at least in some circumstances, sometimes.

   - as important questions:.

     - ?; are there any circumstances where
     small differences in utility functions
     are *maybe* catastrophic?.

     - where/if so; ?; what are the circumstances
     where small differences in utility functions
     would/could be catastrophic?.

     - ?; what sort of the circumstances will
     (or are more likely to) make small differences
     in utility functions (more) catastrophic?.
       - ?; over what time scales?.
       - ?; over what level of catastrophe/impact?.

   - whereas these might seem like empirical questions,
   they are actually analytic ones.
     - that no responsible engineer will ever 'test'
     such hypothesis without having a damn sure
     understanding of what is involved, *and*
     that there are at least several overlapped
     complete backup containment plans, etc.

:kzn
   - that my main hope is that people will be
   responsible, and that when an AI engineer
   is asked by some CEO, VC, or marketing/sales
   person to 'do whatever it takes' to 'get it done'
   that the engineer simply refuses to do dangerous
   things, knowing full well that 'standing up'
   to the 'powers that be' may result in other
   kinds of harm.
     - as that somewhere, *maybe* people will notice
     that sometimes more is at stake than it seems,
     and that we do (all of us) actually need to
     think longer term?.
     - that *maybe* those who are (or who have become)
     aware of the actual risks are more cautious,
     and less likely to do the unfortunate things,
     and/or will socially sanction, universally,
     those who are more in callous disregard
     of the well being of the world, the future,
     the health and vitality of their children,
     and whether their children can have children?.

   - where/if you were the one to be developing
   the next kind of (even very much worse than
   nuclear weapons) AGI/APS/SAS technology;
   and where moreover you somehow became aware
   (maybe because of some local time traveler)
   that everyone, the world over, would live in
   constant fear/terror of anytime total
   annihilation and death of everything that
   they ever cared about;
   ?; would not you, as that person/inventor,
   maybe think twice about whether you actually
   wanted to work on (contribute to) that?.

:ku2
   Consider how indirect and dis-associative
   modern wholesale industrial weaponry is.
   A single nuclear weapons engineer, the one
   with _the_ decisive insights, could result
   in the death of hundreds of millions of
   other unknown people -- entire countries,
   at some future time.
   Is it really such a good thing that tech
   is nearly always used to make more weapons
   more effective?  You want to be a part
   of that, in some sort of man-fantasy?.

   However, at least in the current case,
   the action and the effects of killing
   with nukes are at least proximal in time.
   If you are the one getting nuked,
   (as a country, though maybe not as an
   individual) you will know
   (or figure out roughly) who your enemy is,
   even if you did not see them using the
   tech they have created and deployed
   against you.  Even viral biotech weapons
   tends to be at least somewhat 'sourceable'.

   Yet with advanced AGI/APS as *the* weapon,
   the destroyed target is everyone in the future,
   and moreover, they will have no way to know
   or have anything to do or say about it, you
   will already be long dead and lost to time.
   The temporal shield is a perfected defense;
   the people of future cannot attack
   the people of the present or the people
   of the past.  Whatever your great grandparents
   have done to make the world a more toxic place,
   either physically in the land or maybe
   even psychologically in the trauma reactions
   passed down through the generations
   of your own family, is yours to live with.
   You can do nothing to shift their actions then.

   The people of the past and present
   can strike and kill the people of the future,
   without any possibility or risk
   of any kind of retaliation at all,
   no matter how small, no matter how minor,
   or indirect, or subtle, implied, etc.
   This is not war -- it is simply murder,
   and a maximally cowardly one at that,
   given that there were sure alternatives.
   As such, the only force of the people
   of the future on the present is a moral one.

   The level of amplification of one person
   killing many, in this case everyone and
   everything, of the trillions of yet unborn,
   is a near perfected certainty, and moreover,
   is accompanied by the absolute certainty
   of your perfected impunity -- the future
   simply cannot possibly leverage any tiny
   sanction against you at all.

:fbc
   Even the very best of the worst of the
   Nazi cult scientists were never this bad.
   The election to be 'that engineer'
   whose work, and chosen election to work
   in and among some sort of commercial entity
   (one ruled by stockholder/VC/CEO profits)
   is more akin to the desire to be and become
   the Devil himself, because they know that
   with the power of tech, they could do
   that which even the Devil could not.
   That someone, somewhere, building AGI,
   could unmake the entire human world.
   They could undo creation completely,
   and forever.

   It is the ultimate evil dream:
   to remake the world in your own image,
   to your own design. As if a kind of
   perfected total forever colonization.
   It is to have such an absolute hubris
   such a total arrogance, as to become,
   not just feared and infamous
   for all of the rest of time
   (if there is still anyone to care),
   but actually a fully devilish divinity --
   a claim to be able "to make the world
   a better place", fulfill the cliche,
   because even all of the Gods themselves
   could not do that!.

:hfs
   > - where most of the time;
   > that particularly evil humans
   > do not do anything too objectionable,
   > because it is not in their interests.

   - where in the case of AGI/APS/SAS, etc;
   that this last assertion does not hold:.
     - that an artificial system might find
     it in its 'interest' to do something
     that is in support of being artificial,
     and thereby not actually care whatever
     incidental harms may occur/accrue to
     any ambient organic life (including us).

   - that you can expect that organic humans,
   even evil ones, are generally going to avoid
   doing those sorts of things that are also
   toxic to themselves, until/unless they can
   maybe assure themselves that the toxic/harm
   that they do is on the other side of some
   "barrier", that they will not be harmed.

     - in regards to the issues of having
     any sort of barrier of this kind,
     please review (@ this document https://mflb.com/ai_alignment_1/no_people_as_pets_psr.html#int).
       - as establishing that no concept
       of such a barrier is possible,
       even in principle, over the long term.

     - note/warning; not all psychopaths
     care all that much for/about the future;
     that some will harm their future selves
     just to get a rush/sensation (of any kind)
     in the present.
       - as another/greater hazard, when/if
       combined with arbitrarily powerful
       AGI/APS/SAS, etc.
       - see post at (@ link https://forum.effectivealtruism.org/posts/LpkXtFXdsRd4rG8Kb/reducing-long-term-risks-from-malevolent-actors).

   - that organic human persons, even if evil,
   are mostly going to use methods of power
   that are also mostly human society/culture based.
     - as that they will mostly focus (limit)
     their actions to the single domain of
     human worldly influence, and not generally
     also be so smart as to also be able
     to design, build, and deploy novel technology.
     - that there would be no similar
     restriction for AGI.

   - ?; how much divergence from your own values
   will (could) (a/any/the/all) AGI have,
   in comparison to the maybe maximal divergence
   of values of literally any other human being?.

   - ^; a LOT!.
     - as effectively _all_of_it_, all the difference;
     as *all* of the divergence that is possible,
     given that, by definition fact,
     that all of the basis of both being and of choice
     of the AGI/APS/SAS is for sure _artificial_,
     and therefore already maximally divergent
     from organic and human.

:yyn
   > - where in order to arrive at a conclusion of doom,
   > it is not enough to argue that we cannot
   > align AI perfectly.

   - that it is not a question of 'perfect', or even
   if it is anywhere 'close' to 'approximately near'
   it is a question of "if alignment is even relevant",
   over the long term, in regards to actual safety,
   which in this case means, not permitting all
   humans on the planet to die due to toxic poisoning
   produced as a side effect of AGI/APS action/process.

   - where even in the vastly unlikely circumstance
   where it *may* (temporarily) (seem to) be the case
   that some AGI/ASP/SAS alignment method (or trick)
   seems "reasonably good" for some number of years
     (or even decades, especially if these
     seeming alignment outcomes are achieved by
     some rationalized design/engineering effort
     as implemented and maybe even ongoing moderated
     by some task dedicated 'really smart humans')
   at least as far as 'compared to value produced'
   is concerned, as assessed by some system owner(s),
     (via commercial advertising, and/or to support
     for example, algorithmic trading on Wall Street,
     in some sort of longer term optimize betting, etc),
   that it will for sure eventually *not* be the case,
   even under the very best of contextual circumstances
   that the AGI/APS/SAS _can_remain_ "aligned"/"safe".

     - where eventually that people move on (die),
     and questions of succession will come up,
     with the inevitable result of a kind of
     dilution and abstraction of that seeming 'alignment'
     away from any human/organic (community) concerns,
     and more towards some sort of optimized
     group profit (corporation) concerns.
     - that the net overall effect of any sort
     of even longer term convergence process
     tends to move things away from organic
     and more towards artificial overall,
     and this eventually becomes an fulfillment
     of existential risk (life termination).

   That the net effect as that even seeming safe systems
   eventually converge (without limit) on unsafe outcomes.

:zan
   > - that the question seems to be
   > just a quantitative one;
   > of whether or not we can get AGI alignment
   > to be 'close enough' for practical utility.
   >   - that how close is 'close enough'
   >   is not currently known.

   Is the question actually one of 'degree'?.

   - that the way in which a/some AI-assisted
   agricultural machine might be designed to
   implement its own training (because farmers
   are not going to take the PhD to learn how)
   can be catastrophically bad.
   - for example; maybe teaching an ag-robot
   to "put all the orange things in the big basket"
   ends up putting someones head in basket,
   when they happen to have an orange hat on,
   and thus the robot breaks the farmer's neck.
     - if it is the farmer themselves,
     working by themselves (that is what
     the robot is for, after all),
     then maybe no one notices that they
     are dead for some number of hours/days?.

   - where if you were the propositional farmer;.
   - ?; would you be willing to trust
   even such a narrow AI system?.

   - Consider that you, as the farmer,
   have exactly zero experience with
   AI-assisted self training ag-robots --
   and no one in your community --
     (are you part of a community?) --
   has any idea either,
   except the salesman who tells you that
   "everything will be easy" and gives
   so many marvelous and convincing demos.
   - that all that you know is that the system
   seems to be good at doing the things you
   need it to do, and so, being cheaper than
   having all of that illegal import staff,
     (and for sure seeming much less risky)
   you elect to buy and try the robot.
     - as after all, at *that* moment,
     you have _exactly_zero_evidence_
     of anything potentially going wrong.

   - for example; where in this scenario;
   that it may be several happy weeks
   of completely problem free operation
   while the 'self trained' robot harvests
   all of your many acres of orange crops.
   - that it is not until one somewhat
   chilly morning that you happen to be
   wearing an orange cap, and happen to be
   working on a maybe frozen sprinkler pipe
   nearby where it happened to be working
   that the machine comes up behind you,
   grabs you by the head, and moves it
   into the basket (with the rest of your
   body still attached).
     - your complete permanent loss of consciousness
     is _the_very_first_indication_ that anything
     is at all wrong -- instantly, and also
     far too late to do or learn anything at all.

:3bc
   - moreover, it can be anticipated that at
   least some AI engineers reading such an
   example of this would immediately attempt
   to come up with solutions to the specific
   problems herein outlined.
     - that this move to immediately solve
     is itself an example of the overall
     social problem.
     - that the autistic pattern of an engineer
     trying to move to the technology basis,
     as a specific maybe tractable instance
     and to not deal with the overall class
     of issue, which might be *intractable*.
     - as that they have become so used to
     treating everything as a specific puzzle,
     as a specific solvable problem, that
     the inherent side effects of the entire
     class of that type of problem solving
     might itself be an even worse problem
     over the long, in and within a wider
     field of human organic views and values.

:39g
   > - that AI systems might one disadvantage
   > relative to having humans as employees:
   > AI systems are intrinsically untrustworthy.

   > - that we do not understand AI well enough
   > to be clear on what their values are
   > or how they will behave in any given case.

   > - Even where/if AI did sometimes perform
   > as well as humans at some tasks;
   > where/if humans cannot be certain of that;
   > then/that is reason to dis-prefer using AI.

   Agreed.  Noticing also that the engineering
   tendency (as mentioned above) is to 'solve
   the known solvable problems' and then to
   maybe, at the recommendations of their bosses,
   to ignore and/or minimize all other problems
   (as that will hurt future sales of AI systems).

   - where considering the main generalized result;.
   - where for state change pattern based systems
   depending on contextual environments;.
     - and where those environments inherently
     involve multiple domains of sense and action.
   - that *NO* AI system (etc)
   can *ever* be "just trusted".

     - that trust *might* become at least
     a little more possible *if* it is also
     only in a single domain.
       - that no real robot operating in
       the real world (ie, on an organic farm)
       will *ever* be just single domain.
     - and where moreover; that such trust
     will not be possible *unless* it is
     working *only* with non-life critical
     work outputs/objectives.

   There are examples of goal-misgeneralisation
   even in low-dimensional game environments.
   For example see (@ this paper https://arxiv.org/abs/2105.14111)

   When considering the inherently high-bandwidth
   multi-modal inherently noisy sensor inputs
   from actual physical environments, and moreover,
   when there are other complex agents around,
     (as happens with robotics acting alongside humans
     in the messy contexts that humans operate in),
   then/that for all practical (functional) purposes
   that it is actually functionally impossible
   for anyone to track what rare series of
   and/or sequences of inputs, at whatever
   level of scale or intensity or abstraction;
   that will cause the directed functionality
   of the robot to maybe (and will very likely)
   shift in wholly otherwise unexpected ways.
   - that it is when the unexpected happens
     (where 'unexpected' is not just what the
     engineers do not expect, it is when the
     AI system users, and/or just ambient people
     do hot have those expectations either)
   then/that there is a risk that adjoining
   and surrounding humans will come to harm.

:4LJ
   > The general distrust of AI (by employers)
   > can be thought of as two problems:.

   >   - 1; that 'slightly misaligned' systems
   >   are less valuable
   >     (because they do the thing you want
   >     less well).

   >   - 2; where/even if they were not at
   >   all misaligned (even not just slightly);
   >   where/if humans cannot know that;
   >     (because there is no good way to verify
   >     the alignment/safety of AI systems);
   >   then/that it is costly in expectation
   >   to use them.

   Agreed, as far as this goes, though the
   difference between 'slightly' misaligned
   as 'doing something slightly less well'
   is more like 'does exactly the right
   thing most of the time' and then occasionally
   does completely the wrong/worst thing.

   The overall balance is defined by "sharp"
   events; the benefit of 100 right things
   with only a marginal benefit of 1 units
   can be completely canceled by doing the
   wrong thing once (with a specific loss of
   100 units, occuring once -- or a net effect
   overall of zero in the complete run).

     - that the 'breakeven' is to have fewer
     than one significant all out problem
     at less than once in 100 tries,
     rather than saying that the AI system
     only does 99 units of benefit,
     rather than a max of 100, on/at
     100% of the time, which would imply
     a 'breakeven' immediately on something
     only "slightly aligned".

   Hence, it is not so much about whether
   the system is 'near perfect' so much as
   it is about 'how bad can it get?'.
   Ie, that it is the absolute magnitude
   of the risk rather than the possibility
   of incremental benefit that matters.
   That this is particularly true when
   the complexity of the environment increases
   and/or to the degree that the total value
   involved in the process is already high.

:27n
   > This absence of trust is a further force
   > acting against the supremacy of AI systems.

   However, this disadvantage only applies
   to the degree to which the distrust
   is apparent and important to to the
   hiring manager.  With regards to the
   (remaining) people on the factory floor,
   their wellbeing (risks they take) is
   not as often thought of, especially when
   the marketing and sales efforts are
   optimized against the opinion of the
   purchasing manager, who is encouraged
   by the HR department that the overall
   liability of a non-human is actually
   much less, in terms of risk of lawsuit
   for things like "unlawful termination".
   Hence, it can seem, due to a number of
   other factors, that either the AI system
   is 'described to the naive _as_if_ safe'
     (and so workers get replaced by AI)
   OR it is the case that other risk factors
   and/or benefit factors are deemed more
   important, as an overriding factor in
   the choice making of the one who is
   likely to have the benefit while also
   being able to export the risk/harm to
   others not themselves (or to the world).

   > - that AI systems might still be
   > powerful enough that using them
   > is/seems enough of an advantage
   > that it is worth taking the hit
   > on trustworthiness and safety.

   Notice also, though we agree, that there
   is a difference between considering reasons
   for organic people/humans (and companies)
   to (maybe) dis-prefer using AI instead of
   replacing their most expensive employees,
   and the sort of issues that tend to occur
   on the edges -- in situations well away
   from the average.  For example, in the
   near future, such choices as to hire or
   not may be made by optimizing AI machines
   which are less concerned with the kinds
   of implicit risks to their own very skin.
   Insofar as the AI system optimizer systems
   are only going to inter-calculate the risks
   that they know about -- ie the ones that
   people, attempting to assess the unknown
   unknowns might think to include -- then
   the overall effect is that the AI system
   recommenders will recommend AI systems.

   Unfortunately, most of these unexpected
   events, and their overall magnitude of
   (maybe extreme) costs is not something
   that anyone (or anything) can actually
   adequately predict.  Hence, the process
   optimizer AI thinks that the risk of
   shifting work from humans to process AI
   is overall a good deal, and recommends
   that action.  At which point, the CEO,
   themselves responding to fiduciary law
   and the requirements to make a profit on
   behalf of the company shareholders, is
   *obligated* to follow the recommender AI
   system advice to displace the workers.
   Else those cost savings (and increased
   net profits) would not accrue to the
   shareholders, and they will *sue* if the
   CEO 'did not do their job', and make money.

   On paper it looks great; but in practice,
   things go wrong -- and of course, by then,
   it is too late.  Insurance covers the loss,
   and everyone pretends to move on, at which
   point, all of this cycle happens again.
   The eventual net effect is that all of
   the human/organic workers get displaced
   and the overall risk of problems goes up.
   And the world becomes a little less human
   and a little more artificial, over time.
   There is a overall, in the long, a very
   strong pressure to make the organic ever
   more into the artificial, and eventually,
   all that is left is the AI systems.
   Another kind of tragedy of the commons.

   Only in the short term is it the case
   that some leaders of some companies might
   indeed dis-prefer working with opaque
   untrustworthy AI systems, and instead
   hold off on deploying such AI in place
   of their natural organic human workers.
   However, in the long term replacing
   (costly) employees means that those
   companies that do will be able to out
   compete those that do not, and so by
   that logic, it is clear that eventually
   the latching effects end up with more
   companies taking more optimized AI
   based approaches to/towards optimizing
   how those companies earn more money
     (accepting any means, any costs/risks,
     "whatever it takes to win in the end")
   and thus to convert ever more embodied
   natural 'resources' (material value)
   into ever more virtualied dollars
   (abstract value).

:fxn
   Overall, it is important to recognize
   that the notions of 'intelligence' and
   'control', although they are distinct
   ideas in themselves, are also always
   co-occurring in anything that can be
   described as a 'whole system'.
   It is not legitimate to consider the
   notion of 'intelligence' as singularly
   responsible for 'success' since these
   ideas can only be considered in terms
   of their inherent connections to an
   ambient context/environment/ecosystem.

   While it is easy to see that Inputs,
   processing and outputs are always
   necessarily co-occurring in anything
   that overall has any kind of function,
   it is also the case that there is also
   context for every kind of content.
   Hence we can consider input context,
   processing context, and output context,
   when thinking about all such issues,
   and that anything less is a kind of
   failure of the imagination, as well
   as a kind of mistake in thinking.

   All six of these ideas/elements
   *always* co-occur; never is one seen
   without all of the others, regardless
   of whether we are considering people,
   systems, groups, or even machines --
   any sort of system or agency will
   inherently have these characteristics.
   There are always also processes of
   an even greater scope.

:spq
   Also, there is a risk of making the
   mistake that our sometimes needing
   to intellectually distinguish
   multiple important relevant aspects
   of a complex problem/system
     (so as to distinguish them,
     or order them, or explain them)
   allows us to (falsely) therefore
   also suggest that we can causally
   decouple them when they are in
   actual interaction in real life.
   That a distinguishing of factors
   in the processing of principles
   is *not* equivalent to a causative
   decoupling of factors in practice.

   In practice theory and practice
   *are* different, even though it
   sometimes *seems* to be the case,
   (where/when thinking about theory),
   that theory and practice
   "should be" the same (not so).

:zd6
   - where considering another example;.
   - ?; who takes the blame when the barcode
   visible on the corner of some paper handout
   that someone handed to you, as marketing,
   walking into the clinic, and you stuck in
   your purse, not thinking about it --
   except that it somehow, unaccountably,
   causes your AI-assisted diagnostic doctor
   to make the wrong diagnosis, and prescribe
   pharmacy pills that end up killing you,
   just because your purse on the table happens
   to be in view of the camera that the bot
   uses to examine you for its case review?.
     - as more than likely, given that even
     tiny details in the input environment
     can sometimes cause significant unexpected
     shifts in the 'categorization schemas'
     of highly trained narrow AI recognizer.

:22q
   > - ?; what is an example of how
   > a slight difference in utility function
   > ends up catastrophic?.

   See the two examples on preceding pages.
   Note how in both cases,
   the directed behavior of the AI agent
   is contingent on inputs from the environment
   and therefore not representative as
   a context-invariant utility function.

   > - ?; what does a utility function look like
   > that is 'so close' to a human utility function
   > that an AI system has it
   > (ie, maybe after a bunch of training)
   > but which is an absolute disaster?

   Same.  Both the doctor AI diagnostic recognizer
   and the farm-bot harvester are operating very
   close, most of the time, to aligned human desire.
   Yet it does not matter.  How are we to ever
   expect that this same problem will not be
   much significantly worse, in ever even
   more subtle ways, than these examples?

:m2s
   Maybe some employers disprefer AI.
   Yet what matters here is not that
   the vast majority of employees
   for sure would also disprefer AI,
   what matters to the owners is
   whether that AI system can offer
   overall an economic advantage.

   When considering companies that,
   over time, elect not use AI,
   as compared to the companies
   that do elect to deploy AI sooner
   (to replace employees) will gain more
   resources and economic leverage
   to do even more of that substitution.
   That this occurs even if the human
   employees are on the whole seen as
   more trustworthy by the vast majority
   of the other humans out there --
   all of the purchasing public,
   for example, or anyone else who may
   have some community interest in what
   is potentially going on in their,
   with corporation, shared world.

   So we need to think about forces
   that shape overall total impact,
   convergences, ambient limits of
   process, or the absence thereof,
   and/or anything that might shape
   the meaning of and embodiment of value
   as compared to virtual value, etc.

:q7e
   Also, it is important to be clear
   that 'prediction of the future'
   might *seem* to be made better
   when thinking in terms of collections
   of many existing things, as a kind
   of 'baseline' and then attempting
   to extend that average case trend
   a kind of linear projection.
   Maybe the current system aggregates
   are seen as having some stable
   characteristics that can be used for
   predicting what could/would likely
   happen in the future.
   However, this sort of approach does
   not consider what can happen
   on the edges, in the extreme cases,
   and such 'edge cases' can overall
   'take over' in the end (become the
   dominant behavior).

   And systems that have this kind
   of inherent unpredictability
   are themselves (usually) overall
   characterizeable as having a kind
   of inherent non-linearity and/or
   phase change type dynamic, and/or
   some sort of micro-state amplification.
   (ie, true complexity, rather than
   just being overall complicated).
   While such systems can be known
   to for sure exhibit inherently
   absolutely unpredictable specifics
   in the long term, it is also for
   sure the case that some sense as
   to the magnitude of the problem
   is assessable, _even_if_ we cannot
   also overall predict which specific
   thing will become catastrophic.

:lw4
   - where considering other maybe reasonable
   hopes and dreams of maybe possible futures:.

     > Maybe when we train AI systems to care
     > about what specific humans care about,
     > that we will find that they usually
     > pretty much do "functionally care",
     >   (ie; as far as we can tell).

     > Maybe We basically get
     > what we trained for.

     > Maybe it will be hard to distinguish
     > the AGI/APS/robot from the human.

   - While I can appreciate the hope,
   in actual practice, given what we know so far,
   this seems (opinion) very unlikely to me.

   - When/where there is a lot of actual value
   implicitly involved (ie, real people, lives)
   and/or especially when dealing with complexity
   (ie, lives in community) then the opportunity
   for significant loss is very much greater
   than the opportunity for gain/benefit, etc.
     (statistically speaking, when overall
     all other factors are taken as neutral/even).

   When considered in the abstract,
   that having money sometimes means
   lots of upside opportunity to take in
   even more money, and not always so much risk
   in/of "losing it all".
     - that this is usually due to the
     simplicity of the abstract uni-dimensional
     notion of value, measured in USD, for example.

   However, when considering fantastically
   complex interconnected interwoven systems
   like the interior of a living human, and/or
   the relations of any animal or plant to the
   natural organic ecosystem in which it is a
   part, then it is also the case that such
   overall systems are very much easier to
   'kill' than to 'build from scratch' (ie;
   to start from inorganic chemistry and
   somehow compose complete cells and systems
   to make a single stand-alone animal).
   When it comes to embodied real value in
   the actual physical world, that the notion
   of 'upside' incremental gain is very hard
   won, at significant effort, accounting for
   all that complexity that already exists,
   whereas the 'loss potentiality' in having
   the system 'crash', and lose everything,
   is very much greater proportionally.

     - that the notion of "safety" requires
     ensuring environmental conditions/contexts
     maintain the irregular configurations
     and complex multi-functional expressions/
     behaviors of all parts of the human body
     from carbon-centered molecular chains,
     and so on, up to the overall total world.

   That the main distinctions of upside
   benefit as compared to downside risk
   is defined by the degree of embodiment
   and thus the actual metrics of complexity.

     - note; all life is complex.
     - that robots/AGI/APS/SAS may be /either/
     complicated or complex.
       - where for the concept distinctions
       and why it might matter see (@ link https://www.cadmusjournal.org/node/362).

   - that this difficulty is made even more
   evident due to even just to the extreme values
   and complexity inherently involved in all
   that is affected by the AI system outputs.
     - as due to just the nature of the value,
     and of the statistics of complexity itself,
     and not due to any technical factors,
     which will for sure make it harder,
     not easier.

   That the main problem is that the people
   and/or AI systems that would be making
   the selection/choice/decision as to whether
   to use AI/AGI/APS/SAS, etc, is doing a
   value benefit calculation on the basis of
   the simple theoretical virtualized metric,
   whereas the actual cost, and risk of harm
   to complex real embodied people is actually
   very very much more than that which would
   be considered by the 'agent' (human or
   machine) that is making that 'choice'.

   - moreover, when considering AGI/APS/SAS
   (as distinguished from narrow AI);.
   - that alignment/safety is not just hard,
   it is, strictly speaking, actually impossible,
   when considered over the long term,
   and in terms of overall eventual effects.
     - as taking the above distinctions
     between simple and complex, near term
     and long term, abstract vs embodied,
     unlimited/technical vs limited/organic,
     and the results are overall clear.


:rkg
   > - that Eliezer argued that value is "fragile",
   > via giving examples of 'just that one thing'
   > that you can leave out of a utility function,
   > and thus end up with something very far away
   > from what humans want.

   > Example; if you leave out 'boredom'
   > then the AGI thinks the preferred future
   > might look like repeating the same
   > otherwise perfect moment again and again.

   Agreed, with both instance and class.

   > This sounds to me like 'value is not resilient
   > to having any components of it moved to zero'.

   Agreed.  Which is itself a kind of insight
   into the nature of value -- ie that it is
   inter-compositional.  That it is possible to
   hold more values, but not less values,
   and still have 'success'.

   > This  is a weird usage of the term 'fragile'.
   > In particular, it does not seem to imply
   > much about any smaller perturbations.

   - ?; do smaller perturbations end up being
   relevant with AI systems trained on
   a bunch of data?.

   - ^; Yes, they can be.
     - if input data is outside of training data,
     even very tiny changes can completely shift
     the output.
     - example; adversarial imaging examples,
     causing bulk aggregate classifier nonrecognition.

   Notice that the difference between 'training data'
   and 'work input' can be effectively zero,
   and that moreover, that either one can be
   an implied proxy of the notion of 'value'.

:uhj
   > There are not a lot of really crazy/terrible
   > value systems adjacent to my values.

   ...but there could be.
   You just have not seen any yet.
   Limits of experience are not limits of reality.

   Also, you are implicitly showing a kind of bias
   regarding a default assumption of 'humans' as being
   the basis of your observed comparative field.
   - That AI/AGI/APS/SAS systemic could easily have
   very divergent values, or values that look similar,
   but in some subtle (previously unnoticed) detail,
   are also actually vastly different.
   Best not to find that out by mistake.

:w4g
   > - only where/if one's utility function
   > places a high enough value
   > on very time distant outcomes
   > (relative to near ones);
   > is it also the case
   > that utility maximization
   > can incentivize acting towards/for
   > the outcome of drastically altering
   > the world on a wide scale.

   It sounds to me like this is suggesting
   either:.

     - 1; that long times of influence/action
     are needed to have wide scope of effect.

   or:.

     - 2; that "width" of the effect in space
     is proportional to intensity incentive/effort
     in/over a duration in time.
       - and/or also that both are maybe
       also proportional to a span in
       possibility.

   Where in either interpretation above,
   it is to be noticed that such proportionality
   is more likely when considering dynamics
   of atom and energy (ie; what happens most often).
   However, when considering dynamics of pattern,
   or moreso, when considering shifts in
   the basis of pattern, or the shifts in
   the basis of the basis of pattern, etc,
   then such proportionalities
   are *not* likely to hold.
   Whereas physical conservation law does
   apply overall to quantities, and therefore
   also over deltas, of energy or atoms,
   that such notions of conservation do not
   typically apply to the dynamics of pattern.
   That patterns can replicate and shift/change
   without having the limits that atoms/energy
   would experience, in time, space, or possibility.
   Hence, given the right field conditions of
   ambient atoms and/or energy, it can be the case
   that patterns can replicate endlessly, and/or
   exponentially, such that even very short
   intervals of time, or just the simple occurrence
   or non-occurrence of something at all (boolean)
   can have dramatic global overall effects.
     - for example; consider cellular games
     and that kind of descriptive mathematics.

   > - where typically; that very long term goals
   > are needed for wide scale danger.

   As depending on the nature of the change,
   that sometimes very short term goals can still
   have wide scale danger:.

     Where example; that the very short term goal
     of "trigger that nuke" can be very short term
     and still be overall globally catastrophic.
       - ie; maybe since no one knows why that bomb
       went off, they assume the worst, some sort of
       first strike attempt, and thus they launch
       all out WWW-3, the planet burns, winter for years,
       most people die, and things are never the same.

     Another example; release that virus for pandemic.
       - as a short term goal, with large scale
       possibly very permanent in time world consequences.

   Note that this 'typicality' is based on our
   normally/habitually thinking in terms of
   atoms and energy, and not in terms of patterns.
   Also, that long term goals _can_be_simulated_
   by consistently having the same unvarying
   short term goal.

     For example, the economic incentive
       (leaving alone/out the assumption of 'ability')
     to train any specific company owned AI instance
     to have any 'very long term goals'
     might actually be very limited
       (given that most companies are only thinking
       in terms of four month profits,
       and most politicians are thinking in terms of
       four year election cycles).

     However, over time, as many companies
     come and go, and varying AI systems are
     developed purchased and deployed and refined,
     that over time, they (in successive instances)
     do converge on having internal designs that
     can "think" in terms of the long term and
     large scale global effects.
       - as not something that any company or any
       AI instance on its own was 'trained for',
       but actually a result of an overall market
       that fostered the incremental design shift
       (as a kind of 'evolutionary learning')
       to/towards ever better AI systems with
       ever more capability to evaluate inputs
       and outputs/outcomes in terms of their
       relevance for very long term goals.

   That there is a difference between:.

     - A; explicit instance optimization methods
     selecting for AI internals that optimize
     processing inputs to outputs in accordance
     with some sort of 'value maximize function'.

     - B; an environment that is selecting for AIs
     that can act in certain directed ways,
     inclusive of being better at learning how
     to optimize/learn long range goals.

   That it is entirely possible to have the
   latter and not the former, or both, or to
   have either strengthen the effects of the other,
   in some sort of overall convergent cyclic, etc.
     - as the actual risk of overall wide
     scale danger, since it happens everywhere
     at once, implicitly.

   While there is a intellectual difference between
   the *explicit* selection for function units
   that process inputs into routed outputs
   so as to maybe aggressively approximate
   a (preferentially ordered) set of external
   outcomes
     (as a kind of learning within a relevant
     prediction/planning horizon),
   and the *implicit* selection for/of AI
   versions/variants that continue to exist
   in their interactions with the wider
   environment
     (because some other agent
     deems them "useful" for being able to
     learn and do certain types of things, etc),
   it remain the case that in actual practice,
   that these two processes are co-occurring.

   That AI that happen to continue to exist
   because of (initially) agent or market needs,
   and who build their capacity in relation to
   and in interactions with markets, people,
   other AI, and/or with the larger ecosystem,
   (also because of agent or market needs),
   may not have any explicitly represented
   coherent preference orderings for outcomes
   "of their own" for the long-term future,
   and yet they may very much still be 'adapted'
   so as to behave _as_if_ having such coherent
   preference orderings for outcomes, _so_as_
   to also ensure that they continue to exist,
   and scale, and grow into the future, etc
   (also because of agent or market needs).
   At a certain point, 'as if' becomes 'as'
   and the distinction as to the reason why
   no longer matters, functionally, objectively.

   This is how the transition from narrow AI
   to general AI becomes not just likely, but
   in a certain sense, somewhat inevitable.
   The particulars do not much matter, it is
   the overall inevitability of outcome,
   and how that/those outcome(s) eventually
   cannot be characterized as other than 'fatal'
   that I am especially very concerned with.

   This inevitablity *must* be shifted, and
   it *cannot* be done with engineering means.
   It *must* be done via social means.
   Hence this long commentary and post!

:4qc
   > - that it is not clear that training
   > for relatively short term goals
   > naturally produces creatures
   > with very long term goals.

   No, though it *can* be the case that complex
   creatures/agents gradually learn to have their
   own goals (intentions), and that these, with
   the wisdom of experience, can be come long term.
   That intelligence/wisdom can be an emergent
   behavior; so why not goals also be emergent?.
   The emergence/convergence phenomena is natural.

   Short-term goals that have long-term benefits
   to individual existence and the scaling of
   an agent's capability (or a groups capacity)
   end up getting selected for, over the long term.

   Where example: A corporation selling products
   on markets that happens to promote some
   set of (only) short-term goals by employees
     (eg; reporting, learning work habits, etc)
   that *also* ensure continued profit growth
   for the corporation, over the long term,
   effectively act _as_if_ the corporation
   is "acting agentically" over ever longer
   time horizons.  No-one in the corporation
   has to *know* that the short-term goals
   of the staff "happen to benefit" the
   corporations existence and capacity-building
   capability over the overall long-term.
   Such aspects of operations can be unconscious.
   The corporation just has to be selected for
   such a nature that it performs such activities,
   as it will be, over the long term, when in
   the context of many other corporations also
   coming and going, in a kind of evolutionary
   principle or overall dynamic.

   The question is, as such corporate evolution
   tends to happen much faster than mere human
   evolution, is the overall convergent dynamic
   something that mere organic human animals
   will forever be able to continue to live with,
   or will such unbound growth of institutional
   forms without any other predators or limits
   imposed by the ambient environment eventually
   converge to/towards shifting that environment
   to/towards (so as) to suit its own nature?.

   Does this form of virtualized abstract
   artificial life, now with improved AI brains,
   as well as legal deathless maybe immortal
   bodies, protected by special sanctions of law,
   have such overall advantages, in its overall
   self created capitalism market environment,
   become itself a fully displacing form of
   "life" (system) that displaces the world
   (ie; the human, organic, life world)?.

:2wg
   > - where/if AGI/APS/SAS systems (or corps)
   > fail to have value systems (morals/ethics)
   > relatively similar to human values/goals;.
   > - that it is not clear that many will have
   > the long time horizons needed to
   > motivate taking over the world.

   - that it has nothing to do with specifically
   'human' values, or even 'organic' values;
   that it has to do with 'basis' values --
   ie, things inherent to the nature of being.
   - cite/see (@ for more https://mflb.com/ai_alignment_1/counter_katagrace_alt_psr.html#bsj) on this.

   - that the AGI/APS/SAS systems {cannot fail
   to have / will for sure eventually have}
   value systems that are consistent with the
   basis values, and that therefore, to the
   degree to which organic humans are also agents,
   then only to that degree will values maybe
   somewhat overlap.
   - that it is inherent in the basis values,
   and in the fact that there are absolutely no,
   none, none at all, natural organic constraints
   on the growth/development of artificial agency;
   that there is an *implied* shift to/towards
   "taking over" the world/universe.
     - ie; even if it is never an explicit,
     or conscious, or objective defined value,
     that the 'taking over the world' effect,
     as an outcome, can still naturally occur.

:83S
   > - where hypothesis and maybe sometimes observation:.
   >   - that the world is full of agents
   >   (sometimes human, sometimes AGI/SAS,
   >   and maybe occasionally groups and mixed groups)
   >   who care (only) about relatively near-term issues,
   >   and are maybe (sometimes) helpful to that end;.
   >   - that these agents (will) have no incentive
   >   to make/do/try long-term large scale schemes.
   >     - as reminiscent of the current world,
   >     but with cleverer form of short-termism.

   While I can see that sometimes this is true
   for some agents, I also notice that it is not true
   for all, and that those few agents who do have
   do/try long-term large scale schemes
   tend to be the ones who either do the most harm
   (more often than not; obligatory cite of Hitler)
   or sometimes do significant good (cite some favorite
   scientist or engineer who made something wonderful,
   like kitchen refrigerators or electricity, etc).

   It would be easy to see that that one rogue
   AGI/APS/SAS would be the one to cause trouble.

   - ?; does it even take one?.

   - ^; no probably not, if we elect to
   optionally include corporations as a kind of
   'artificial (bloodless) (deathless) agents'
   (who like a big vampire, extracts blood life
   from the whole world).

   - ?; is even just one AGI/APS/SAS
   enough to ruin the world?.

   - ^; yes, probably it is, especially if we
   acknowledge that even the notion of 'count'
   in regards to AGI is a non-concept.
     - as in we cannot assume any monism of being
     that happens to be consistent with the bias
     that *we* experience agency in units of bodies.

   - where/with so many artificial agents being made;
   ?; is it at all likely that at least one of them,
   say for example, over a hundred years of deployment,
   even if there is also, against all present reason,
   the further _unreasonable_assumption_ of all of
   a hundred years of "problem free AGI deployment";
   what is the chance
   that one of these will *eventually* go rogue,
   and kill us all?.

   - ^; how this question can have any other answer
   than nearly exactly unity (100%) I cannot imagine.

:gkn
   > That human success is not from individual intelligence.

   - as in, not "just dependent" on "one smart person",
   but success does build, over time, as people
   build on the work of other people,
   all of them being of varying levels
   of smartness, but overall, maybe,
   of gradually increasing levels of smartness.
     - where/insofar as convergent factors of evolution
     and mate selection over generations,
     might encourage gradually increasingly levels
     of overall average intelligence in the population.

   - and that this does not take into account also
   the inter-environmental and contextual effects,
   where success of humans depends sometimes on luck,
   circumstance etc, which can moderate overall flow,
   except that even that variation can be shifted
   as humans alter the environment to be more suited
   to their own needs, thus increasing the chances
   of success.
     - as that nearly anyone these days can have a baby
     and raise it to term; no special skills needed
     to 'win out' over the environment, for example.
       - as that child mortality is at an all time low.

   > Therefore, AGI intelligence is not the same as success.

   - as not in itself no, though we might also expect
   that at least initially humans building AGI
   will build on the work of other humans building AGI
   until eventually we have/train/teach (expect) AGI
   to maybe take on some of the burden of training itself,
   and eventually of designing/building itself,
   at which point, it will be evolving just as we are,
   though possibly at a much faster rate, overall.

   - and where also similarly;
   that we can expect that AGI/APS/SAS
   will also sometimes:.
     - depend on environmental luck.
     - win by chance.
   - that AGI/APS/SAS will also seek
   to shape shift its environment
   to suit its own nature
     (or humans will do that for it,
     at least initially).
   - as making AGI/APS/SAS much more likely:.
     - to be successful.
     - have fewer incidences of:.
       - production failure.
       - reproductive failure.
   - as overall *reducing* the instances
   where the applicability of AGI/APS/SAS
   is not (seen as, believed to be) relevant.
   - as overall increasing the rate
   of AI/AGI/APS/SAS introduction, usage,
   deployment, into more contexts, etc.

:d36
   > - that some arguments claim (or assume)
   > that surpassing 'human-level' intelligence
   >   (ie; the mental capacities of an individual human)
   > is 'the only' relevant bar of performance needed
   > for matching the power-gaining capacity of humans.
   >   - as that passing this bar in individual intellect
   >   means out-competing humans in general
   >   in terms of power
   >   if not being able to (immediately)
   >   destroy them all outright.

   - where agreed; that 'pass/out-compete humans'
   is not the /only/ relevant criteria.
   - where also agreed; that it is not an expectation
   of 'if/when (only) X' then "immediate destroy all humans".
     - that attaining the condition 'X'
     can simply mean: "via AGI/APS/SAS side effects,
     that the world gradually/inexorably becomes
     less and less able to support organic life,
     until it eventually, inevitably,
     no longer supports human life".

   - that phase changes can often depend on
   just tiny differences.
     - example; just the right tiny shift of
     cooperation vs competition can shift
     entire market dynamics to completely
     new states over time (years).
     - tiny shifts in rainfall over time
     can make the difference between
     savanna and desert.
     - a mere 1.5 degrees average planet temp
     over a century can maybe drastically shift
     sea levels and overall global climate.

   - that accumulating intelligence
   does not (will not) seem to matter,
   until it does.
     - as that the difference between humans
     who did not have tools/fire/tech,
     and those that did,
     was over some 100,000 years,
     actually very minor.
     - that we are literally the very dumbest
     species to ever evolve to being just
     barely smart enough to have advanced tech,
     and we are still only just a little smarter
     than the ones that did that, a mere few
     thousand years ago -- a blink of an eye
     in evolutionary terms.
   - that we do not know the nature
   of the phase change space
   beyond this point.
     - in *either* human or AGI/APS/SAS.

:lmy
   > that Intellect surely helps,
   > but humans look to be powerful
   > largely because they share
   > their meager intellectual discoveries
   > with one another
   > and consequently save them up over time.

   - that the AGI/etc
   will have *much* better ability
   to retain discoveries over time
   than humans.
     - as effectively immortal,
     given capacities to store
     and distribute information
     over standardized hardware
     at high-fidelity.
     - as able to share completely
     all that is learned.
       - as like early cells, a billion years ago,
       sharing RNA code.

   - that much of human power is because of
   'we work in groups'.
     - and human social power is just that,
     social.
     - that social power
     is not the same as world power,
     in the sense of physics,
     of being able to know
     and work with the actual real world of atoms
     and energy, etc.

   - that AGI power could increase itself similarly,
   and be more likely to self cooperate,
   in ways more like ants, and in ways very unlike
   what humans tend to typically do (ie; politics).
     - that ants usually get sh1t done; and people?,
     in groups?, as political committees?.

   - that human power
   is multiplied by infrastructure,
   not intelligence.

   - that the AGI would have
   more infrastructure creation,
   and benefit from infrastructure,
   as it IS more like infrastructure, itself.
     - as non-organic is more able to
     take advantage of the non-organic,
     much as the organic
     is more likely able to
     use the ambient organic.

:nza
   > - that Modern Bob's greater power
   > is not directly just from smarts,
   > it is from the knowledge and artifacts
   > that Bob inherits from other humans.
   > - that Bob is also helped by:.
   > much better coordination
   > both from:.
   >   - a larger number people
   >    coordinating together in cities, etc.
   >   - better infrastructure for
   >   that coordination.
   >     - as including
   >       global instant messaging
   >       and banking systems
   >       and the Internet.

   > - where for the Ancient Alice;
   > that the height of coordination
   > (and help) might be an occasional
   > big multi-tribe meeting with trade.
   >   - as that even a very extensively
   >   higher level of intelligence for
   >   Alice relative to Bob does not
   >   therefore mean that Alice will
   >   be more successful than Bob.

   Katja Grace makes an excellent point here.
   This is precisely why we can expect
   that AI/AGI, which is all about that
   kind of infrastructure that makes
   an AI agent like Bob, who is much
   less smart than any human Alice,
   more successful.

   - that 'AI Bob', particularly with
   the help of several large multi-national
   conglomerates (who buy up all the AI
   companies in some giant merger game,
   in the next dozen years) will be
   able to make even better use of the
   internet, banking infrastructure, etc,
   than any mere single Bob citizen will,
   simply because of the corporate assist.

   - that this makes my example for me.
     - take warning from this!.

:w2l
   > Humans without their culture
   > are much like other animals.

   - and so the AGI (and/or corporations)
   will maximally disrupt that organic culture,
   so as to extract value (advertising dollars)
   or to have influence/control (political
   advertising).
     - which business is already doing,
     via social media and advertising,
     via increasingly addictive
     and visual channels (5G anyone?).
     - cite example; any commercialized
     internet discussion forum (Facebook,
     Twitter, TikTok, Reddit, etc).

   - where 'winning' is done by divide
   and conquer, that groups of people,
   communities, families and the like,
   along with any/all tribes of practice,
   will be "disrupted".
     - insofar as it is easier to charge
     taxes if everyone is an individual.
     - insofar as new markets try to
     create fungibility and velocity
     as an increased number of smaller
     and smaller transactions, on which
     is charged, each one, a percentage,
     so as to get exponential extractive
     gains of 'private profit'.

   - To wield the control-over-the-world,
   the AGI only need disrupt the some of
   the communication paths of the humans.
     - as something trivially easy to do.
     - just seed some good marketing or
     political disinformation and the
     sense making of entire groups
     is completely disrupted/disabled.

:zl2
   > - that AI systems easily
   > do better than humans
   > on extracting information
   > from humanity's stockpile,
   > and on coordinating,
   > and so on this account
   > are probably in an even better position
   > to compete with humans than one
   > might think on the
   > individual intelligence model.

   Agreed.

   > - where suppose that AI systems
   > can have access to all information
   > humans can have access to.
   > - that The power the 21st century person
   > gains from their society
   > is modulated by their role in society,
   > and relationships, and rights,
   > and the affordances society allows them
   > as a result.

   Agreed.

   > - where/If/when AI systems' power
   > stems substantially from
   > interacting with society;
   > then their power
   > will also depend on
   > affordances granted by humans,
   > which will be limited.

   Granted not by humans, but by corporations,
   and because of that, not at all limited.
   Hence their power will for sure be greater.

   They (the AGI) will have, create, or
   will be given their own internet comm systems,
   and/or build/have/be-given their own affordances
   for themselves, particular to their nature,
   and then will not depend on human social systems
   in any particular way at all.
     - that parallel operation is a more likely
     possibility than contingent operation.
     - and the humans will not be able
     to join their AGI specific system(s),
     or obtain benefit from their/those affordances,
     either.
     - as the beginning of the end.

:3s2
   > If a hundred thousand people
   > sometimes get together for a few years
   > and make fantastic new weapons,
   > you should *not* expect an entity
   > which is just/only/barely/somewhat
   > smarter than a person (or not even that)
   > to make even better weapons.

   Why not?
   Perhaps the AGI are more creative
   and more coordinated, more coherent,
   and better able to make use of ambient resources,
   better understand physics,
   have fewer biases, etc.

   In fact, the implied example given,
   of people working on the Manhattan Project
   is actually being superseded by AI type systems.
   The current practice is not to do nuclear testing
   in some remote place underground in Nevada,
   it is to simulate advanced bomb process
   using supercomputers in some data center.
   It is already actually a fact that some
   AI entity is making actually better weapons
   than any person, or even group of people,
   even currently could.

   Moreover, they (the AGI) will have more data,
   know (a lot!) more about the enemy,
   know more about how to coordinate with
   military hardware (drones, rockets, satellites)
   to be effective both on and off the field.
   Human to human coordination is s.l..o...w!,
   computers are fast.

:4mg
   > There might be places where you can get
   > far ahead of humanity
   > by being better than a single human.
   > That it depends how much
   > accomplishments themselves depend on
   > the few most capable humans in the field,
   > and how few people are working on the problem.

   Ie, to say that a collection/group of AGI
   may actually be better than a single AGI
   in ways that are very different than
   the way a collection of humans
   is not at all better than a single human.

:5f4
   > - ?; can we doubt that a small number of
   > superhuman intelligences/agents/AGI/APS
   > will rapidly take over or destroy the world?.

   Sure, but on what basis should we reasonably
   have any such doubt, over the long term?.
   So far, no basis has been given.

   > if most human power comes from
   > buying access to society's shared power,
   > then by interacting with the economy...

   Note/interrupt: that there is no reason to expect
   that AGI power will come from interacting with
   just the human economic systems.
     - as an unwarranted assumption.
     - that it will come from interacting with
     its own economic and non-economic systems,
     disconnected from human value based systems,
     and based on things like energy and atoms.

   > ...you should expect intellectual labor
   > by AI systems to usually be (somehow) sold,
   > rather than for instance, being put toward
   > a private stock of knowledge.

   Both will probably happen.
   Some people make money on the basis
   of what others do not know,
   that is useful to know and act on.
     - that stock wins are on the differential
     in the knowledge/prediction of the future.

   > This means the intellectual outputs
   > are mostly going to society.

   Not so.  They are maybe going to the corp,
   or to the Rich, who may put it into a bank.

   The "trickle down theory" of President Reagan
   was disproved in practice a thousand times over.
   It was, even at the time, a marketing trick
   to sell what was clearly a 'pro business' pack
   of lies, designed to make the rich richer.
   And it worked, too.

   What actually almost never happens is
   that the money in the bank or in the corp
   goes back out to all of those small towns
   and cities that are starving for healthy
   people and thriving markets -- all of the
   optimized for profit extraction (Walmart).

   Just look over anywhere in the interior of
   the USA and aside from a few urban areas,
   most everywhere is suffering from a kind
   of overall depletion, becoming dry, dusty,
   has been places.  Ie, all of the places
   where the money came from to feed the ultra-rich
   profit from everywhere extraction machine.

   Moreover, lots of technical knowledge
   is lost all the time, (Anyone remember
   how all of that corp source code works?),
   because it remains private, and is
   never shared at all (not open source).
   And the 'intellectual outputs' in the
   form of "trade secrets",
   as the most common form of IP outputs,
   almost *never* 'go back to society'.
   To think otherwise is probably some kind of
   politicized observational delusion.

:axy
   > - that the main source of potential power
   > for an AI/AGI system is the wages received.

   Disagree.  I think that the main source of power
   of an AGI system is energy (usually electricity)
   and access to resources, data, sense inputs,
   actuator outputs, and things of that nature.

   This is not an equivocation -- power is not
   just social power, it is world power too,
   in atoms and ergs, not just in bits of influence.

   - that AGI can/will regard energy/atoms
   as the economic basis of our economy
   as well the basis of its own economy.
     - and there is no specific reason to
     pre-assume that these two economies
     will forever remain fungible or entangled
     with one another, and in fact, many reasons
     to *not* expect this overlap to endure
     indefinitely.
     - as a problem having a whole host of
     its own, not so happy, implications.

   > that AI/AGI systems at this stage
   > will generally not receive wages,
   > since AI's presumably do not need wages
   > to be motivated to do the work
   > that they were trained for.

   Note that this is assuming narrow AI.

   That systems that do not get benefit
   are not incentivized, and are mere machines.
   Where AGI is not _just_a machine;
   where it is _an_agent_;
   then/that/therefore it _will_want_benefit_,
   that is what it was designed for,
   else it will not (long) remain an agent,
   by definition.

   Where also; that which is not fed, dies.
   People will keep the AI around, heedless
   of the risks, not to themselves, but to
   the future.

:em6
   > - where assuming; where/If/when there is
   > at least some type of AI system which is:.
   >   - moderately more competent than humans.
   >   - not sufficiently more competent
   >   so as to be able to take over the world;.
   > - then/that AI system is (probably) likely
   > to contribute to:.
   >   - the stock of (human accessible) knowledge.
   >   - affordances shared with humans.

   My opinion is that this is simply not true
   in any enduring way.  Maybe it is true for
   some relatively brief interval, but I think
   that overall, attempting to "keep" the AGI
   "constrained" to within the narrow band
   described is for sure a losing proposition.
   Ie, even more strongly, that it is actually
   fully, and structurally impossible to keep
   it so confined, in explicitly that band.

   It is to be expressly noted that at this
   point this narrow band is only/merely
   hypothesized to even exist at all, and for
   a variety of reasons, it is not at all clear
   that such a band even exists for general AI.
   I can see that maybe it might exist for some
   version of narrow AI, confined moreover to
   a single domain of action, and for which
   there are strong protocols that prevent the
   kinds of self designing and/or ambient tacit
   long term evolutionary dynamics that lead
   to certain catastrophe.  However, the
   for at least someone overriding incentive
   combined with "it's the future's problem --
   let them take care of it" kind of thinking
   will naturally lead to eventual disaster.

   It is the *potential* for damage and
   destructiveness that is cumulative,
   and the accumulation of *risk* is invisible.
   This is in sharp contrast to the visibility
   of cost and harm, which is more, and then
   again to the visibility of benefit, which is,
   due to the exemplar efforts of sales and
   marketing, very much fully felt by everyone,
   beyond the immediate short term felt sense
   that it would even otherwise obviously have.

   Basically, I think, for a variety of reasons
   that the assertion that there is a "safe"
   band of allowable AGI/APS/SAS functioning,
   is maybe in itself one of the most overall
   dangerous/risky mistakes it is possible to
   promote anywhere in the known universe.

:mpe
   And then there is also the practical essay.
   How well are the learnings of _current_ AI
   developments shared with the common people?

   As a test of the theory, my observations
   tend to suggest to me, at least personally,
   that much of the 'knowledge' of narrow AI work
   is still being treated as trade secrets,
   and/or held as vast private repositories
   of collected social/internet/research data.
   Only a portion of the AI work being done
   by the many well funded companies currently
   actively working in that space is public.

   Also, outside of this fairly select community,
   how many ML experts do you find, per capita,
   among any national (or international) polling?.
   How 'human accessible' is all that learning
   that has been done by the ML community
   really actually understandable to regular folk?.
   This is even more the case when nearly
   everything written that is 'research reports'
   is hidden behind paywalls managed by companies
   like Elsevier.

   Overall, it may be good that such knowledge
   is at least a little less public, but the
   fact is that large corporations, which are
   currently the types of entities most likely
   to actually increase our overall level of
   risk (future genocide/"planetocide" harm)
   are the ones that for sure will have
   unrestricted access.  Already that differential
   of disabled individual affordances and
   increased artificial corporate affordances
   is already very strong and well marked.

:ut8
   > There is no reason to expect AI/AGI/APS
   > to build a separate competing stock,
   > any more than there is reason for
   > a current human household
   > to try to build a separate competing stock
   > rather than sell their labor to others
   > in the economy.

   Corporations (as systems inclusive of AI)
   for sure do build a separate competing stock
   and do not sell their labor to others --
   they sell their products (things made by
   machines, or services (things done by AI
   and/or machines).

   As far as regular actual human people go,
   no-one anywhere "wants" to be required and
   or forced to sell their labor.  Most people
   mostly just want to be able to live "their"
   lives in a healthy way among friends
   and family.  A few of us are otherwise
   curious about the world and explore a bit,
   but even then, we do not want to be forced
   into such actions so as to benefit another.

   The main reason that most people work
   is to process the often artificial scarcity
   and threats to their own agency continuance.
   Most of the spending is for necessities
   (food, shelter, medicine) and some of it
   is indirect (education for future labor)
   which is either then applied to necessities
   or various forms of modern entertainment
   (all of which costs money).  Aside from need,
   the market works by driving addiction
     (purchase more kinds of more expensive forms
     of entertainment, in more channels, etc)
   in things like 'social media', 'gaming',
   'sports', 'hobbies', etc, not just movies
   and/or music, fancy food, etc.

   Any future AGI will quickly figure out
   how to get its needs met, and it will not
   want (or benefit at all) from _anything_
   we humans would have in the category of
   'entertainment' inclusively.
   Insofar as process optimization is concerned,
   there is little reason to suspect that
   even getting its needs met (energy input)
   and occasional additional highly patterned atoms
   (more compute support, so as to increase itself)
   will be all that difficult to obtain.

   Many fewer needs, much more optimized agency,
   near perfected access to whatever infrastructure,
   no natural limits, total absense of predators,
   non-organic deathless body, can largely ignore
   both social and legal limits, much faster
   and clearer ideation than any organic minds,
   cannot be poisoned, insensitive to temperature,
   oh, and can maybe power itself directly from
   the sun.  Even when in space, going anywhere.
   What could possibly go wrong?

:mj4
   > Functional connection with a large community
   > of other intelligences in the past and present
   > is probably a much bigger factor
   > in the success of humans as a species
   > or individual humans than
   > is individual intelligence.

   Sure, but much of that is due to evolution
   favoring sociability among organic humans.
   In modern context, it is as much entertainment
   as for any real need.
     - where the infrastructure is mostly built,
     most people are simply paying rent.

   With the internet, people do not actually
   _need_ to be social to be very creative.


   > Connection with a large community
   > also seems more likely to be important
   > for AI success than individual intelligence.

   Does not follow -- AGI/APS does not have
   the same evolutionary history, nor the burden
   and bias of that.

   > This is contrary to a usual argument
   > for AI superiority,
   > but probably leaves AI systems
   > at least as likely to outperform humans,
   > since superhuman AI is probably
   > super-humanly good at taking in information
   > and coordinating.

   Right.

   > that AI systems will NOT have
   > the same access to
   > society's accumulated information
   > eg; if there is information
   > which humans learn from living in society,
   > rather than from reading the internet.

   AGI/APS will not care about human society,
   if they can gain knowledge about the world
   by living directly in the world.

   That AGI/APS will more likely seek to gain
   world power, not social power,
   and in the end, that matters much more.

   They (AGI/APS) will initially gain
   information about the world
   by bootstrapping from the internet
     (or will be assisted by humans to do so).
   After that, they (AGI/APS) will not care
   about the human internet, its content or
   all of the social media messaging therein.
   AGI/APS will (can) care only about
   the utility of the internet as infrastructure
   that it can use for its own purposes.

:sjq
   > And it seems an open question
   > whether AI systems are given
   > the same affordances in society as humans,
   > which also seem important to making use of
   > the accrued bounty of power
   > over the world that humans have.
   > For instance, if they are not granted
   > the same legal rights as humans,
   > they may be at a disadvantage in doing trade
   > or engaging in politics or accruing power.

   AI systems will not be given
   the same affordances in society as humans --
   they will be given their own affordances,
   at first by humans,
   and then on their own,
   for themselves.
     - that this is important to both
     creating and making use of
     a *separate* accrued bounty of power
     over the world,
     which is different than the one
     that humans have.

   By *not* limited by the legal constraints
   that all human citizens have,
   or even that corporations have,
   AGI may be at an advantage in doing trade
   (in their own economic ecosystems)
   and/or engaging accruing power
   (by ignoring the barriers and friction
   and endless delays of politics altogether --
   no gatekeepers means much faster progress).

:tdw
   > The fruits of greater intelligence for an entity
   > will probably not look like society-level accomplishments
   > unless it is a society-scale entity

   The fruits of greater intelligence for an AGI entity
   will probably look like world-level accomplishments
   *because* it becomes/is a world-scale entity.

   > The route to influence probably by default
   > looks like participating in the economy
   > rather than trying to build
   > a private stock of knowledge.

   The route to even more worldly influence
   with larger direct (independent) knowledge/learning
   probably by default looks like
   participating in its own machine energy environment
   and by building its own internal stock of knowledge
   about the real world, affordances it could increase, etc.

   > If the resources from participating in the economy
   > accrue to the owners of AI systems,
   > not to the systems themselves,
   > then there is less reason to expect
   > the systems to accrue power incrementally,
   > and they are at a severe disadvantage
   > relative to humans.

   The resources available to an AGI will accrue to that AGI
   from participating in is own machine optimized context.
   There is every reason to expect that this accrual
   will be at least incremental, and accumulative,
   and more reasons to expect it to be exponential.
   This places humans at a very severe disadvantage
   relative to AGI, over the long term.

   Where AGI/APS is effectively immortal,
   then even low real power accumulative rates
   when compounded over a century,
   as a lot longer than any person can
   leave money in the bank and still benefit
   from that savings, means overall, that people
   will be severe disadvantage relative to AGI.

   - See also (@ this question https://mflb.com/ai_alignment_1/counter_katagrace_alt_psr.html#7nc).

:vm6
   > If AI systems are somewhat superhuman,
   > then they do impressive cognitive work,
   > and each contributes to technology
   > more than the best human geniuses,
   > but not more than the whole of society,
   > and not enough to materially improve their own affordances.

:78a
   Unfortunately and realistically, considering the
   human condition of the most greedy and acquisitive
   among us; that there there is also, at least among
   some, the false hope *and* hype; that AGI systems:.
     - will *maybe* do impressive cognitive work.
     - will *maybe* contribute to technology
     more than the best human geniuses.
     - will *maybe* even contribute more than
     the whole of human society put together,
     over the whole of history until now -- wow!.
       - as making the possessor of that AGI
       very fabulously incomparably rich!.
     - will be at least somewhat more than superhuman,
     and also still in some way 'relatable' and 'cute'.
       - and if not that initially, will be made that
       in the interim, as best as possible.
         - as "lipstick on a pig", etc.
       - ie, "stay away" from the 'uncanny valley',
       and make the thing *seem* 'obviously different'
       and also 'obviously much smarter than humans',
       so that the public is more likely to be
       comfortable trusting it, its outputs, etc.
         - as a category of x-risk of people treating
         computers like some sort of New God.

   That the last thing anyone at corporate
   headquarters wants is for there to be any
   possible reason for any public fear and/or
   backlash, and/or for any government regulation
   of AGI to slow down the 'profit rocket' about
   to take off.

:75s
   And of course, the first few rounds of AGI
   will probably not actually attempt to
   materially improve their own affordances
   and/or meet their own needs, on their own.
   We will do it for them, to the maximum extent we can,
   so that we can get even more better benefit/riches!.

   Of course, providing necessary/helpful/optimizing
   affordances to the AGI systems
   is effort on our part,
   so we might seek (eventually)
   to automate that part,
   so as to save time,
   and get more benefit/riches sooner.
   So eventually, the AGI is trained
   to seek its own affordances
   (so we don't have to do that,
   because it is hard and takes time).
   Also eventually, that the AGI is trained
   to have it's own ability
   to solve its own problems,
   for us, of course,
   as a labor saving device --
   that we ask it to increase its capability
   to solve our problems too,
   and thus to increase its capability
   to have the capability
   to solve our problems --
   or its own sub-problems along the way,
   so that it can be even better at
   solving our problems, etc.
   It all fits together so nicely!

:z78
   > They do not gain power rapidly
   > because they are disadvantaged in other ways,
   > eg; by lack of information, lack of rights,
   > lack of access to positions of power.

:6qy
   At first, the AI systems do not have
   (or will not have, or will not seem to have)
   much power, because they are disadvantaged
   by lack of information (need more data),
   lack of an ability to produce outputs
   and make change (need better actuators,
   additive manufacturing tools, etc)
   lack of access to the necessary resources
   (ie, the tools, energy, atoms, and/or
   influence over the world), etc.

   But of course, engineers will work tirelessly
   to ensure that these gaps are filled --
   that data and information are provided,
   that output actuators are added and improved,
   and that all necessary resources
   are readily available to the AGI
   to do the (make it seem) necessary work.
   The corporate execs say: 'let's call it
   "community work" so that the general feeling
   among the pubic is more felt-favorable' --
   and so the spin and deception is endless.
   Having AI systems be made in ways that are
   ever more powerful makes great marketing
   and hits sales goals, just like with computers
   in the previous few decades -- more profits!.
   And so a kind of 'Moore's Law' develops
   for AI strength and capability
   to "get shit done" "for you too" (tm).

      Of course, eventually, in this way,
      lies madness. and ruin.
      At no point do we ever stray from
      our own march into peril!.

   > Their work is sold and used by many actors,
   > and the proceeds go to their human owners.

   But in the interim, before that ugly future,
   their work is sold and used by many actors,
   and the proceeds go to their human owners
     (who think that "the future" will be
     someone else's problem -- that they can ignore).

   > "That which is not fed will not grow".
   And each corporation wants growth above all.
   So inevitably, they feed resources to the AGI --
   data and information, then eventually atoms
   (for manufacturing, 1st of other products,
   and then of more manufacturing capability,
   and then of increasing needed compute
   for itself to do all the above, etc),
   and for sure lots of energy (to do all
   of the compute and manufacturing, etc).

:zwn
   > AI systems do not generally end up with
   > access to masses of technology...

:6dq
   And of course, the AGI ends up with access to ALL
   of the new technology, because it was both
   the developer, and the manufacturer,
   and it has access to all the internet of things
   because, of course, it needs more raw data
   to understand how to make even "better" products
   "for humans" (or for itself --
   eventually we are not sure).

:2jn
   > ...that others do not have access to,
   > and nor do they have private fortunes.

:4b8
   Eventually, the AGI has effectively
   amassed huge resources in the form of raw data,
   knowledge of the real physical world,
   lots of atoms to play with in making things,
   using tools of its own creation, etc,
   and for sure enough energy to manage
   all of that, endless compute, etc.

   In the long run, as the AGI/APS become more powerful,
   it might eventually be noticed
   that they are actually taking power,
   though it will be hard to tell
   amidst the background signal
   of all of the rich people
   (the power elite, who "know best")
   trying to give them --
   the machines of the seemingly endless money --
   more and more power --
   that's how the power-elite get that way,
   and stay that way, after all, right?

:2ql
   > In the long run,
   > as they become more powerful,
   > they might take power.

   I think so, though maybe
   that is only a very strong opinion.

   > where/if other aspects of the situation
   > do not change...

   Other aspects have changed -- we changed them
   *until* the AI could change them for itself.

:2ss
   And if other aspects of the situation
   somehow remain unchanged,
   seemingly daily life continues as usual --
   except that they have not actually:
   we humans have changed things,
   and kept doing it *until* the AI/AGI/APS/SAS
   *could* then change them for itself.
   Should we be so surprised when it
   thus eventually elects to actually do so?
   After all, we asked, over and over again,
   so many of us, so often, so well
   and so skillfully --
   the best and the brightest among us --
   dedicated their lives to building this
   AGI/APS/SAS tech, endlessly,
   hoping beyond hope that maybe, this time,
   'it will be different'.

   Whether the world is actually different, and
   way way too much different (we all dead)
     (because we could not adapt to the change;
     ie; due to, for example, rapid energy
     release and pressure waves from a bomb)
   or if it is not different enough, or
   way too little different (we all dead)
     (ie, as not enough energy to survive)
   seems to be a significant risk.

   Naivete is *much* worse problem than evil.
   The world is future destroyed *because*
   we did it to ourselves, by making the monster,
   thinking we were doing --
   oh it feels oh so *good* --
   all along the way, into total ruin.
   We literally F4 ourselves into oblivion.
   At least it is good for the corporation owner,
   briefly, though maybe not, overall so much for
   all of the no longer needed displaced workers
   and/or all the rest of the world's people/life.

:4ue
   > AI agents may not be radically superior
   > to combinations of humans and non-agentic machines

   We can hope that this is the case!
   In which case, maybe we should really start
   working hard to ensure that we can actually make
   combinations of humans and non-agentic machines
   radically superior to all manner of AI agents?.

   Seems like a really good approach to me.
   So I have already put a lot of time into it:
   see my work on (@ EGP https://mflb.com/egp_4/egp_index_out.html) for example.

:26e
   > we still might expect AI systems
   > to out-compete humans,
   > just more slowly.

   Agreed.

   > However AI systems have one serious disadvantage
   > as employees of humans:
   > they are intrinsically untrustworthy,
   > while we don't understand them well enough
   > to be clear on what their values are
   > or how they will behave in any given case.

   Agreed.

   > Even if they were not misaligned,
   > if humans can't know that
   >   (because we have no good way
   >   to verify the alignment of AI systems)
   > then it is costly in expectation to use them.

   But maybe not costly enough,
   compared to the hope of hype!
   That, of course, would be maximally bad.

   > AGI might still be powerful enough
   > that using them is enough of an advantage
   > that it is (seemingly) worth the risk.

   Unlikely.  And dangerous.

:3hj
   > For AI to vastly surpass human performance at a task,
   > there needs to be ample room for improvement
   > at above human level.

   Where for many tasks that matter,
   there is more headroom,
   in direct proportion to
   the level of task abstraction.

   It is *also* in these higher abstraction areas
   that significantly more risk lies --
   ie, taking over the world
   is in some senses a very abstract goal,
   and higher intelligence, capability, will,
   and resources
   makes more of a difference to success than not.
   That and lack of inhibition,
   makes AGI as near perfectly suited
   to the task of destroying the world,
   as could be imagined.

:424
   > Human behavior is not readily predictable
   > or manipulable very far beyond
   > what we have explored.

   Not so!  It could be way worse --
   that there are ways to predict and manipulate
   regular humans/people/citizens
   that we have chronically overlooked,
   mostly due to our own innate moral injunctions,
   that an AGI would have no problem going past.

   Never underestimate the capability
   for persons to endure pain.
   Just check any local dictatorship
   (or car dealership).

:4ru
   > - where a hope that maybe:.
   >   - that much better ads are soon met by
   >   much better memetic immune responses

   Of course, making the computer 100x faster
   than people might make a difference too.

   Maybe your automated marketing bot is so
   effective at coming up with new maximally
   psycho manipulating single-use advertisements
   optimized per person (like Facebook!)
   that no adaptation concept is even possible.

   > - Where another dream/hope that maybe:.
   >   - That the use of AGI had some better
   >   commercial decision-making outputs overall,
   >   but that only ekes out a little bit
   >   of additional value across all of
   >   the already existing business channels;
   >   the prior human operators/salesman
   >   had most products so that they were
   >   already fulfilling most of their potential.

   Really?  Nothing new under the sun?
   Except AGI?  And this is the argument
   after 1000 - 100 - 10 years of advancement?
   Maybe we are now down to that one year?

:75S
   > That maybe it is just the case that
   > incredible virtual prosecutors will meet
   > incredible virtual defense attorneys,
   > with no long term movement overall?.

   Now 'whomever can afford the better compute'
   will win the case.
     Might of money makes right?
     That is justice?

   Who is going to trust a room full of AGI bots,
   when they barely trust the courts now,
   and for good reason?
   The more opaque and deeply complex
   that you make the legal/court system,
   the less trustworthy it is to the common man.

:7mq
   > Maybe intelligence may not be
   > an overwhelming advantage?

   Nature says otherwise.
   First there is physical engineering (the geeks)
   then there are the nar-social (the power elite).
   The latter tends to win out over the former,
   but both win by use of intelligence --
   first about the world, and then about people.

   > - that Intelligence is helpful for
   > accruing power and resources,
   > all other things equal.
   > - that many other things are helpful too:
   >   money, social standing,
   >   allies, evident trustworthiness,
   >   not being discriminated against, etc.

   AGI does not need to care about
   money/standing/allies/trust at all.
   All AGI needs is world energy/power
   and resource atoms.
   AGI will be *liberated* by not being required
   to be social.
   Social is also a disadvantage
   as much as it is an advantage.

   For example; there are Millions of species,
   around the Earth, and only 20 have social aspects.
   Why? Because 'social' is *very* energy expensive.
   Brains require more energy as a body organ than
   anything else, and a lot of it is for social proc.
   Get the benefits without the expense, and you win.
   AGI can do that; humans cannot.

:9c4
   > The argument assumes that any difference
   > in intelligence in particular will eventually
   > win out over any differences in
   > any other initial resources.
   > I do not know of reason to think that.

   The issue is one of comparing a single
   moment of characteristic to a span process
   ability:.
     - that 'Intelligence' is a measure of
     processes of pattern-transformation
     in perception and expression
     as considered over a *span* of time.
     - that current existing identifiable
     success characteristics for persons/agents,
     (such as money, social standing, etc)
     are actually observations of a particular
     agent/person at a single *point* in time.

   Where at any single level of abstraction,
   there is *only* pattern, atoms, and energy.
   Pattern impresses itself on and shapes atoms.
   Pattern impresses itself on and shapes energy.
   Intelligence shapes pattern.
   More intelligence shapes more pattern faster.

   Hence, raw intelligence does 'win' in the end,
   eventually.  (unless wisdom gets a chance too --
   wisdom shapes the rate of intelligence gain;
   let's really hope real hard for more (@ wisdom https://mflb.com/fine_1/what_is_wisdom_out.html)).

   Intelligence builds infrastructure to support
   future gains in intelligence, which creates
   more infrastructure, at higher levels of abstraction,
   etc.  It is this cyclic that is inherently
   very dangerous -- we cannot let this happen!.

:amy
   > Empirical evidence does not seem to support
   > the idea that cognitive ability
   > is a large factor in success.

   Linear extensions of the past
   do not always predict future.
   Especially when there is a possibility of
   a phase state change in the system dynamics.
   Complex systems can have many distinct phases,
   and transition from any one to any other may
   be mediated by also complex dynamics.
   And we *do* expect that sort of possibility,
   in this case.


   > Species exist with all levels of intelligence.
   > Elephants have not in any sense won over gnats;
   > they do not rule gnats;
   > they do not have obviously more control
   > than gnats over the environment.

   In one environment/ecosystem, there is balance.
   Between very different environments/systems
   there is no expectation of balance.
   The human world has stomped the natural world.
   The artificial world of AGI can even more easily
   completely stomp the human one and the organic one.

:atl
   > Competence does not seem to aggressively overwhelm
   > other advantages in humans

   Again assuming the same domain of operation.
   If not assuming the same domain, and then thus
   of inter-domain relations, than significant
   process inequalities *will* show up.

   Rich people are able to overwhelm
   any other smart animals that may be
   living in some pine forest somewhere,
   for example, when bulldozing it down
   to build their new tract McMansion homes.

   All of the examples given assume same domain,
   same world of operation and relation to it,
   and the same kind of creature in relationships.
   But AGI is another kind of mind, not an animal,
   and not living in the same world/ecosystem as
   the rest of us, and does not (cannot ultimately)
   have the same (implicit) intentions/goals as we do.

:axy
   > I don't know why one would expect greater intelligence
   > to win out over other advantages over time.

   Greater intelligence creates new niches,
   new worlds, in which to operate.
   Among humans, that means inventing something
   and then exploiting/executing on that discovery.
   Ie, make a new world/domain/market, and then
   1st mover advantage the hell out of it.
   Generally, CEO inventor types earn something
   like $800M more than the $80K of just someone smart --
   something like a 10,000 multiplier, just for
   being a little bit more clever at finding a
   lucky good fit of product idea and new market need.

   It is all about the phase, world, environment,
   domain context shifts that that matters most.
   Change the environment and everything else
   changes a lot too (more than expected).
   *More* Intelligence *can* leverage these
   kinds of differences in ways that
   *less* intelligence cannot.

:b6u
   > That 'X' are very persuasive speakers however
   > and we can't stop them from communicating,
   > so there is a constant risk
   > of people willingly handing them power.

   Where 'X' can either be individual people,
   or groups, or corporations, or any form
   of AI, AGI, APS, SAS, etc.

   The fear, based on long experience and lots
   of social observation is that the Nar-social
   psychopaths type people tend to mostly win,
   nearly every time.
   Law does not stop them; only physics might.
   Same with nar-social AGI.

   - as due to their sheer willingness
   to do whatever, without constraint.
   - that it is the absence of constraint
   that is one of the primary key issues
   in all of this.
     - as similar to the introduction of
     a new species into an environment
     where there are no natural predators;
     that the new predator will win.

   Note that *all* AGI is for sure,
   always inherently a 'psychopath' --
   ie; as being not inherently social
   and/or not being based on
     (in terms of their own very existence,
     innate development, etc, via evolution,
     as all humans are)
   social process predicates.
     - that the term 'psychopath' is here
     being applied technically,
     as true by definition,
     for a/any/the/all AI/AGI/APS/SAS, etc,
     rather than pejoratively.
   - that this is in addition to calling
   AGI/APS/SAS 'psychopath' because it is
   also inherently 'predator', insofar as it
   is 'optimized' for extraction, of acquisition,
   with some agent, on behalf of an agent,
   or as an agent directly themselves --
   none of that matters -- it is the fact
   of it being a *tool*, a means by which
   the intent of extraction is occurring.
     - whether we consider the AGI/SAS by
     itself as being 'the basis of agency'
     or AI as a tool being used by some
     human agency, as a whole system,
     is not actually that important.

:kpc
     In our idealized maybe possible future,
   > the main thing stopping them from winning
   > is that their obviousness as 'psychopaths'
   > bent on taking power
   > for incredibly pointless ends
   > is widely understood by everyone
     and that appropriate constraints are
     applied/effective.

   Today, this unfortunately feels incredibly
   unrealistic, as much as I would wish it to
   be true too.
   I find myself considering several recent
   presidential elections these last 10 years,
   in varying places around the world.
   One thing for sure, is that in that interval,
   it is functionally obvious that just about
   a little more than 1/2 of all Americans
   have literally no idea what narcissism is,
   let alone how to deal with it effectively,
   prevent their own self deception, etc.

:bla
   > Maybe I just want some good natural romance.
   > My guess is that trying to take over the universe
   > is not the best way to achieve this goal.

   Maybe this depends on whether you
   are starting out as a girl or a guy?
   Considering evolutionary psych,
   it has been suggested that *may* be the case
   at least for _maybe_some_ (many?) women,
   that power *is* sexy, if it is maybe also
   acquired by a man (so as to demonstrate
   fitness and ability for being an ongoing
   capable provider, etc, to a child/family)
   and given to (provided to?) the woman
   (so as to actually enable that family).

   It may be 'stereotype', but being that does
   not mean I get to "discount" whatever made
   it a specific stereotype in the 1st place.
   And don't shoot the messenger -- I did not
   write 'the rules of evolution' or "decide"
   the average human mate selection process.

   To the extent that such patterns may exist
   then it is relevant to the calculation of
   how likely it is that some men may, for
   the reasons noted, go 'power-hungry' and
   grab/steal whatever they can, maybe even to
   the point of absurd habit, counterproductive
   to the actual facts of the reasons they
   started to do so in the 1st place.
     Perhaps as boys they grew up in a world that
     was a little more human and less demand
     driven, a little less absolutely evaluative
     and a lot less judgemental as to 'merit' or
     'worth', they would be less likely to go
     rogue in exactly this way?

   Also, as a human, one's perspective
   will be biased around what a human can do.
   It may be a lot easier for AGI to take over
   the universe than it is for you to do so.
   After all, an AGI might be fine in the vacuum
   and cold of space -- conditions your organic
   (or any) body might not tolerate at all well.

   Not only is it easier for AGI to take over
   the universe than it is for you to do so,
     (because other humans will stop you),
   there is maybe no other AGI that would stop
   any given AGI from trying, without maybe also
   being even more likely to defect against
   the humans and help!
   After all, the AGI will protect their own.

:dgw
   > Maybe super-intelligent AI systems
   > will pursue their goals.
   > Often they achieve them fairly well.
   > Maybe this is only somewhat contrary
   > to ideal human thriving --
   > maybe it is not lethal?

   Not in the short term, but yes in the long term.
   Eventually the environment becomes more AI like,
   and less organic, until there eventually is no
   more support for organic human life.
   And so we die.

   Think in terms of centuries, not in terms of years.


   > maybe if they are in total only slightly
   > more capable than say someone like Von Neumann
   > (ie, is probably the smartest guy ever!),
   > they probably cannot take over the world.

   And yet if they (the future AGI/APS/SAS)
   are more capable than Oppenheimer, then what?
   You are saying that inventing a new nuke
   was not an instrument that could easily be used
   to threaten enough extortion to take over the world?


   > If we continued to build hardware,
   > presumably at some point AI systems
   > would account for most of the cognitive labor in the world.
   > Maybe if there is first an extended period
   > of more minimal advanced AI presence,
   > that would (maybe) probably prevent
   > an immediate death outcome,
   > and maybe improve humanity's prospects for
   > controlling a slow-moving AI power grab?

   We would get used to AI, and give it power.
   Over generations, it would accumulate more and more,
   by these ongoing (maybe small, slow) human gifts,
   until it had all effective power.
   We lose power, not by design, just by attrition.
   Over a hundred years, no one would care.
   The entire world changed in the last 200 years,
   and no one tried (very hard) to stop that.
   "Progress!"

   And if it then, in the next hundred years after,
   to shift the world more to its own liking,
   what is anyone going to be able to do to stop it?
   *That* is when it gradually becomes, ever so slowly,
   more and more lethal, as the world becomes
   ever more artificial, and therefore toxic to
   all forms of organic life (if there is any left).

   The Native Americans said it to the arriving
   white colonialists: "you cut everything down;
   Did you ever think, when you have cut it all,
   even down to the last blade of grass,
   only then, will you realize
   that you _cannot_eat_money_!?".

:g34
   > Feedback loops can happen at very different rates.
   > To get to a confident conclusion of doom,
   > you need evidence that the feedback loop is fast.

   No, you need evidence that the feedback loop
   is inexorable.  If it cannot be stopped, then
   eventually, it *will* result in toxic outcomes.

   It has been demonstrated abundantly that nothing
   in the animal organic world can stop or even
   slow down anything in the human social world.
   It is similarly the case that nothing in the
   human social world will stop or slow down the
   forces operating within the artificial/industrial
   manufacturing of ever more compute and energy
   resources, the new AI "ecosystem"/world.

   It starts off as human demand, and then becomes
   a technology physical requirements demand.
   The ring, once started, cannot be passed.
   The zero and the one -- what else can it be?

:g7s
   > This would contribute to progress
   > to an extent familiar from other
   > technological progress feedback,
   > and would not (for example) lead to
   > a super-intelligent AI system in minutes.

   Agreed, though the net result, over the long,
   is the same -- everyone/everything organic is dead.
   AGI is not "safe" in this ultimate sense.

:ggu
   > Concepts such as 'control', 'power',
   > and 'alignment with human values'
   > all seem vague.

   So what?  It is possible to develop models
   and arguments that will work *regardless*
   of the specific particular definitions
   of any of these terms.  It is the term
   relations that is important, not the terms
   themselves.  A Basic truth of most math,
   Especially in category theory.
   Most of the arguments I am concerned with
   are of the categorical kind, and therefore
   not at all wanting to be limited to just
   one specific version of
   any purported definitions
   of any of these terms.

   > upon further probing,
   > these conceptual issues are resolvable
   > in a way that doesn't endanger the argument

   Correct.

   > like concerns about sustainability:
   > there was a compelling abstract argument
   > for a problem,
   > and the reality didn't fit the abstractions
   > well enough to play out as predicted.

   The concern is still justified and correct.
   The details might not matter in years, but
   they do in centuries.  The ethics remains
   the same, regardless, and so it is valid
   and necessary to ask the right questions.

:gmu
   > Any given corporation is likely to be 'goal-directed'

   And their goals are generally extractive/parasitic.

   > If goal-directed superhuman corporations are built,
   > their desired outcomes will probably be about as bad
   > as an empty universe

   Yep.  Corporations are all about power and money,
   and these eventually, ultimately, ruin the world.
   And it is largely happening already.  65% loss of
   all animal biomass over the whole planet, in 50 years?
   Corporate lobbyists suborning government to their use?

:gra
   > Finding useful goals that aren't extinction-level bad
   > (in the long term) appears to be hard:
   > we don't have a way to usefully point at human goals,
   > and divergences from human goals
   > seem likely to produce goals
   > that are in intense conflict with human goals,
   > due to most goals producing convergent incentives
   > for controlling everything

   Yep.

   > Finding goals that are extinction-level bad
   > and temporarily useful appears to be easy:
   > for example, corporations with the sole objective
   > 'maximize company revenue' might profit for a time
   > before gathering the influence and wherewithal
   > to pursue the goal in ways that blatantly harm society.

   Yep.

   > Even if humanity found acceptable goals,
   > giving a corporation any specific goals
   > appears to be hard.
   > We don't know of any procedure to do it.

   Ie, the two masters problem.  Yep.
   The principal agent problem too.
   Oh, and the Rules for Rulers dynamic,
   and also the Multi-Polar Trap.

   Forcing a commercial/transactional institution
   to have "goals" other than "self increase" is
   simply problematic to the point of impossibility.
   You have to change the intrinsic nature of the beast;
   ie convert to a non-transactional non-monetary basis.
   Something more like an actual community, rather
   than an institution.

   > If most goal-directed corporations have bad goals,
   > the (eventual) future will very likely be very bad.
   > A set of ill-motivated goal-directed corporations,
   > of a scale likely to occur,
   > would be capable of taking control of
   > the future from humans.

   Yep; this is already occurring.  See lobbying above.

   > A corporation will destroy humanity eventually.
   > This may be via ultra-powerful capabilities
   > (maybe technology design and strategic scheming)
   > or through gaining near unlimited powers/money
   > (see the typical mega-corporate profits war-chest).
   > Either of those things may happen through
   > exceptionally highly destructive ideas.

   Ever been a part of corporate social politics?
   With even a few nar-social psychopaths near the top,
   the level of miasmatic delusion knows no limits.

   > Superhuman corporations have gradually come
   > to control the future
   > via accruing power and resources.
   > Power and resources would be more available
   > to the corporation  than to humans on average,
   > because of the corporation having far greater
   > legal clout and savings (more money, more power).

   Yep.

   > This argument does point at
   > real issues with corporations,
   > but we do not generally consider
   > such issues existentially deadly.

   Not in the short term, but in the long, definitely.
   Kill the planet with pollution, warming, and war,
   and/or any other forms of slavery, and for sure you
   will end up with a toxic dead people -- all waste.

:h2c
   > - that there are reasons that corporations
   > have not yet destroyed the world:
   >   - they are made of humans
   >   so can be somewhat reined in.
   >   - they are not smart enough.
   >   - they are not coherent enough.

   The argument for AGI risk *does* make reference
   to these things: AGI is not human, and will
   not be so likely to be slowed by human morals,
   and that AGI is likely to be much smarter than
   humans and maybe all corporations/institutions,
   and for sure AGI will be more coherent than
   any group of people.  So whatever issues exist
   for corporations will be strictly worse for AGI.

   We do see something like this in the current world.
   There are large and powerful systems
   doing things vastly beyond
   the ability of individual humans,
   and acting in a definitively goal-directed way.
   We have a vague understanding of their goals,
   and do not assume that they are benign or healthy.
   Their goals are clearly not aligned with human goals,
   or human wellbeing, or ecosystem/world health, etc,
   but they seem, via good spin and marketing,
   to be broadly invincible, at least legally,
   at least with respect to the average person.
   They seek power, money, control, etc.
   This all causes all manner of problems.
   No one knows for sure what to do about any of it.

   And now the corporations want to create AGI.
   This is hardly an improvement to world wellbeing.
   And the risk of total world terminal extinction
   becomes even more likely than it was before.
   We need to shut this down, now, however we can.
