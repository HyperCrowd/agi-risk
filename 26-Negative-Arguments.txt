TITL:
   *"Negative Arguments"*
   *By Forrest Landry*
   *Oct 10th, 2022*.

ABST:

   As about the distinction of arguing against something,
   (as a kind of a report of bad news)
   vs arguing for something (positive/beneficial).

TEXT:

   Technical alignment rests on a premise:
     That all of a series of
     by-default-deficient crucial factors
     can be solved as to be 'not-deficient'.

   It tacitly assumes a positively specified hypothesis:
     That *all* of the relevant critical factors
     (that must be) can be, an can at once be
     simultaneously and continuously present
     so as to ensure both
     that generally-capable self-learning machines
     continue to exist *AND* will elect to keep
     the conditions of the ambient environment
     within the narrow ranges of magnitudes
     that humans (and other life) needs to survive.

   That this positive condition constitutes a very
   minimal-threshold definition of AGI alignment
   and/or a very minimal definition of 'AGI safety'.

   Proving alignment of future AGI to be possible,
   even as a lower-bound statistical guarantee,
   is a conceptually daunting task.
   It depends on unknown unknowns --
   whatever number of crucial technical factors
   exist, any one of which *might* fall _outside_
   the hypothesis space covered by human learning,
   (and/or capability) and thus stay unsolved.

   However, proving AGI alignment to be impossible,
   on the other hand,
   is conceptually very much easier.
   Ie; to show that something would not be possible,
   even as a lower-bound statistical guarantee,
   it is only required to proving that,
   for at least one of the crucial technical factors,
   that that specific selected
   and known necessary factor
   (out of however many unknown others)
   cannot be constantly present alongside
   with another (any other) also crucial
   known necessary factor,
   that is also needed for humans to stay alive.
   This is a negatively specified hypothesis.

   Our work is to suggest and review
   a series of arguments (developed multiple ways)
   for why technical alignment
   is not possible, now or ever,
   since some misaligned code variants
   are not detectable (in practice or principle)
   nor are their effects containable.
   ie; there is a 0% chance of anyone
   overcoming the problematic contradictions
   at any point in the future,
   *regardless* of whatever other constraints
   might later be identified, as also necessary
   to AGI alignment and/or safety.

:drn
   > Your argument reads in the general pattern of:
   > 'convince the reader that certain problems exist'.
   > That is not helpful; we do not need more of that.

     Your opinion has been noted.
     What motivates that opinion?


   > What I am interested in is: 'convince the reader
   > that certain types of solutions to AGI problems
   > are worth trying, experimenting with, and supporting'.

   > As an engineer, I want to actually solve problems,
   > not create more of them.
   > The former is more fun; the latter, not so much.

   > Besides, if we are not even suggesting
   > that something is worth working on,
   > how do we expect anything to actually get done?.
   > No effort means no real progress in the world.

     Not all problems can be solved.

     If a given specific given problem has been shown,
     via valid reason/logic and sound argument,
     to be actually and strictly within
     the class of known unsolvable problems;
     then it is simply not actually productive,
     to suggest to other people that they should
     "try to make progress" (on unsolvable problems),
     or that someone should provide unlimited funds
     to employ you to "work" on such issues, indefinitely.

     Moreover, a lot of less scruplous people
     would simply suggest that something is possible,
     merely so that they can apply for support.
     It becomes, marketing, basically, to get NSF grants.
     Or maybe they think OpenPhil maybe a better option?

     Knowing otherwise, I will not pretend
     that there are maybe solutions
     available to the technical alignment problem,
     if there are consistent and sound arguments
     for why technical alignment is not solvable.

     In such circumstances, it is not modest to imply
     that progress can be made on all types of problems.

     And moreover, a specific unwillingness
     to even look at and/or actually consider
     the formal basis of the impossibility results
     (due to some kind of motivated opinion heuristic)
     is a kind of *group* intellectual dishonesty,
     especially when the precautionary principle
     is to be applied strongly in any area
     that has been clearly identified as
     a known planet-terminal x-risk.

     As a practicing large scale systems engineer
     with a number of successful deployed projects,
     I can personally relate with the good feeling
     of being able to analyze a problem
     into its constituent parts, in a neat way,
     so as to make a system's behavior predictable,
     and thusly of the satisfaction of constructing
     an actual working practical novel solution,
     something real people can use, on the ground,
     in their real lives.

     Doing the seemingly impossible, the miraculous,
     while knowing and using all manner of arcane forces,
     (and while also having zero inhibitions in so doing),
     is something of a dream --
     a technologist fantasy.

     Unfortunately, conventions of provincialism live large.
     In a world of 'short optimization cycles',
     the saving of countless lives in some long future,
     in some completely utterly foreign people,
     living in a truly far and distant land,
     counts for exactly nothing here and now, today,
     (no corporate profits or marketing gains)
     though of course, it very much *should*.
       (ie; there is no one to thank you now,
       while you still life, and they, then,
       have not yet even begun their own lives).

     As such, I am also well aware of the
     responsibilities that *all* engineers have
     (and software development is _not_ excepted)
     to account to/for the real harms/risks
     to/for/in *all* of the (also future) lives
     affected by those (proposed) systems.
     It is not just about benefits,
     it is also always, about costs and risks,
     and whether,
     when considered in a socially responsible way,
     the "trade" is worth it, in community,
     for *everyone* involved (ie; _not_just_
     the investors trying to make a short term profit
     from your willing paid for corporate efforts).
       Example; Social media software in apps on phones
       has had clear damaging overall effects
       on culture, democracy, and civilization.

     Unfortunately, not all problems can be solved;
     not all dreams can be made real (in *this* universe).
     Sometimes, at a profound level, you run up against
     a deeper principled/fundamental contradiction
     and therefore a solution to that problem
     is not possible (in *any* universe).

     For some examples:.

       - where in abstract math:.
         - Simultaneous consistency and completeness
         of mathematical models
         cannot be solved for (Godel's theorems).

       - where in distributed computing theory:.
         - Simultaneous message content consistency,
         and continuous availability of messages
         and tolerance to partitions
         cannot be solved for at once
         in a distributed data network
         (@ CAP theorem https://en.wikipedia.org/wiki/CAP_theorem).

       - where in physics/thermodynamics:.
         - that perpetual motion cannot be solved for.

     Even more unfortunately, in the past, historically,
     it often took a decade or more (maybe centuries)
     for experts of a given field of study/practice
     to come around to realizing that the problem,
     as defined by them, that they were trying to solve,
     is/was not (ever) actually solvable at all.
     I hope this does not happen to the "AI Safety Community" --
     particularly as they continue to move forward
     to constructing any number of "believable rationales"
     about our "not actually" making slow doom devices
     (in the form of superintelligent machines, etc).

       Note; that the majority of the researchers
       in the current AI x-safety community
       are not themselves directly working on
       constructing "slow-doom devices" (AGI/APS).
       Instead they are accidental tacit *enablers*.
       By engaging in so many "what if" senarios,
       they provide the ready means for executives,
       product marketing specialists, to 'spin'
       design and development researchers,
       to believe the idea that "eventually,
       we will find a way" (to great value/profits!).
       It mistakenly facilitates people at corporations
       into falsely believing, and feeling more confident,
       that they can convince themselves and others
       that they can legitimately 'bring to market'
       these 'slow doom devices', because they are
       seemingly (though not actually) safe/aligned.
       Hence, the mass self-deception continues.

:dv4
   > I suggest, where for anything you write, and/or present,
   > you should avoid the 'I am going to disprove X' framing
   > and instead use the 'I am going to argue for Y' framing.
   > People respond better to positive things (carrots)
   > more than they respond to negative things (sticks).

     How about something given in the form of:.
        "I am going to argue in favor of
        accepting the _'AGI_not_safe_at_all'_ argument,
        and to therefore also be in favor of
        *NOT* allowing _any_ AGI research and/or builds"?.

     However, we are suggesting a kind of 'not-action',
     (ie; do not build the doom/death making machine(s)),
     as a kind of verified/verifiable formal practice.
     Does that count as sufficiently positive/marketable?.
     Is it ethical for everyone to avoid meeting
     the publicist of an unpopular negative truth?
     Is it ever valid to shoot the messenger?

     Most tech people agree, where for technology, apps, etc --
     Where for anything that interacts with humans at all --
     that no one can make *any* system or device (AGI or not)
     a full 100% safe (or 'aligned' with the makers interest).
     There will always be sufficiently smart motivated hackers
     and/or sufficiently stupid humans (end users)
     who will always be making critical mistakes,
     which disable fail-safes, security protocols,
     and enable extortion/stealing of (community) value
     from any app or service.

     This is not our concern here, even though some people,
     somehow, still hold out the impossible dream
     that superintelligent AGI will allow the building
     of cyber-physical systems that are "100% safe",
     or "incorruptible", and/or "perfectly aligned".
     Of course, this false dream is never going to happen,
     at least not for any reasonable definition
     of "safe" or "aligned" that actually has meaning.
     (ie; assuming that you are not also in some sort of
     weird techno-utopian singulartarian fantasy,
     with hopes of being a living-forever billionaire).

     Yet our concern is not just about proving
     that AGI systems cannot be exactly 100% safe --
     ie; about some sort of silly 'confidence' nit-pick
     that there is "a real difference" between
     'actually safe enough for practical use',
     and the irrational "absolute perfection 100% safe".
     This is not an argument against 100% fail-safe alignment;
     any 100% fail-safe alignment is impossible in any case.

     Our claim is about showing that AGI/APS cannot be,
     over the long term, eventually and for sure,
     anything other than actually 100% harmful --
     ie, not safe at all, not for *anything*,
     not for even ridiculously bad notions of "safe",
     where the bar is so low as to be unrecognizable
     to most forms of popular human common sense.

     Unfortunately, this result is also rather negative.
     I simply do not see how any sort of marketing "spin"
     can help here.  We need to stop investing/wasting
     time and resources on the actually impossible,
     and/or on delusions that it could ever be
     any other way at all, no matter how utopian
     and/or hyped the AGI benefit outcome fantasies
     were at one time.

     Getting something from nothing
     (getting benefit without effort)
     is simply another perpetual motion delusion --
     no matter how hopeful
     the researchers or investors might be
     or what buzzword terms are suggested
     along the a way to yet more lost capital.

     Of course, no investor will ever thank us
     for saving the resource, for helping them to avoid
     near total embarrassment and eventual total ruin.
     Non-investments make no money.
