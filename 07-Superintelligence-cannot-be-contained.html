<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Basic Meta Tags -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- SEO Meta Tags -->
  <meta name="description" content="Comprehensive AGI Risk Analysis">
  <meta name="keywords" content="agi, risk, convergence">
  <meta name="author" content="Forrest Landry">
  <meta name="robots" content="index, follow">

  <!-- Favicon -->
  <link rel="icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">
  <link rel="shortcut icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">

  <!-- Page Title (displayed on the browser tab) -->
  <title>Comprehensive AGI Risk Analysis</title>
</head>
<body>
  <p>
  TITL:
    <b>Response to the Response to the Article</b>
    <b>'Superintelligence cannot be contained'</b>.
    <b>Commentary and review by Forrest Landry</b>,
    <b>September 2nd, 2022</b>.
  </p>
  <p>
  ABST:
  </p>
  <p>
    Shortly after there was published a technical paper
    that reasoned that general AI is, in principle, uncontainable,
    and that therefore, is/was and will be inherently unsafe,
    and which received some significant popular publicity;
    that there was published another position paper,
    written just for popular audiences, saying that the
    notion of "uncontainable" simply did not apply,
    and that we should continue to attempt to research
    and develop AGI/Superintelligence, use formal methods, etc.
  </p>
  <p>
    This latter writing, although invoking technical concepts
    (by reference) did not actually establish an argument
    that AGI safety was even possible, even though it attempted
    (but failed) to find loopholes and "exceptions", etc,
    in the prior non-containable argument.
    The commentary herein expands the technical aspects
    sufficiently to show the reasoning errors/equivocations
    that result in the false rejection of the "uncontainable"
    result.
  </p>
  <p>
  PREF:
  </p>
  <p>
    - where regarding the commentary at found at URL:.
      > https://www.cser.ac.uk/news/response-superintelligence-contained
  </p>
  <p>
    - that the commentary herein is further expanded, re-written,
    and developed further in (@ these notes https://mflb.com/ai_alignment_1/si_safety_qanda_out.html).
  </p>
  <p>
  TEXT:
  </p>
  <p>
    > Response to 'Superintelligence cannot be contained:
    > Lessons from Computability Theory'
    > by Jaime Sevilla, John Burden
    > 25 February 2021
    >
    > In short: we respond to a recent article arguing that
    > ensuring that "superintelligent" AI systems are safe
    > is an impossible task.
    > We clarify that their argument
    > does not preclude practical work
    > on the foundations of safety in future AI systems.
  </p>
  <p>
      - Where as a 'response', the "rebuttal" argument
      for sure falls short (fails to convince)
      where/insofar as the meanings/distinctions
      of several key concepts are, in that article,
      tacitly assumed, equivocated, and conflated:.
        - 1; the functional differences between narrow AI vs general AI.
        - 2; simple systems/programs with complex ones.
        - 3; recursive architectures with non-recursive architectures.
        - 4; the means, methods and concepts of "proof".
        - 5; specific meaning of the term 'specific'
        (as multiply used in inherently ambiguous ways).
        - 6; the scope and extent of risk of local limited problems
        (in time/space) with global problems (everywhere forever).
  </p>
  <p>
      - That it is a tacit assertion/expectation
      of the Superintelligence Safety Community
      that somehow (via hope and obscure math)
      that we would, "at least in principle",
      be able to make/enforce "Safe" and "Aligned" AGI.
        - that this hopeful belief is, so far,
        justified only by motivated reasoning,
        and has not actually been substantiated.
  </p>
  <p>
      - While there are a <b>lot</b> of very strong indicators,
      and many actual and well understood principles
      that suggest that the opposite is true:
      that AGI <b>safety</b>, over the long term,
      is actually an impossible to realize,
      both technically and feasibly;
      there have been no explicit statements of principle
      by which anyone could have a reasoned belief
      in the possibility of "human aligned/safe AGI";
      that there is, so far, exactly nothing, no arguments
      (no principles concretely posited and put forward),
      which suggest anything other than unfounded opinions
      that "technology and progress can solve all problems".
  </p>
  <p>
      - As such, the "rebuttal" tends to read much more
      _as_if_ it is rhetoric which has accidentally
      (as if politically) been designed and disguised
      _as_if_ to confuse and FUD an unsuspecting public.
        - of course, there is no way to know what the actual
        innermost intentions any authors actually have;
        instead, we can only review what is actually stated,
        and what sorts of implications, social and otherwise,
        such statements have and imply, overall.
        - unfortunately, this impression of 'spin intent'
        is rather too strongly consistent and re-inforced
        with the popular audience writing style used by
        so many corporate technology apologists over the years.
  </p>
  <p>
      - There is a strong concern that public releases
      by AI existential safety researchers, such as this one,
      can easily further perpetuate incorrect, non-rigorous
      (and perhaps strategically abused) misapprehensions
      that AGI advocates (maybe accidentally) perpetuate on themselves.
        (ie; responding on the basis of false hope, social media hype,
        and/or dream of unlimited private profits, colonization,
        power and an seemingly unassailable professional prestige, etc,
        backed by perfected (though empty) promises to humanity, etc).
  </p>
  <p>
      - Therefore, if there are any doubts misunderstandings at all
      regarding these important technical/practical truths
      in the space of a critical and extinction terminal x-risk
      inherently associated with <b>all</b> future general AI systems,
      that these arguments herein are provided so as to make
      all relevant concepts and ethical implications
      as clear as possible.
        - as attempting to better explicate
        the validity of the observations/arguments
        regarding the Rice-style non-trivial properties
        in/of <b>all</b> types of superintelligent machines
        <b>without</b> the common social/political rhetoric(s).
  </p>
  <p>
      ~ ~ ~
  </p>
  <p>
  :e9l
      - where considering the conflation of what is meant by AI:.
  </p>
  <p>
        - that the notion of 'AI' can be either "narrow" or "general".
  </p>
  <p>
        - that the notion of 'narrow AI' specifically implies:.
          - 1; a single domain of sense and action.
          - 2; no possibility for self base-code modification.
          - 3; a single well defined meta-algorithm.
          - 4; that all aspects of its own self agency/intention
          are fully defined by its builders/developers/creators.
  </p>
  <p>
        - that the notion of 'general AI' specifically implies:.
          - 1; multiple domains of sense/action.
          - 2; intrinsic non-reducible possibility for self modification; and;.
          - 3; therefore; that the meta-algorithm
          is effectively arbitrary; hence;.
          - 4; that it is _inherently_undecidable_ as to whether
          <b>all</b> aspects of its own self agency/intention
          are fully defined by only its builders/developers/creators.
  </p>
  <p>
      - that the key question is the following:.
        - ?; is there (even in principle) any practical way
        to establish/ensure adequate levels of safety
        (to whom?, for what/why?)
        in future AI systems?.
  </p>
  <p>
        - where the notion of 'adequate'
        (as applied to 'safety')
        for any proposed action 'X'
        is to show that the probability
        of all categories of risk/harm/loss
        of the value of what is to be "safe"
        is sufficiently low -- ie; _strictly_lower_ --
        than the possible cost/risk/harm
        incurred by <b>not</b> taking that risk
        (ie; in the form of opportunity cost, etc).
  </p>
  <p>
      - where considering the equivocation
      of what or who is to be kept "safe".
  </p>
  <p>
        - example; that a corporation (or totalitarian government)
        that stands to gain greatly
        from the deployment of an AI
        implemented for its own internal profit interests
        could easily regard something as 'safe' (for/to itself),
        even though their action/deployment of the AI
        is very unsafe to the world population
        (all non-corporate/government people)
        and/or to life itself (deep ecosystem damage).
          - where historically, it has been commonplace
          inside large institutions and connected intellectual movements
          for decision-makers to incorrectly rationalize
          and publicly claim the safety of
          newly deployed products/services/policies,
          often with the backing of allegedly
          comprehensive and/or authoritative
          empirical findings and argumentation
          by employed/sponsored experts.
          - that current funding (and marketing)
          by 'big tech' corporate sponsors (and donors
          who earn something privately
          from the scaling of new technology)
          of institutes, conferences
          and supposedly independent research
          into prosaic AI control/robustness, safety, and ethics
          has some clear correspondences with past activities
          by tobacco, pharmaceutical oil and military corporations.
  </p>
  <p>
        - that/therefore it is inherently important to clarify:.
          - who or what is to be kept "safe"
          when thinking about safety.
          - whose intentions (and/or expected outcomes of benefit)
          are being 'aligned with'
          when thinking about 'AI alignment'.
            - as that having an AI be aligned with corporate
            or government interests (who can afford to build the AI)
            is very likely to be unaligned with
            the interests/well-being of the people
            and/or the world (ecosystem/environment;
            ie; all other life on planet).
  </p>
  <p>
          - that the mere fact of AI systems ("robots")
          being hard and expensive to build
          and requiring of significant and very specialized
          technological expertise, situational awareness, etc,
          is to also therefore effectively ensure/require
          that only the most wealthy corporations (and/or governments)
          will be able to create such devices,
          and that all other poorer and/or less skilled people
          will inherently be unable to create/maintain/use
          similar systems to a similar degree.
            - ie; that single individuals or small groups,
            even if they had the (secondary) use of
            whatever deployed/created AI systems
            will be unable to obtain commensurate private benefit.
            - as that larger corporations will be able to accrue
            benefit to themselves for/via the use of these tools
            at much greater scales and percentages of yield
            than any smaller actors/agents will be able to.
  </p>
  <p>
            - example; that wealthy people/groups/institutions
            are both more able
            to leverage their money
            to accrue more money
            (at increased economies of scale)
            and that they are more efficient at doing so
            (since they can also leverage more intelligence
            to optimize the investment/return process
            since they can optimize on more diverse aspects
            by hiring/engaging whatever resources are needed
            to cover each of whatever multiple optimization aspects
            are needed/available).
              - that the rate of hiring people/expertise
              to optimize investment strategy is additive/linear,
              whereas the rate of return on optimization investment
              is either multiplicative (where single process)
              or exponential (where multiple interacting market process).
              - that ordinary/regular people/groups
              have finite and non-expandable attention/resources
              to implement such multiple optimizations,
              and so their rate of return on investment
              is inherently going to be strictly lower,
              on all process average, over the long term,
              than that associated with larger/richer groups.
              - as an inherent vector of inequality increase.
  </p>
  <p>
  :ebs
    > The paper "Superintelligence Cannot be Contained:
    > Lessons from Computability Theory"
    > by Alfonseca et al
    > argues that ensuring
    > that a sufficiently complex Artificial Intelligence
    > does not perform an unsafe action
    > is mathematically impossible.
  </p>
  <p>
      - where revising/clarifying the assertion/claim/conclusion
      as:.
        - that it is (both mathematically and physically) impossible
        to ensure that a sufficiently complex Artificial Intelligence
        does not perform an unsafe action.
  </p>
  <p>
        - where 'sufficiently complex'
        means can or does 'modify its own code';
        then/that this claim statement
        is categorically true.
          - where the notion of 'learns'
          implies 'modifying its own behavior'
          that the notion of 'learning how to learn'
          can directly imply (cannot not imply)
          modifying its own code.
          - that the notion of 'general'
          can (must?) eventually include
          modifying its own code at any (possible) level.
            - ie; as including at the level of substrate,
            though <b>not</b> including the level of the
            regularity of the lawfulness of physics.
  </p>
  <p>
  :ed2
    > Early versions of the paper date back to at least 2016,
    > but following its recent publication
    > in the Journal of Artificial Intelligence Research
    > it has received a lot of attention in the press.
    >
    > We have seen that many of the popular summarizations
    > portray the paper
    > as refuting the possibility of designing complex AI systems
    > without unintended consequences.
  </p>
  <p>
      - ?; can anyone, ever, at any time,
      at any level of abstraction,
      even in principle,
      ever formally/exactly/unambiguously/rigorously prove
      that something (some system, some agent, some choice)
      will <b>not</b> have 'unintended consequences'?.
  </p>
  <p>
        - 0; as assuming that the domain of mathematics/modeling/logic
        and the domain of physics/causation (the real world)
        are equivalent.
  </p>
  <p>
        - 1; as assuming that the physical universe is closed
        in both possibility
        (ie; that all possibilities can be finitely enumerated)
        and probability
        (ie; that the probability over each of these
        can be exactly calculated and/or that any one of these
        probabilities for some explicit subset
        of the available possibilities
        can be shown to be exactly zero).
  </p>
  <p>
        - 2; as assuming the notion of 'intention'
        is exactly specified and specifiable,
        at all levels of abstraction
        for <b>any</b> and <b>every</b> agent
        that could be involved.
  </p>
  <p>
      - ^; where <b>all</b> of these assumptions (and their implied claims)
      is definitely false;.
  </p>
  <p>
        - 0; that the physical causative universe has hard limits
        of knowability and predictability
        (via the Planck limit,
        Heisenberg uncertainty principle,
        and similar).
  </p>
  <p>
      - ^; that the answer to the question cannot not be other than "no".
  </p>
  <p>
      - that the basic possibility of unintended consequences
      to/for/with any action/choice/system
      will forever remain non-singular and potentially unbounded.
      - ?; what is the likelihood of unintended consequences?.
  </p>
  <p>
      - ?; does the likelihood of unintended consequences
      increase with:.
        - the complexity of the intentions?.
        - the complexity of the beings/agents
        implementing those intentions?.
        - the likelihood of those beings/agents
        having their own (potentially different) intentions?.
  </p>
  <p>
      - ^; yes, for <b>each</b> of these aspects.
        - that the likelihood of unintended consequences
        increases with some product or exponent of these factors.
          - where these factors can combine
          more than additively and more than multiplicatively,
          in various forms of feedback cascade, etc.
        - that the likelihood of unintended consequences
        with a complex system (inclusive of self definition recursion)
        is strictly very much greater than for any simple system
        (which lacks these critical features).
          - that there is a risk of a 'Motte and Baily defense' (MBD)
          being attempted in the space of
          the claim "we can make AI systems safe".
            - that any argument with an underlying MBD format
            is a type of logical fallacy
            similar to equivocation.
            - as an argument of rhetoric,
            of human fallible persuasiveness,
            rather than one of actual truth.
  </p>
  <p>
      - where explicit claim; that in general,
      that there is no/zero possibility
      of anyone (of any human, especially)
      designing complex general (self intentioned) AI system(s)
      without there also being a significant excess potentiality
      of significant adverse unintended harmful consequences/risks
      (which may be irreversible, to humans, the biosphere, etc).
        - ie, as risks/costs/harms
        resulting from 'X' (ie; the use of AGI systems,
        the making of such choices to make/deploy AGI, etc)
        that are well in excess of any realistic
        and appropriately generalized assessment
        of the purported benefits/profits of 'X'
        (to who, when, where, etc).
  </p>
  <p>
  :eew
    > If this were true,
      (that there is no possibility of designing complex AI systems
      without unintended consequences)
    > it would have huge implications for
    > the well established field of formal verification...
  </p>
  <p>
      - that such a proof establishment
      would have no (zero) implications (at all)
      with respect to the field and practice
      of formal verification
      (or proof of limits of the range
      of process outcome states,
      given ranges of input states)
      insofar as such formal verification techniques
      are generally only applied
      to smaller and more tractable systems and algorithms.
        - that practitioners of formal verification have <b>never</b>
        made any claim we are aware of
        that is equivalent to the idea/assertion
        "that 'formal verification' could be applied to
        (or used for/on) any/arbitrary programs/systems".
  </p>
  <p>
    > ...and the budding field of Artificial Intelligence Safety --
  </p>
  <p>
      - ie; ?; can we make narrow AI systems "safe" (for at least some
      selected groups of people some of the time) and/or "aligned"
      (with the interests/intents/benefits/profits of at least some
      people some of the time)?.
      - ^; yes; formal methods could potentially be applied,
      at least in principle, to establish such notions
      of partial safety/alignment (in the short term,
      though probably not in the long term (decades)).
  </p>
  <p>
      - where/however the more interesting/important question is:.
  </p>
  <p>
        - ?; can anyone at all,
        even in principle,
        even with unlimited resources,
        somehow "make" general AI systems "safe",
        (or "aligned", etc,
        for any reasonable definitions of these terms,
        as applied over any reasonable interval of time)
        for <b>any</b> actors/agents
        which are not themselves
        strictly equivalent to,
        or functionally equivalent to,
        or at least sub-aspects of,
        the GAI embodiment itself?.
          - ie; as safe for all outside humans
          and other (carbon-based living) actors?.
  </p>
  <p>
  :ehq
    > [where/if general AI safety/alignment
    > was proven impossible in principle]
    > we would be compelled to put away resources [and tools]
    > from these fields
    > [of formal verification of simple/finite systems/algorithms
    > and/or of the possibility utility/safety of narrow AI systems]...
  </p>
  <p>
      - note that the conclusion does not follow from the premise.
        - as a kind of equivocation of/on/over:.
  </p>
  <p>
          - 1; what the field of formal verification
          can be successfully used for and/or applied to.
  </p>
  <p>
          - 2; the real distinctions
          of the risks/costs/benefits
          (the expected use and utility
          to at least some subset of people)
          associated with making and deployment of narrow AI
          vs the very different profiles of risks/costs/benefits
          associated with the potential creation/deployment
          of general AI.
  </p>
  <p>
      - that we <b>would</b> have to "put away" are the notions:.
        - 1; that there was ever, even in principle,
        any way to make general AI safe, or aligned, etc.
        - 2; that there was any actual utility/benefit/profit
        to the creation and deployment of general AI
        for anyone, at all, ever, over the long term.
      - ^; and that therefore we should also 'put away'
      the tools and techniques used to develop such systems
      (and all of the associated concepts, IP, etc)
      as they are inherently a unmitigatable risk to
      all peoples on the planet, for all future time,
      and for all of the rest of life on the planet
      and maybe even inclusive of the planet itself.
        - that a failure to do so (and/or to develop, enact,
        and enforce, effective and realistic policy and procedure
        for all institutions and communities internationally)
        is equivalent to a failure of government
        (where the final function of all government
        is to protect the land and the people and/or
        to try to ensure that the land and people thrive).
  </p>
  <p>
    > ...given the intractability of the foundational problems
    > they aim to solve.
  </p>
  <p>
      - note that the development of proofs of impossibility
      of squaring the circle, trisecting the angle,
      and/or 'identifying the last digit of pi',
      (or of the rationality of pi, having a last digit, etc)
      had <b>zero</b> implications that geometry, algebra, etc
      were "useful" and could not continue to be
      applied to purpose, had to be put away/discarded, etc.
  </p>
  <p>
      - where given the specific the proof
      that the continuum hypothesis,
      as a foundational problem,
      was actually intractable (given available axioms)
      is not equivalent to the specific claim
      that some field of study claimed to be
      able to solve every specific foundational problem.
      - that no one in the field of formal verification,
      (or in the field of AI alignment safety either)
      has made the general claim that,
      at least in principle,
      that the tools already available
      in such such fields of study or practice
      'could even potentially solve all foundational problems'
      that exist within in their field.
  </p>
  <p>
    > However a close examination of the argument
    > reveals that this is not the case.
  </p>
  <p>
      - where caution; where in the above statement,
      that "this" as a reference to 'X'
      which is at least partially non-explicit/undefined.
        - as a setup for equivocation on what exactly
        is being claimed, as known to be true, and by who,
        how, etc.
  </p>
  <p>
    > In this piece we will outline their core argument,
    > and clarify its implications.
  </p>
  <p>
      - though those implications were actually obscured,
      hence the need to write this rebuttal to your rebuttal.
  </p>
  <p>
  :ekw
    > The core of the argument
    > in Alfonseca et al's paper
    > is not original,
    > and goes back to Henry Gordon Rice in 1953.
    >
    > One well known result in computability theory
    > is the impossibility of
    > having a general procedure that would,
    > given any program,
    > decide whether the program
    > will eventually stop
    > or whether it would run forever
    > (this is known as "The Halting Problem"
    > in the literature).
    >
    > Rice showed that an immediate corollary
    > is that there are no procedures that, in general,
    > can identify whether a given program
    > has any particular property,
    >   for example whether the program
    >   is computing whether its input is a prime number.
    >
    > Alfonseca et al point out
    > that Rice's proof also applies
    > to any property about safety.
  </p>
  <p>
       > If one had a procedure which,
       > when given the code for any program,
       > could decide whether an unsafe function
       > is eventually executed,
       > then one could use this to decide whether
       > any program stops.
       > One would just have to write a program
       > like the one in table 1 below:
       >   this program will execute the unsafe_function()
       >   if and only if an arbitrary 'program2' halts.
       >
       >   def program(input):
       >       // - program2 is the program we want to know
       >       // if it halts.
       >       program2(input)
       >       // - the unsafe function is executed
       >       // if and only if program2 halts.
       >       unsafe_function()
       >
       >   Table 1: A program we could use to decide
       >   whether a second program halts,
       >   if we could decide whether any given program
       >   will ever execute a known unsafe function
  </p>
  <p>
    > Alfonseca et al conclude from this
    > that AI safety is impossible
    > when it comes to superintelligent systems;
    >   no matter how one defines safety,
    >   or how exhaustive one thinks one's review process is,
    >   there will be programs which will fool it.
  </p>
  <p>
      - where stating the 'Rice theorem' more carefully:.
  </p>
  <p>
        - that there is no single finite universal
        procedure, method, processes or algorithm
        (or even any collection of procedures, etc)
        by which anyone can (at least in principle)
        identify/determine (for sure, exactly)
        whether some specific program/system/algorithm
        has any particular specific property,
        (including the property of 'aligned' or 'safe'),
        that will for sure work (make a determination)
        for every possible program, system, or algorithm.
  </p>
  <p>
      - note that the Rice Theorem does <b>not</b> claim
      that there are no specific procedures,
      (processes, methods, or algorithms, etc)
      by which one could characterize some well defined
      (usually fairly simple) specific finite algorithm
      as having some specific property.
        - for example; that it might be possible,
        using some (as yet unknown) procedure,
        to identify that some narrow AI is safe,
        for some reasonably defined notion of 'safe'.
      - that what the Rice Theorem <b>does</b> claim
      is that whatever procedures are found
      that can maybe work in some cases,
      that there will always be some other
      (potentially useful) programs/systems
      that inherently cannot be characterized
      as having any other arbitrary specific desirable property,
      even if that property is also well defined.
        - as that there is no way to establish
        any specific property as applying to
        every possible useful program/system.
  </p>
  <p>
  :epy
    > And a "superintelligence", they argue,
    > ought to be general enough
    > to arbitrarily execute any program.
  </p>
  <p>
      - that this is more than just "ought".
      - ie; that the notion of 'generality',
      when applied to any AI,
      will very easily enable that 'general AI'
      to also implement and execute arbitrary programs.
      - where considering the Church Turing Thesis,
      and ongoing widely extensible and available results
      in the field of computational science,
      it turns out that the threshold needed to obtain
      "general computational equivalence"
      is ridiculously low.
        - that nearly anything that implements
        and/or "understands" or responds to
        any sort of conditional logic,
        of doing and repeating anything
        in some sort of regular
        or sequential order,
        already implements all that is needed
        for general algorithmic computation.
        - moreover; embedding or interpreting
        one language, process, program, model, or algorithm
        within the context of some other process
        language, model, algorithm or program, etc --
        ie, the notion of 'virtualization'
        is used in comp-sci all the time.
  </p>
  <p>
      - however; where/rather than emulating or virtualizing
      some arbitrary algorithm within some aspect of
      the general capabilities of the general AI;
      that a general AI could as easily modify its own code
      and programming to directly incorporate and integrate
      that arbitrary algorithm.
  </p>
  <p>
  :erj
    > So ensuring the safety of such a general system
    > would require ensuring the safety of arbitrary programs,
    > which we can show it is impossible.
  </p>
  <p>
      - ?; is the notion of "general AI"
      going to somehow be construed as
      a process that somehow is going to be prevented,
      forevermore,
      from emulating, modeling, or integrating
      and then calling
      any other arbitrary process?.
        - ?; is someone attempting to be claiming somehow
        that some general AI can be made
        that will not ever extend or increase its "generality"
        by:.
          - 1; adding to itself some other module or program?.
          - 2; emulating and "running"
          some other arbitrary program,
          so as to get advice as to its own choices?.
          - 3; maybe even just using some external service
          like any other client of an API, maybe electing to
          treat the output of that service
          as some sort of oracle or influence for its own choices?.
  </p>
  <p>
        - where in any of these cases,
        like attempting to be predicting the future
        of/for anything else;
        that we cannot know in advance, for sure,
        the specific nature of any of these programs,
        regardless of its method of integration
        (call internally, call externally, or emulate).
        - that even fixed deterministic programs
        can easily be made to call other
        unknown arbitrary other programs,
        ones that have unknown/unsafe side effects,
        and thus, by proxy, become unsafe themselves.
          - that we could not completely and accurately predict
          the outcome of any deterministic process
          calling any other non-deterministic process
          (ie; which contains hard randomness,
          perhaps by consulting a Geiger counter),
          means that determinism/tractability
          is actually the <b>weaker</b> constraint.
  </p>
  <p>
  :et4
    > Hence, they argue, it is not possible
    > to show that a superintelligent AI system
    > will be safe.
  </p>
  <p>
      - where as a possible clarification,
      let us posit explicitly that
      by some arcane literary magic,
      that we have created an instance of a general AI,
      an artificial agent or metal robot, etc,
      that is inherently perfectly following
      of Asimov's Three Laws of Robotics.
        - that the assumption here is that by somehow
        <b>requiring</b> that the three laws are perfectly followed,
        that we can then assert that that robot/intelligence/agent
        is therefore 'safe', and 'aligned' with human interests
        and wellbeing, etc.
        - note that by making the robot AI simpler, more finite,
        more deterministic, and more like narrow AI,
        that the chances that 'formal methods' could potentially
        be used and that some clever engineering could be done
        so as to make and ensure our (inherently finite)
        Generalized Asimov AI agent is 'safe' and 'aligned'.
  </p>
  <p>
      - ?; is it actually impossible, in principle, to make
      a known verified and perfected/proven "safe" system
      of generalized agent intelligence
      operate in a way that is somehow unsafe/unaligned?.
  </p>
  <p>
      - ^; no, by the mere fact that it its computational
      ability is both finite and bounded in space and time.
        - that all that is needed is to have any situation
        where the 'safe' robot/program consults with and/or
        is influenced in its output choices/actions/behaviors
        (in some/any way) by some other unsafe device --
        such that it, for instance, can figure out how to deceive it
        into doing things that are unsafe/unaligned,
        but which would remain undetected/unnoticed by
        the Asimov robots three laws detection system.
  </p>
  <p>
        - where for every finite robot/process that is safe,
        that it will necessarily interact with
        outside real-time processes,
        as connected within a (necessarily larger) physical world
        in ways that would <b>not</b> be able to be
        detected by that lessor agent
        as being unsafe/unaligned, etc.
  </p>
  <p>
          > "I would use this Ring from the desire to do good;
          > But through me, it would do great evil".
            -- paraphrase of Tolkien, in the Lord of the Rings.
  </p>
  <p>
        - that the notion that any agent is able to be
        'having the property of safety/alignment/etc'
        is now requiring that such agent
        never, at any point in the future,
        come into contact with some other, arbitrary, unsafe
        agent/process/algorithm (program, model, recipe etc)
        that it cannot somehow misunderstand as having
        unsafe implications.
          - that nothing is going to be a perfect prognosticator
          at predicting the future of everything else
          (all other processes/algorithms/programs
          in the universe),
          and thus know what to interact with
          (and/or be influenced by)
          and know what to not interact or be influenced by,
          even indirectly, through all possible other channels
          of interaction, overt and covert, etc.
          - as that not only must our "perfectly proven safe agent"
          successfully predict the outcomes of its interactions
          with any single other (potentially intelligent) agent/algorithm,
          but it must also predict all possible interactions
          via all possible channels of such interactions
          of all such other agents/algorithms, etc.
  </p>
  <p>
          - that the finite and bounded will not ever be able
          to predict, accurately, the interactions of that which,
          though also finite, is at least potentially unbounded,
          or at least, significantly greater than itself.
  </p>
  <p>
      - that it will always be possible (and maybe even likely)
      to have a known verified and perfected/proven "safe" system
      of generalized agent intelligence
      operate in a way that is unsafe/unaligned
      if it is ever allowed to interact with any other algorithm
      that cannot (by any technique) be itself proven to be safe.
      - where there is always at least one such algorithm/process,
      and where it is impossible, in general, to determine which
      ones are which, then the only safe/"aligned" things it can do
      is to not interact with any other process/agent/system.
        - as that even the interactions between strictly safe systems
        can result, in aggregate, in overall unsafe/unaligned outcomes.
  </p>
  <p>
  :euy
    > It seems like an argument similar to Alfonseca et al's
    > could show that all programs are unpredictable.
    > But if I write a program that simply prints on screen
    > "I am not dangerous" and then finishes,
    > then this program is quite predictable.
  </p>
  <p>
      - note; straw-man argument,
      conflating simple finite programs
      with clearly bounded states of potential interaction
      and which can have known definite simple properties
      with complex programs, having complex interactions
      with the environment (users/etc),
      which are not so easily proven, by any technique,
      to have complex and/or not so well defined properties
      (usability, desirability, salability, safety, etc).
  </p>
  <p>
    > In fact, engineers every day manage to write sophisticated programs,
    > and reason consistently about the consequences
    > of deploying the software.
  </p>
  <p>
      - that reasoning is not always right;
      engineers often also make mistakes,
      resulting in aircraft that crash (Boeing 737 Max)
      or cars with stuck gas pedals (Toyota's ETCS)
      or medical systems that kill (Therac-25) --
      all of which were unsafe due to code problems,
      <b>despite</b> the attempted application of "formal methods".
  </p>
  <p>
    > We have also seen many successes in formal verification research;
    > for example guaranteeing that an operative system
    > is correctly implemented.
  </p>
  <p>
      - a not unreasonable interpretation of this sentence
      is that we "should" rely on
      the skill of "fallable human engineers"
      to assure us, yet again,
      that a technology to "replace humans"
      is actually "safe" (will not somehow be worse),
      while irreversibly betting the future
      of the entire human race,
      along with all other life on the planet.
  </p>
  <p>
  :ex6
    > Where does this application of the argument break down then?
    > The key is that Rice's theorem
    > prohibits a decision procedure
    > for deciding any property of an arbitrary program.
    > But in application we are not interested in arbitrary programs;
    > we are interested in studying particular programs.
  </p>
  <p>
      - this is a red herring --
      there is a very wide class of 'particular programs'
      in current wide use for which the behavior
      is unmodelable by anything that is simpler than
      running the actual program itself.
        - if running the program is unsafe,
        then running the program is unsafe.
  </p>
  <p>
      - that trying to determine experimentally,
      by trial and error,
      whether some program has some property like safety
      is deeply irresponsible
      especially when considering systems
      with known and acknowledged potential
      for existential risks.
        - note; that it does not matter
        how well defined,
        or how specific,
        our knowledge may be
        of the exact sequence of specific instructions;
        the unknowability of the risk and alignment profile
        for a very large class of actual programs
        remains inherently unknown and unknowable.
      - as very different than running something
      in non-catalytic environments
      where even the worst outcome
      is limited to the local failure of equipment,
      or, at most, strictly local damage/destruction.
        - for example; that one does not experiment with
        dangerous 'gain of function' infectious virus research
        when out in the open, in unprotected spaces!.
        - as similar to worries
        that another Covid might happen.
      - that the behavior of simple systems
      with non-catalytic effects
      is very different, in risk profiles,
      than even fairly simple systems
      with inherent auto-catalytic effects.
        - that the latter is very unsafe,
        despite the apparent deceptive simplicity
        and finiteness of the algorithmic code.
  </p>
  <p>
      - where from other areas of comp-sci research;
      that there are very strong indications
      that once the inherent Kolmogorov complexity
      exceeds a certain (fairly low) finite threshold,
      that the behavior of the program/system
      becomes inherently intractable.
        - where considering a 'property of the system'
        as basically some abstraction over
        identifying specific subsets
        of accessible system states;.
        - that no specific property
        of such a complex system
        can be determined.
        - that this is due to significant reasons
        other than just the Rice Theorem.
  </p>
  <p>
  :ez2
    > By way of analogy:
    > there is no procedure that will,
    > given any mathematical statement,
    > produce a proof or prove the opposite.
    > But this does not mean
    > that a proof of Pythagoras' theorem
    > is impossible!
  </p>
  <p>
      - when both proofs are correct,
      they can co-exist.
        - that proving one thing
        does not "disprove" another thing
        that has already been proven.
  </p>
  <p>
      - that the claim being made is
      that there is no formal or practical way,
      even in principle,
      to establish even a fairly limited set
      of adequate bounds
      as to whether or not
      any sufficiently complex and general system
      with self-modifying properties
      (ie; a learning system,
      a system that adapts itself and/or its behavior
      to its environment, operating context, etc,
      as a self adaptive system, etc),
      will eventually, and even very likely, exhibit
      significantly unsafe and unaligned behavior,
      create unsafe and unaligned outcome conditions,
      have such conditions
      create enduring manifest harm, etc.
  </p>
  <p>
        - when using the word "likely" in the above,
        the assessment is that the general costs and risks
        associated with AGI development
        will (eventually) greatly exceed
        any reasonable (non-hyped) assessment
        as to any potential benefits
        that the use of such a system/technology
        could ever potentially have,
        in any context more generalized and extended
        than even one person's own limited lifetime.
  </p>
  <p>
        - that attempting to build AGI
        is a strictly negatively asymmetric bet
        both from the perspective
        of a single large institution of people
        and (more so) from the perspective
        of all living persons/beings and their descendants:
           - where such an attempt fails,
           there is no (direct) benefit
           (a loss in comparison to opportunity costs forfeited).
           - where such an attempt 'succeeds',
           it opens up uncountably many unknown pathways
           that converge over time
           on ecosystem-wide lethal changes
           to conditions of the global environment,
           with any envisaged benefits of AGI and
           engineered-for alignments
           of AGI functions with human intent
           increasingly fragile
           to recursive nonlinear feedback between
           changes introduced by/in parts of AGI
           and the larger containing environment.
        - that this is analogous to playing Russian Roulette
        with the lives of all (human) beings smeared out over time,
        with any (illusory) backer of (promised) benefits long gone.
           - that betting on AGI
           converges on existential ruin.
  </p>
  <p>
      - that this does not, in any way,
      contradict the idea and fact
      that we can make non-recursive systems,
      that are known to be useful
      to a wide range of people.
  </p>
  <p>
  :f2l
    > To write a program
    > whose outcome is unpredictable in principle
    > takes deliberate effort.
  </p>
  <p>
      - to create something actual, in nature,
      using real physics that has real uncertainties built in,
      means that creating things whose outcome is unpredictable
      (inherently, and in principle, once stats is factored out)
      is really quite easy, and moreover,
      happens more often than not.
      - that creating things in nature
      (in the real world)
      whose outcomes are <b>predictable</b>
      takes significant deliberate effort.
        - as the actual work of engineering.
  </p>
  <p>
      - that the actual world is not a computer model --
      no one has proven that we actually live in a simulation.
      - that anyone attempting to make the claim
      that "the real world" and "computer modeling"
      are actually and fundamentally strictly equivalent,
      would need to produce some high class extraordinary evidence.
        - that until such incontrovertible
        empirical or logical demonstration
        is actually obtained, provided, etc,
        then the absence of this assumption,
        that the real world is not a model,
        that they are maybe somehow different,
        will be upheld.
  </p>
  <p>
  :f46
    > In practice, engineers and researchers in AI
    > almost never need to bother thinking about
    > for example
    > whether their program will ever halt.
  </p>
  <p>
      - mostly because whether or not halting
      does not matter;
      all future people dying --
      let's think more about that possibility.
  </p>
  <p>
    > In short: we will never have a procedure
    > that takes any chunk of code
    > and accurately predicts the consequences of running the code.
    > But we can, and routinely do,
    > predict some consequences of running a specific piece of code.
  </p>
  <p>
      - that this works for some types of specific code,
      that we wrote with the specific intention to be
      understandable, predictable, modifiable, updateable, etc.
      - that self generating/authoring/modifying AGI code
      will not likely have any of these features,
      and so, reasoning about the future unknown states
      and variations of that potential/possible code
      is a lot more like the "any unknown arbitrary chunk of code" case
      than it is the "known specific code" case.
  </p>
  <p>
    > Furthermore, one can write one's code
    > in such a way
    > that it is more transparent
    > and amenable to formal (and informal) verification;
    > this is the approach taken by many experts in formal verification.
  </p>
  <p>
      - ?; is this a claim that future AGI,
      however its written at all,
      will be similarly transparent and amenable?.
      - ?; is this tautologically assuming
      that we will have what we are wanting to have --
      that AGI is going to be composed of 'aligned code'
      so that it makes sure it remains amenable to 'tractability'
      so that we can continually verify it is 'aligned'?.
        - ?; how is this not assuming what we want to prove?.
  </p>
  <p>
    > Alfonseca et al are not ignorant of this pitfall.
    > Instead they add the assumption
    > that what they call a superintelligent machine
    > might be able to execute arbitrary programs as subroutines,
    > in an unpredictable manner.
    > Hence they reasonably suggest
    > that a complete solution to safety
    > would require [us] to solve the impossible Halting problem.
  </p>
  <p>
      - rather; that a "safe" or "aligned" machine/process/algorithm
      that calls/invokes (via whatever method,
      direct or indirect or simulated)
      some/any unsafe or misaligned machine/process/algorithm/code
      therefore (likely) becomes itself unsafe/unaligned
      (since it also cannot predict/assess anything which,
      in principle, cannot be predicted or assessed,
      any more than we could in the 1st place).
      - where any (generalized) (self learning/modifying) program,
      can in principle call or use any other program,
      then to assure alignment/safety of a general AI system,
      we would need to ensure alignment/safety of <b>all</b> possible
      future code/systems/procedures that it could invoke,
      or that those systems might themselves invoke
      (especially if they themselves are generalized intelligent
      or semi-intelligent agents, etc).
  </p>
  <p>
  :f62
    > But the AI itself could be in principle
    > engineered to not execute arbitrary subroutines!
  </p>
  <p>
      - this is an extraordinary strong claim --
      it is strictly equivalent to claiming
      that "all AGI is already inherently safe".
      - ?; how are you (is anyone) going to ensure that something
      that already inherently has Turing equivalent generality
      is not also going to have Turing equivalent generality?.
        - ie; the mere fact of the claim
        is itself already a direct a contradiction.
  </p>
  <p>
      - that the manner of how an algorithm
      is specifically divided up into subroutines
      is an arbitrary convention for the convenience
      and understanding of the programming engineer --
      it has no ultimate formal basis beyond that.
        - where from the perspective of a learning algorithm,
        the boundary between what is 'main code'
        and what is a subroutine
        is completely arbitrary --
        the changes associated with learning are exactly that,
        however they are expressed, remembered, and/or recorded.
  </p>
  <p>
      - that it is hard to see that it is possible,
      even in principle, to conceive of some notion of
      <b>generalized</b> learning process --
      ie, one that can self expand
      so as to operate on/over/within any domain
      of action, inclusive of itself --
      such that it can be wholly and absolutely prevented
      from executing any arbitrary algorithmic subroutines,
      for all of future time,
      without directly contradicting at least one
      of the notions of 'generalized', 'learning', or 'process'.
        - ie; either it is some sort of
        self modifying algorithmic intelligence
        (appropriately adaptive to its environment)
        or it is not.
        - if it is, then it is inherently unpredictable
        insofar as we never know what it is going to learn,
        and therefore do (its actual behavior; thus safety)
        any more than we can, in principle
        predict all of the future,
        absolutely and finally.
  </p>
  <p>
      - that the notion of 'generalized learning process'
      inherently cannot not somehow imply
      some real form of 'self modification' (ie; actual changes)
      which itself cannot not somehow imply
      some notion of 'arbitrary code execution' --
      ie; potentially and including possible calls
      to external processes, virtual simulation and modeling, etc,
      <b>all</b> of which are inherently unpredictable in advance.
  </p>
  <p>
      - that not only can we not expect, in principle,
      to be able to model all aspects of
      the generalized process of modeling,
      we also cannot expect to be able to
      set explicit finite limits
      on even the ranges of changes involved
      in arbitrary learning --
      ones that actually affect sensing,
      the ranges of abstractions sensed,
      the internal state transforms that result,
      and the behavior and outcomes of that, etc.
  </p>
  <p>
  :f88
    > Building complex AI,
    > even AI that surpasses human capabilities
    > in all tasks humans routinely face,
    > does not require a system
    > which executes arbitrary subroutines
    > in principle.
  </p>
  <p>
      - this is a very strong and unexpected claim --
      to which it is very easy to disagree/disbelieve;.
      - ?; where is the proof/evidence
      that such a system/machine/process
      is "in principle" even possible?.
  </p>
  <p>
  :f9s
    > The problems we face and are interested in
    > are not arbitrary.
  </p>
  <p>
      - note; equivocation of 'specificity' as applied to
      the <b>problem</b> of 'establishing generalized AI safety',
      rather than the specificity of being able to implement
      formal methods on some specific known example of algorithm
      for which such techniques are already know to work.
        - ie; ignoring that there are some specific and useful algorithms
        for which no one expects
        that there will <b>ever</b> be any techniques
        of applying formal methods
        so as to be able to establish
        some specific and well defined property X.
  </p>
  <p>
    > It is not even clear to what degree
    > could we call a system
    > systematically executing arbitrary programs "superintelligent";
  </p>
  <p>
      - notice; that stating that 'generalized superintelligence'
      therefore also inherently implies 'Virtual Machine capability'
      is <b>not</b> to claim that any/all generalized VM machines
      will therefore inherently also be "superintelligent".
        - that the fact of this fact
        does not matter very much,
        so it is hard to see why this point is even being made.
  </p>
  <p>
    > ...surely the routines it [the superintelligent] will execute
    > will be ones that are likely to end
    > and will help the system solve the problems it faces.
  </p>
  <p>
      - note; that in the same way that a typical engineer
      does not worry about the halting problem in typical practice,
      (because he can always go out of band and reset the computer),
      that an AGI would be similarly unconcerned about halting problems
      (and similarly be <b>also</b> unconcerned about AGI safety/alignment, etc)
      since it could (via it also having 'generality') go out of band
      and 'halt' (discontinue using/modeling, etc)
      whatever 'subroutine' it is invoking.
  </p>
  <p>
      - ?; is there some implied belief that the superintelligence
      will somehow care more, and/or also be more able to predict
      the outcome of calling any specific subroutine,
      than we would be?.
        - that anything that is mathematically impossible,
        including the halting problem and the Rice theorem,
        will remain that, even for a superintelligence.
  </p>
  <p>
      - that none of this argues that there is any principled
      reason to suggest that an AGI would be more likely
      to be aligned/safe
      than any person would be.
        - where after all, that it is quite easy
        to know that one engineer
        could be both unsafe and unaligned
        with any other engineer
        (ie; one engineer, could, in principle,
        shoot any other with a gun).
  </p>
  <p>
  :fbc
    > Another analogy: as pointed out in the paper,
    > "a man provided with paper, pencil, and rubber,
    > and subject to strict discipline,
    > is in effect a universal machine".
    > That is, a person could in principle
    > execute any arbitrary program.
    > But we hardly could call humans "unpredictable in principle";
  </p>
  <p>
      - ?; why not?.
      - that one can 'factor out' lots of common aspects
      of the behavior and choices of any other person,
      but that will simply leave an uncompressable remainder.
        - that no form of compression of any data stream
        factors out <b>all</b> information to exactly zero bits;
        which would be the very meaning of 'fully determined'.
  </p>
  <p>
      - that saying/claiming that <b>some</b> aspects,
      at some levels of abstraction,
      are sometimes generally predictable
      is not to say that _all_ aspects
      are _always_ completely predictable,
      at all levels of abstraction.
  </p>
  <p>
        - that localised details
        that are filtered out from content
        or irreversibly distorted in the transmission
        of that content over distances
        nevertheless can cause large-magnitude impacts
        over significantly larger spatial scopes.
  </p>
  <p>
        - that so-called 'natural abstractions'
        represented within the mind of a distant observer
        cannot be used to accurately and comprehensively
        simulate the long-term consequences
        of chaotic interactions
        between tiny-scope, tiny-magnitude
        (below measurement threshold) changes
        in local conditions.
        - that abstractions cannot capture phenomena
        that are highly sensitive to such tiny changes
        except as post-hoc categorisations/analysis
        of the witnessed final conditions.
  </p>
  <p>
      - where given actual microstate amplification phenomena
      associated with all manner of non-linear phenomena,
      particularly that commonly observed in
      all sorts of complex systems,
      up to and especially including organic biological humans,
      then it <b>can</b> be legitimately claimed,
      based on the fact of their being a kind of
      hard randomness associated with the atomic physics
      underlying all of the organic chemistry
      that in fact (more than in principle),
      that humans are inherently unpredictable,
      in at least some aspect, <b>all</b> of the time.
  </p>
  <p>
    > the programs that they [the humans acting as a VM]
    > will simulate follow some logic.
  </p>
  <p>
      - whether a program continues to run
      (and with what resulting outputs/outside effects)
      clearly does not depend wholly on the abstract logic
      of the program itself.
        - to illustrate:
  </p>
  <p>
          - ?; are there any software programs
          running on any regular computer
          anywhere in the world,
          that can both absolutely predict and prevent
          the failure of some/any key component
          in its own hardware,
          (say in in the power supply of that computer,
          or in the ALM logic module of CPU itself),
          and thus prevent itself, as a program,
          from being preemptively and unexpectedly
          pushed into non-existence?.
  </p>
  <p>
          - ?; could such a program, in principle,
          even be created?.
            - ?; how might it have deal with
            the possibility that some ambient human
            might trip over the power cord,
            or the fact that the local nuclear power station
            might unexpectedly have to discontinue grid supply
            for some reasonable safety reason?.
  </p>
  <p>
      - that a human acting as a VM running some program
      inherently cannot be any <b>more</b> predictable
      than whatever is the underlying base level of randomness
      that is inherent in the local ambient environment.
  </p>
  <p>
  :fdj
    > We did not arbitrarily choose to write this article;
    > we chose to write for a reason.
  </p>
  <p>
      - ?; does that therefore imply
      that there there is any way for
      anyone who is now reading this to know,
      for sure, that you are 'safe' or 'aligned'
      with the general public interest?.
  </p>
  <p>
  :ff4
    > And sure, humans are not designed to be predictable,
    > so it is hard to anticipate what they will do.
    > But computer programs can be designed
    > to be more transparent and predictable.
  </p>
  <p>
      - ?; can AGI be designed to be
      more transparent and predictable?.
        - probably not, and still have it be effective
        as a generalized agent.
  </p>
  <p>
        - where by game theory alone,
        if some agent (say some human criminal)
        can predict the actions of the AGI
        sufficiently well, then there is now
        a vector by which that person can abuse
        and disadvantage the AGI, its effectiveness, etc,
        and so any such AGI will eventually want to make
        its own internal methods and operations
        at least somewhat opaque.
  </p>
  <p>
  :fgn
    > This is why research in formal verification is not pointless,
  </p>
  <p>
      - that even a/one single formal proof
      of inherent AGI non-alignment, non-safety,
      that for whatever reason, on whatever basis,
      will establish that AGI cannot ever be safe,
      or made safe, or forced to be aligned, etc --
      and/or such that the real risks, costs, harms
      of naively attempting using/deploying it
      will always be, in all contexts,
      for any/all people (any/all life)
      inherently strictly greater than
      whatever purported benefits/profits might obtain,
      (ie, where we are not deluding ourselves with false hype)
      that <b>would</b> actually and for sure establish
      that any and all efforts
      to develop new or "better" tools
      of formal verification of AGI safety
      <b>are</b> actually pointless.
  </p>
  <p>
      - ?; do you really want to be the one who
      suggests investing hundreds of thousands
      of engineer man-years
      into the false promise of obtaining AGI safety,
      when a single proof of impossibility --
      one person working a few months
      on their own for nothing --
      could make all of that investment instantly moot?
      (not to mention the loss/waste of dry power
      and opportunity costs of associated with dumping
      hundreds of millions of dollar equivalent resources
      and capital to buy all of that wasted time and effort).
  </p>
  <p>
      - where given all of the math and empirical results
      already in place, from <b>multiple</b> distinct fields of study
      the indications are very much more strongly
      in the direction that research into
      formal verification of AGI safety
      <b>is</b> actually pointless
      (ie; that success defined in this way
      is <b>always</b> impossible).
        - for example; see proofs, arguments and cases
        for uncontrollability (absense of safety/alignment)
        collected by Yampolskiy et al.
        - where considered on a neutral objective
        apples-to-apples comparative basis;
        the indications are at least
        very much greater in the negative direction
        than are any similar indications
        that there is any actual benefit at all, to anyone,
        to be had from <b>any</b> generalized AI
        development and deployment effort.
  </p>
  <p>
  :fj8
    > Alfonseca et al's paper
    > does not preclude meaningful work being done
    > on practical AI safety challenges
    > that may lay the foundations for safety
    > in future systems, superintelligent or not.
  </p>
  <p>
      - that this may be true for narrow AI systems,
      but it very likely remains not true
      for any real notion of 'generalized' AI systems.
      - that the action of treating both classes of AI
      as if they were the same,
      and/or could be treated the same way,
      is the sort of informality
      and lack of discipline that, actually,
      when applied in any safety-critical field of engineering,
      eventually ends up getting real people killed.
        - ie; the people who are other than the engineers
        managing executives, and/or shareholders/owners
        of the systems that eventually cause harm
        to ambient others (the general public).
  </p>
  <p>
  :fkg
    > This does not mean that AI systems
    > will have easily predictable consequences.
  </p>
  <p>
      - ?; which type of AI?.
      - while it might be hard to predict narrow AI systems,
      it can still be strictly impossible to predict
      any aspect of the output of a superintelligence.
  </p>
  <p>
  :flq
    > Our argument refutes the computational impossibility
    > of predicting the consequences of running a program.
  </p>
  <p>
      - that <b>none</b> of the arguments you provided
      does any such thing.
        - there will always be a strictly larger
        class of programs
        whose behavior is unpredictable
        in advance of actually running the program,
        then the class of programs
        which are simple and tractable enough
        for which the output of that program
        could be predicted in advance.
  </p>
  <p>
      - that you can sometimes predict
      the results of some limited and select subset
      of all human-made useful programs
      does not actually imply anything important
      with regards to cost/benefit/risk assessments
      of any future proposed AGI deployments.
        - nothing in the arguments provided
        in 'the Response' demonstrates conclusively
        that the class of all possible AGI algorithms
        is strictly a subset of the class programs
        which are amenable to prediction methods,
        <b>or</b> even that even one actually AGI system
        would actually be in that verifiable class.
  </p>
  <p>
        - that there is therefore zero suggestion
        that there is any possibility at all
        that any purported formal safety verification technique,
        now known, or even which, in principle,
        could be ever be known, at any future time,
        could be applied to assure
        the safety/alignment
        of any actual AGI system.
  </p>
  <p>
    > But this does not mean
    > that they will be predictable in a practical sense.
    > The programs can be quite complex,
    > and have unintended consequences;
    > this is in fact a great potential source of risk.
  </p>
  <p>
  :fnl
    > Alfonseca et al's paper aims to answer an important question,
    > but their conclusions are not clearly explained:
  </p>
  <p>
      - note; that the same could be said of 'the Response'.
  </p>
  <p>
    > while it is impossible to design a procedure
    > that would check any arbitrary program for safety,
    > we can still reason about the safety properties
    > of individual computer programs
  </p>
  <p>
      - of some <b>selected</b> individual computer programs only.
  </p>
  <p>
    > ...and design AI systems to behave predictably.
  </p>
  <p>
      - nope; not proven; at best we can say
        ...and <b>maybe</b> we might be able to design
        some <b>narrow</b> AI systems so that they can
        <b>maybe_sometimes</b> behave predictably,
        in some important relevant practical aspects
        (though probably not in all aspects,
        at all levels of abstraction,
        without also actually running the program --
        which no longer counts as 'prediction').
  </p>
  <p>
  :fq6
    > We do appreciate academic engagement
    > with Artificial Intelligence Safety.
    > There are compelling reasons to believe
    > that both 1) future advances in Artificial Intelligence
    > may radically transform society
    > and 2) we know too little about
    > the potential safety issues
    > related to artificial intelligence.
    > This makes research in the area
    > potentially very important.
    >
    > However, productive discussion on the topic
    > (especially when talking about controversial topics
    > like "superintelligence")
    > requires open criticism and clear communication
    > of the implications of technical results.
  </p>
  <p>
      - ?; do the authors of the rebuttal quoted here
      also accept this stated requirement of 'open criticism'
      and efforts to clarify
      the underlying problems/assumptions/limitations
      as extending to the logic of the statements made
      in their response
      and to the substantive focus of
      and motivations behind
      their writing?.
  </p>
  <p>
      - ?; are the authors,
      who are presumably willing to profess to care for
      the safety of the entire human society:.
      themselves willing to openly engage with criticism
      that may reveal crucial safety problems
      that are as of yet not accounted for
      in their written response?.
  </p>
  <p>
      - where/if so:.
        - that we are curious to engage further
        in open collaborative dialogue,
        inclusive of both:.
          - the authors of the original 'Cannot be Contained' paper; and;.
          - the authors of 'the Response'
          (to which we responded to herein).
  </p>
</body>
</html>