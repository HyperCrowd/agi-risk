TITL:
   *Moral Repugnance*
   By Forrest Landry
   October 28th, 2022.

ABST:
   Why social action in the space of AGI/APS
   total global research stigmatization
   is actually needed.
   Some examination as to why this is hard.

TEXT:

   "Aligned AGI" is as unlikely
   as a perpetual motion machine,
   aka as a "perpetual benefit machine".
   It is similarly as senseless to search
   for the "philosophers stone",
   a purported stone that could turn anything
   into gold, ie, a kind of "perpetual
   money making device" -- also unlikely.

   The only difference between these three
   is whether the principle of action
   is about atoms, energy, or pattern.
   The unfortunate aspect is things
   based on 'pattern' are way more dangerous.
   Metaphorically, consider the risk of
   the Midas Touch, for example.

   Knowing that these are unlikely,
   that "Aligned AGI" is simply not
   realistically a possibility,
   we should (therefore) just stop investing
   any further energy/effort/money
   In AGI development, as it will be a waste.

:gwl
   We need to also be similarly careful
   about any AI development that could lead to AGI,
   by accident, (or by design).
   Knowing where the boundary is between
   these two types of technology --
   one of which is a high category x-risk,
   and the other is merely mainly a precursor
   to that x-risk, without itself also
   being an x-risk -- is critically important.

   While some forms of narrow AI development
   and deployment are maybe sometimes "ok",
   in the sense of not being an x-risk,
   there are, of course, still some concerns.
   Particularly that there are still trends
   of inherent inequality increase with
   many forms of narrow AI usage, and also
   thus, and the usual capitalism problems.
   However, those sorts of things
   are not the topic of this essay,
   and will not be considered further herein.

   When I express the view that *any* form
   of AGI, in any context, public or private,
   is inherently the same as 'the total
   eventual extinction of all organic life',
   I would expect some skepticism, and thus,
   a willingness to examine the arguments.

   This naturally takes some time and patience.
   Some people have this, and others do not.

   Yet the ones actually involved in AGI/APS
   development research do not get a free pass --
   I would expect them to review any and all
   potential and relevant arguments,
   and thus assess collectively and cooperatively
   the actual level of real risks involved.

   When arguments of risk has been identified,
   their actions affect far more than themselves.
   Thus they need to actually understand and
   think about the choices that they are
   actually making, on their own behalf
   and on behalf of the commons in which
   we all live.

:df2
   Overall, when the harm arguments end up
   being confirmed, and that the resulting
   conclusion is "we must *not* develop AGI",
   then certain social responsibilities
   start to become apparent.  Particularly
   actual shifts in the agent behavior of
   any engineers currently working within
   some institutional context on AGI/APS
   advancement, away from such work, and
   to/towards something that is overall
   more humane, and less driven by profits.

   When I express that, in varying ways,
   indirectly in "the alignment community",
   many people reply with some version of:.

      > Well that is just not going to happen.

      > The capitalist forces driving towards
      > AI/AGI/APS R&D cannot be overcome.

      > The best we can do is work tirelessly
      > for the small but nonzero chance
      > that we can successfully align AGI.

   This sort of reaction calls for a measured
   response, particularly when it has been
   shown that the chance of "aligning" AGI/APS
   may as well be perfectly/exactly zero,
   over the long term.  Working to "solve AGI"
   is totally 100% waste, and continuing to
   do so runs the risk of being labeled
   as merely involved in 'social signaling'.

   Instead, there needs to be real effort to
   actually overcoming the forces of capitalism,
   selfishness, corporate greed, myopism,
   personal ignorance, monism, delusion, etc.
   While the probability of succeeding in
   making overall changes of this type are
   admittedly low, they are actually nonzero --
   they have more of a real chance of happening
   than the AGI/APS alignment effort will.

   Our creative skill, rationality,
   and overall hope needs to be applied to
   figuring out the puzzle of cooperative
   human behavior, difficult as it is, so
   that we can overcome the otherwise seeming
   invincible capitalist driving forces.

   From a moral perspective, it is particularly
   the case that our short term choices should
   not have a disproportional impact on others,
   both now and in the future, nor to other
   creatures -- the rest of the biosphere --
   the plants and animals of which also have
   their own rights to live and to be.

   There have been suggestions that engineering
   people like to try to solve impossible problems.
   It is suggested that they apply this to the
   problems of distributed social organization
   that can genuinely be wiser than we could
   even currently imagine -- hopefully wise
   enough to actually solve other hard problems.
   This is, by far, a more realistic (and not
   just safer) course of action, with an actual
   better chance of success.

   The ones own self belief that they can
   'solve anything' eventually, if trying hard
   enough, that every problem has a solution,
   needs to be directed correctly.
   The impulse to make 'great tech' and maybe
   'improve the world' (as in the cliche of
   "make the world a better place") wants
   to be fulfilled in a natural way.

   Unfortunately, the well intended actions
   of technology development and engineering
   can get hijacked by corporate process,
   which, via the executive and marketing aspects
   wants to hype and to sell, for their profit,
   to the maximum extent possible -- whatever
   they can legally get away with, despite the
   damage to community, reputation, health and
   well being, community and culture, etc.

   As such, even if you did not do the hype
   yourself, as an engineer and employee,
   you contributed to it by being the one
   that provided the technology to the corp.
   It is like giving a gun to a thief:
   maybe you did not pull the trigger,
   yet the man would not be dead if not for
   your providing the weapon and bullets.

   So there is a real puzzle: how do we
   overcome the forces of human greed?.

   Interestingly, it can be noticed that
   all manner of religions throughout history
   have actually done this, to at least some
   extent, for thousands of years, and so
   there is at least some hope that the
   problem is solvable, even though we
   might not agree with the techniques used.

:dtu
   Since the center of the practice
   seems to have to do with faith and hope,
   it is valid to consider the belief
   statement:.

     > The best we can do is
     > *try* for AGI alignment,
     > even though it is probably
     > impossible.

   This belief is clearly a mistake.
   This is especially when comparing to
   another option -- cure capitalism --
   which although maybe very difficult
   is at least somewhat plausible overall,
   whereas "make AGI safe" has been shown
   to be actually/reasonably non-plausible.
   Overall, regardless of how assessed,
   the probability of achieving non-greed
   is in all cases always strictly greater
   than 'maybe achieving' outcome AGI/APS
   safety.

   Insofar as this represents some sort of
   'upgrade' to the human cultural/social
   process, which in this case, means overdue
   changes in how we think about
   institutional and community design,
   in cities, countries, and in the "modern"
   world -- what it means to be 'civilized'.

:dy8
   A part of reluctance for these sorts of
   suggestions is probably is cultural,
   ie, particular to the AI safety community
   and online groups like "Less Wrong"
   and "Effective Altruism".  In these groups,
   the idea of "winning the total all future"
   has convinced a lot of thinkers that
   AGI/APS is maybe the best approach.

   The unfortunate reality of the tech
   and engineering industries is that
   many of the most effective participants
   are somewhere on the autism spectrum.
   This implies a number of social aspects
   which in aggregate could explain some
   aspects of the beliefs and responses noted.

   Many of them, having been in their overall
   lives, are socially always the underdog,
   somewhat awkward, autistic spectrum, etc,
   have the natural desire of wanting to
   do something worthwhile, that somehow
   "saves the day", and impresses significant
   others by solving real hard problems.
   Unfortunately, this almost never actually
   impresses other people nearly as much as
   simple personality skills and attractiveness.
   These latter relational factors are only
   rarely overcome by the former ones,
   and usually only under situations of
   that rare form of extreme emergency
   that actually depends on intellectual skill.
   Thus, the apocalyptic fantasies are sometimes
   weirdly thrilling to some of these people.
   It becomes, like anything else, a kind of
   obsession, endlessly and deeply explored,
   as is in the nature of any geek.

   Due to their self identity investment
   in their own focus on technology development,
   many are of these engineering types,
   (attempting to retain something of
   their prior time investment into tech),
   will maybe, in the sort term, suggest that
   they are unwilling to redirect efforts
   away from advancing AI/AGI/APS research,
   and to/towards something more humane.
   (examples: uncorruptible effective distributed
   wisdom enhanced large group governance
   methodologies, global tech impact problem
   solving, ecological restoration, etc).

   They see a much more anthropocentric process
   and outcome as being "less technical" and
   "more social", and therefore "more difficult",
   and this less exciting, tractable, etc,
   than any "solutions" which would involve
   a decrease in the total use of technology.
   These sorts of 'more social, less tech'
   approaches have been elsewhere identified
   as the only viable and realistic solution.

   Nothing less than the actual total global
   stigmatization of AGI/APS research, and/or
   of other x-risk enhancing/increasing
   technologies in any form, need to be stopped.

   Unfortunately, these sorts of suggestions
   will often get rejected, in various ways,
   by other corporate roles who stand to loose
     (corporate owners, investors, stockholders,
     and every manner of executive and manager)
   they are each depending on the engineer
   to do their work so that the corp can profit.

:e4l
   The question ends up being something like
   "either fight the worst offending effects
   of corporate capitalism, *OR* elect to
   'work within the system' to try to make
   the overall profit seeking myopic greed
   maybe somehow a little less harmful".

   Many technologists are already working
   within well funded large corporations,
   and see that path as being inevitable,
   since they are themselves directly and
   personally and maybe hopelessly entangled
   within the overall economic system pattern.
   Like most other workers, they have been
   socially divided, and like everyone else,
   rendered seemingly powerless to "fight
   the system" and/or do something else.

   Given that their skillset is engineering
   rather than social group dynamics process,
   they simply do not look for any type of
   social process solutions.  Everything
   in that space is either only marketing
   and hype, or it is absent altogether,
   the maybe of some old nostalgic dreams.
   It is a form of learned helplessness --
   one encouraged by the existing power elite
   so as to maintain the status quo.

   However, it is no longer so simple.
   If you already know in advance that
   the construction of Advanced AGI/APS
   is functionally equivalent to a forever
   doomsday machine, as a perfected weapon
   against the wellbeing of the world,
   then your maybe choice to 'work within
   the system' is a bit like being a Nazi
   engineer working as part of their atomic
   weapons program.  It is morally repugnant.

   Do you really want to be 'that engineer'
   who made the first worst planet killer?

   How could anyone possibly morally justify
   such an action?

:edy
   To have a near complete absence of any hope
   of anything being different is a kind of a
   lack of imagination, itself a key indicator
   of a response to deep emotional trauma.

   Trauma when defined on a functional basis
   is more about an inner shift in the range
   of what is believed to be available options
   when making a choice.  People who have been
   traumatized tend to have very different
   senses of what is allowable optionality
   then those who do not have trauma in that
   specific way; ie, non-traumatized people
   will see a number of 'reasonable options'
   as possible electives that are already
   implicitly deeply discounted non-options.
   Traumatized people simply have a sense
   of having fewer choices, and less choice
   even when they do have some choice at all.

   So rather than trying to imagine that
   the only hope for humanity is to have
   and use technology to overcome the worst
   aspects of humanity, there is a lack of
   imagination, of faith, in the ability
   of the human to solve problems in/of
   the human.  Moreover, there is a noticed
   lack of imagination as to the ways that
   the artificiality of applied technology,
   as overcoming and displacing the human
   altogether, is actually much much worse.

   Then there is the hypocrisy of saying
   that "have faith in AGI/APS safety" --
   we will figure out a way.  "Human ingenuity
   is infinite, and can solve all problems".
   Of course, technical skill is effective
   when applied to technical problems,
   and much less effective when it comes
   to any actual real social problems.
   Yet we need to solve the problems we have
   rather than the problems we want to have.

   To say that one has no hope in the human
   is to be actively defecting against
   oneself *as* human.  Unfortunately this
   is a common aspect of those suffering
   from whatever forms of C-PTSD, which
   themselves may have been side effects
   from an actual underlying neuro- divergence.
   Most engineers and technologists choose
   that career for actually real reasons.

   The fact is that lots of times in history
   there have been "hopeless causes".
   Not so long ago, famous thinkers claimed
   that there would never be an end to slavery.
   Yet slavery has been abolished worldwide
   (at least legally, on paper -- there is
   still work to be done on 'wage' slavery).

   It at one point seemed that the Nazi Cult,
   as a kind of Borg Army, was invincible,
   that fascism and oligarchy would always win.
   Yet they have not, at least not yet.

   So to have hope that even seemingly large
   scale systemic change is possible,
   especially in the face of actual evidence,
   is to show actual courage.  To deny any
   such hope is a kind of deep theft
   against love and life itself.

:exs
   Many forms of trauma are caused by people
   who themselves had trauma, and simply
   did not take the effort to identify it
   as such, and heal it.  Instead there is
   a kind of retreat into the depersonalized,
   which is what tech is in its essence.

   This shows up as an unwillingness to make
   choices on the basis of what is needed,
   and is rather a choice to choose on
   the basis of only fear, terror, etc.
   The only way to make good effective
   choices is to operate from a basis
   of actual value -- what is wanted,
   rather than what is not wanted.
   This too can be a response to trauma,
   but it cannot remain that.  Especially
   when dealing with advanced powers,
   having outside effects via stronger
   engineering methods and techniques.
   The trauma must be healed, and it
   must not be allowed to cause even
   more trauma, as a side effect.
   Where at all viable, that tn person,
   in presence healing must be restored.

:f54
   Consider how indirect and dis-associative
   modern wholesale industrial weaponry is.
   A single weapons engineers could result
   in the death of hundreds of millions of
   other unknown people -- entire countries.
   Is it really such a good thing that tech
   is nearly always used to make weapons
   more effective?  Does anyone really
   want to be a part of that?

   However, at least in the current case,
   the action and the effects of any war
   are at least proximal in time.
   Yet with advanced AGI/APS as *the* weapon,
   the destroyed target is everyone in the
   future, and they will have no way to know
   or have anything to do or say about it, you
   will already be long dead and lost to time.
   The temporal shield is a perfected defense.

   The people of the past can strike and kill
   the people of the future, without any
   possibility or risk of retaliation at all.
   This is not war -- it is simply murder.

   It seems rather clear that the option
   of "work for a corporation to make ever
   better AI technology so as to fulfill
   a mistaken idea of *maybe* shifting the
   oncoming AGI/APS apocalypse a little --
   it is simply a non start.

   The level of amplification of one person
   killing many, in this case everyone and
   everything, of the trillions of yet unborn,
   is a near perfected certainty, and moreover,
   is accompanied by the absolute certainty
   of your perfected impunity -- the future
   simply cannot possibly leverage any tiny
   sanction against you at all.

:g3q
   Hence, the argument ultimately needs to
   be a moral one, based on organic values,
   rather than merely a utilitarian one.

   Does your need to maybe feed
   your own current family children
   so completely outweigh forever the right
   of your own children to maybe even
   eventually have children of their own?
   That your right to productive work should
   fully and completely preclude theirs?

      > Well that is just not going to happen.

   Good.

      > The capitalist forces driving towards
      > AI/AGI/APS R&D *must* be overcome!

   Much better, yes, agreed.

      > The best we can do is work tirelessly
      > for the small but nonzero chance
      > that we can successfully align humanity
      > towards actual health and wellbeing,
      > on all levels of scale.

   These revised assertions are much better.
   All scales includes personal as well
   as family and community health,
   as well as planetary and ecosystem health.
   That the organic must eventually be supported
   over the artificial, the industrial, and/or
   the delusions of "harmless extraction" of
   personal private "profit" (selfish myopic
   greed, ignorance, myopism, monism, etc).

:fq6
   If I have to signal agreement to some sort
   of weird belief in some utopian hereafter,
   so as to actually have a reasonably good
   chance of a good hereafter for my children,
   then so be it; a small price to pay for
   and towards the actual benefit of humanity.

   Maybe with a little bit of the right kind
   of hope and faith, it will be ok in the end,
   after all.  Perhaps Gurdjieff had it right:
   pray as if everything depended on the prayer,
   and *then* you are to actually work rightly
   as if everything depended on doing
   the right work.  Of course, it is best
   if you actually also understand everything
   that this actually means too.

:note:
   Where it is acknoledged that the author
   of these comments have at one time been
   as much subject to these observations as
   anyone being objectively commented on,
   it is for the very reason of having done
   the hard work of processing these truths
   that any of these comments/observations
   could even be made.  It is not therefore
   not actually a contribution to the overall
   conversation for anyone else to attempt
   to make the 'pot kettle black' observation.
   It has already, implicitly, been done.
