TITL:
   *The Expert Judgemental*
   By Forrest Landry
   Nov 3rd, 2022.

ABST:
   Considers the way that implied pejorative claims
   can derail an otherwise rational conversation
   about critical x-risk safety issues, in any domain.

TEXT:

   The following essay is all about one sentence:.

   > A further consideration is that sometimes
   > people argue that all of this futurist
   > speculation about AI is _really_dumb_,
   > and that its errors could be readily explained
   > by experts who can't be bothered
   > to seriously engage with these questions.

   This was (@ found https://ftxfuturefund.org/announcing-the-future-funds-ai-worldview-prize/) among the general notes
   associated with the 'Worldview Announcement'
   prize contest (about half way down the page),
   and was highlighted in at least one reply (@ post https://www.lesswrong.com/posts/seYS2xaqfhbWpaCNg/loss-of-alignment-is-not-the-high-order-bit-for-ai-risk),
   at the very top of their submitted essay (ie,
   as with emphasis, as orienting their remarks).
   I found this to be rather unfortunate.

   My opinion is the overall remark about 'dumb'
   ends up being surprisingly judgemental,
   on at least several levels, all at once.
   And that moreover, also in several dimensions
   the overall effect is dangerously political.
   (and there will be more on that in a moment).

   To see this as directly manifest, it is
   necessary to expand that one sentence by
   resolving all of the implied references.
   There are several techniques for doing this
   sort of lexical and semantic analysis, and
   I have over the years developed some specific
   techniques for exactly this purpose.  It is,
   or at least should be (my opinion, again), in
   the toolkit of any competent analytic philosopher.

   When actually expanded into the equivalent
   overall (@ EGS https://mflb.com/egs_1/egs_index_2.html) statement form,
   the above quoted line basically becomes some
   near variation of all the following claims:.

     - 1; that all of the (non-expert)
     futurist _speculation_ about AI (alignment
     or x-risk issues, etc) is 'really dumb'.

     - 2; that all of the (presumed) "errors"
     of (non-expert) futurist 'speculation'
     about AI (alignment or x-risk issues)
     can be readily explained by 'experts'.

     - 3; that experts {will not / should not} morally
     bother to seriously engage with (or talk about)
     any questions/concerns raised by (non-expert)
     *speculations* (about AI alignment, x-risk).

   That this comes across as feeling really rough,
   insofar as it leads to the indirect implied
   implications of:.

     - where considering implied claim ^1:.

       - as a very strong judgement, that can
       easily be construed to 'non-experts'
       'are dumb' (and therefore worthless) people.
         - that there is more to life than
         the having of any just one skill,
         and that while not all skills are
         "equally important", it is also
         not the case that not having some
         specific skill is to indicate not
         being a worthwhile person.

       - as also a judgement that having real
       concerns about specific topics that
       have been (pre-) judged as 'taboo'
       (worthless to talk about, etc),
       without any basis for why to think so.

     - where considering implied claim ^2:.

       - where for the idea that 'experts'
       can readily "explain" something;.
       - as usually not so; even the most
       seemingly trivial problems and questions
       of many areas of mathematics (ie; a for
       very sure exemplar of an expert field)
       end up being beyond the powers of *any*
       expert to 'explain' solutions to.
         - example; anything important about
         the distribution of prime numbers,
         or about how 'random' the digits
         of any transcendent number is, etc,
         or about how to show that P != NP.

       - where moreover, that most experts
       are actually *terrible* teachers, and
       have the problem of 'too much information'
       and simply cannot explain anything
       important to 'non-experts' (and even then,
       not always!).
         - that experts tend to be worse at
         listening/learning from anyone else
         as well, as they are already too 'full'.

       - that the pejorative label "errors",
       more than just being assumed, indicates
       that the (communicative) behavior
       of someone (some common person) is
       "wrong" merely for their being a non-expert.
         - as that 'only experts' can *do*
         worthwhile things.
         - as being in combination with the
         above pejorative that only experts
         can *be* worthwhile people, which
         is another form of presumptive arrogance.

     - where considering implied claim ^3:.

       - that 'experts' are described as 'too busy'
       and implicitly as 'too important',
       as a proxy of 'too valuable' to "waste time"
       on anything that matters to any non-expert
       is to also have an arrogant colonialist
       judgement heuristic.

       - as re-enforcing the 'better than you'
       non-conversational power-talk assumption.

       - as maybe covering a non-capability of
       the experts to actually 'engage with'
       such a topic, as it is inconvenient and/or
       uncomfortable (ie; implies a motivation
       or bias of some sort of personal gain).

       - as also attempting to make certain topics
       taboo, as a means to distract or deflect.

       - as a kind of 'judgement' that 'experts'
       know best, and that no one else should get
       a chance to choose, not about the topic,
       or even about what is allowed to consider
       in relation to that topic.


   I am not sure who suggested this way of thinking
   to the Future Fund folk, though I do have the
   feeling impression that I have encountered
   similar notions in a number of other areas.
   In every case, it is overall deceptive rhetoric.

   It is one single sentence that is somehow so
   well designed as to make the possessor think
   that the entire category of topic is not worth
   bothering about.  Ie, that even thinking about
   it is meaningless, and that is a very bad move.

   It even goes beyond this to trigger belonging
   trauma bias and to further imply that somehow
   that someone is moreover faulty in their being,
   that maybe they should feel shame, if they had
   somehow, in their innermost private thoughts
   had the worry that maybe some of these issues
   might actually be real.  It is weaponized shame.

   It is the ultimate in adverse deception leveraged
   against people who are sincerely trying to
   do the right thing.  In that sense, it is a
   an especially and surprisingly dangerous sentence,
   designed to deceive the innocent, completely.

   Thus, each time I encounter this way of thinking,
   it really bothers me.  Largely this is because
   it is overall _dismissive_ of issues that are
   actually really important and serious, as I
   have been at some effort to describe (@ elsewhere https://mflb.com/ai_alignment_1/no_people_as_pets_psr.html),
   and which I know are _not_at_all_ idle concerns.

   Where in regards to the worry that x-risk
   is 'really dumb', and that 'experts' have
   considered this already, these noticings are
   first that the overall action is (actually?)
   (seemingly) (motivated?) (political?) biased
   insofar as it suggests that 'regular people'
   do not get to speak, or have their own choices
   in regards to what is done "on their behalf",
   by "experts", etc.  Ie, it favors one class
   of people (the experts) over and at the expense
   of others, and that those experts may be self
   motivated, or make different mistakes, etc,
   rather than actually world or commons motivated.

   Moreover, it is far too often, as is especially
   the case in this topic of AI/AGI/APS/SAS safety
   that the "experts" have *not* actually been able
   to solve the problem, or even to come up with
   a coherent explanation as to how to even
   consider it (see EY posts in (@ 2008 https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible) and (@ 2022 https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy)).
   The experts have been maybe trying to consider
   the topics, but have consistently been unable to,
   and their (maybe private) feelings of failure
   are not to be taken out on those ambient, as
   to hold them as also non-contributive.

   Second, it is noticed that the the overall
   feeling is that there is a very high level
   of ambient judgement and entitlement that
   is actually entirely unearned/unwarranted.

   One of the main issues with this particular
   category of x-risk (as with at least one other)
   is that one very small group of 'AI experts'
   and 'alignment researchers' is effectively
   making a choice on behalf of the entire rest
   of the world, all other people, and not even
   being willing to consider that as anything
   other than fully completely hubris/arrogance.
   Ie, people act in a way that is dismissive,
   as if anything inconvenient or bothersome
   is to simply be ignored, and that is what
   makes it even worse.

   Third, expert skill in one area does not always
   imply similar level of skill in any others,
   and in the commons, the commons must speak.
   Else there is a noted lack of care in those
   places where care is especially warranted.

   It should be obvious, moreover, that the basis
   of motivation, aim, care, intention, and choice
   are all very different criteria than just skill.
   That one's values are different than one's purposes
   and that both are different than meaning is clear.
   Just because some expert can,
   does not mean that they will.
     - as entirely two different unrelated aspects;
     that assumptions about one do not imply any
     assumptions about the others.

   As such, we should not simply dismiss critical
   x-risk issues as "mere future speculation" or
   as "really dumb" simply because someone, with
   some apparent motivation or (@ bias https://mflb.com/ai_alignment_1/bias_effect_4ai_psr.html), happened to
   suggest that.  Merely making the claim itself
   is not at all equivalent to the justification
   for the level of pejorative judgement implied.

   Anytime I find similar language in any context,
   I find myself wondering sincerely at the
   motivations and/or degree of intellectual
   honesty, let alone emotional integrity and
   overall intentions of process and outcome,
   of any such person as who makes such claims
   (ie, whoever suggested such thinking to the
   Future Fund people in the first place, then).

   In any case, such a statement is for sure,
   best avoided in any rational discourse about
   things that actually matter.
