<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Basic Meta Tags -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- SEO Meta Tags -->
  <meta name="description" content="Comprehensive AGI Risk Analysis">
  <meta name="keywords" content="agi, risk, convergence">
  <meta name="author" content="Forrest Landry">
  <meta name="robots" content="index, follow">

  <!-- Favicon -->
  <link rel="icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">
  <link rel="shortcut icon" href="https://github.githubassets.com/favicons/favicon-dark.png" type="image/png">

  <!-- Page Title (displayed on the browser tab) -->
  <title>Comprehensive AGI Risk Analysis</title>
</head>
<body>
  <p>
  TITL:
     <b>The Expert Judgemental</b>
     By Forrest Landry
     Nov 3rd, 2022.
  </p>
  <p>
  ABST:
     Considers the way that implied pejorative claims
     can derail an otherwise rational conversation
     about critical x-risk safety issues, in any domain.
  </p>
  <p>
  TEXT:
  </p>
  <p>
     The following essay is all about one sentence:.
  </p>
  <p>
     > A further consideration is that sometimes
     > people argue that all of this futurist
     > speculation about AI is _really_dumb_,
     > and that its errors could be readily explained
     > by experts who can't be bothered
     > to seriously engage with these questions.
  </p>
  <p>
     This was (@ found https://ftxfuturefund.org/announcing-the-future-funds-ai-worldview-prize/) among the general notes
     associated with the 'Worldview Announcement'
     prize contest (about half way down the page),
     and was highlighted in at least one reply (@ post https://www.lesswrong.com/posts/seYS2xaqfhbWpaCNg/loss-of-alignment-is-not-the-high-order-bit-for-ai-risk),
     at the very top of their submitted essay (ie,
     as with emphasis, as orienting their remarks).
     I found this to be rather unfortunate.
  </p>
  <p>
     My opinion is the overall remark about 'dumb'
     ends up being surprisingly judgemental,
     on at least several levels, all at once.
     And that moreover, also in several dimensions
     the overall effect is dangerously political.
     (and there will be more on that in a moment).
  </p>
  <p>
     To see this as directly manifest, it is
     necessary to expand that one sentence by
     resolving all of the implied references.
     There are several techniques for doing this
     sort of lexical and semantic analysis, and
     I have over the years developed some specific
     techniques for exactly this purpose.  It is,
     or at least should be (my opinion, again), in
     the toolkit of any competent analytic philosopher.
  </p>
  <p>
     When actually expanded into the equivalent
     overall (@ EGS https://mflb.com/egs_1/egs_index_2.html) statement form,
     the above quoted line basically becomes some
     near variation of all the following claims:.
  </p>
  <p>
       - 1; that all of the (non-expert)
       futurist _speculation_ about AI (alignment
       or x-risk issues, etc) is 'really dumb'.
  </p>
  <p>
       - 2; that all of the (presumed) "errors"
       of (non-expert) futurist 'speculation'
       about AI (alignment or x-risk issues)
       can be readily explained by 'experts'.
  </p>
  <p>
       - 3; that experts {will not / should not} morally
       bother to seriously engage with (or talk about)
       any questions/concerns raised by (non-expert)
       <b>speculations</b> (about AI alignment, x-risk).
  </p>
  <p>
     That this comes across as feeling really rough,
     insofar as it leads to the indirect implied
     implications of:.
  </p>
  <p>
       - where considering implied claim ^1:.
  </p>
  <p>
         - as a very strong judgement, that can
         easily be construed to 'non-experts'
         'are dumb' (and therefore worthless) people.
           - that there is more to life than
           the having of any just one skill,
           and that while not all skills are
           "equally important", it is also
           not the case that not having some
           specific skill is to indicate not
           being a worthwhile person.
  </p>
  <p>
         - as also a judgement that having real
         concerns about specific topics that
         have been (pre-) judged as 'taboo'
         (worthless to talk about, etc),
         without any basis for why to think so.
  </p>
  <p>
       - where considering implied claim ^2:.
  </p>
  <p>
         - where for the idea that 'experts'
         can readily "explain" something;.
         - as usually not so; even the most
         seemingly trivial problems and questions
         of many areas of mathematics (ie; a for
         very sure exemplar of an expert field)
         end up being beyond the powers of <b>any</b>
         expert to 'explain' solutions to.
           - example; anything important about
           the distribution of prime numbers,
           or about how 'random' the digits
           of any transcendent number is, etc,
           or about how to show that P != NP.
  </p>
  <p>
         - where moreover, that most experts
         are actually <b>terrible</b> teachers, and
         have the problem of 'too much information'
         and simply cannot explain anything
         important to 'non-experts' (and even then,
         not always!).
           - that experts tend to be worse at
           listening/learning from anyone else
           as well, as they are already too 'full'.
  </p>
  <p>
         - that the pejorative label "errors",
         more than just being assumed, indicates
         that the (communicative) behavior
         of someone (some common person) is
         "wrong" merely for their being a non-expert.
           - as that 'only experts' can <b>do</b>
           worthwhile things.
           - as being in combination with the
           above pejorative that only experts
           can <b>be</b> worthwhile people, which
           is another form of presumptive arrogance.
  </p>
  <p>
       - where considering implied claim ^3:.
  </p>
  <p>
         - that 'experts' are described as 'too busy'
         and implicitly as 'too important',
         as a proxy of 'too valuable' to "waste time"
         on anything that matters to any non-expert
         is to also have an arrogant colonialist
         judgement heuristic.
  </p>
  <p>
         - as re-enforcing the 'better than you'
         non-conversational power-talk assumption.
  </p>
  <p>
         - as maybe covering a non-capability of
         the experts to actually 'engage with'
         such a topic, as it is inconvenient and/or
         uncomfortable (ie; implies a motivation
         or bias of some sort of personal gain).
  </p>
  <p>
         - as also attempting to make certain topics
         taboo, as a means to distract or deflect.
  </p>
  <p>
         - as a kind of 'judgement' that 'experts'
         know best, and that no one else should get
         a chance to choose, not about the topic,
         or even about what is allowed to consider
         in relation to that topic.
  </p>
  <p>
     I am not sure who suggested this way of thinking
     to the Future Fund folk, though I do have the
     feeling impression that I have encountered
     similar notions in a number of other areas.
     In every case, it is overall deceptive rhetoric.
  </p>
  <p>
     It is one single sentence that is somehow so
     well designed as to make the possessor think
     that the entire category of topic is not worth
     bothering about.  Ie, that even thinking about
     it is meaningless, and that is a very bad move.
  </p>
  <p>
     It even goes beyond this to trigger belonging
     trauma bias and to further imply that somehow
     that someone is moreover faulty in their being,
     that maybe they should feel shame, if they had
     somehow, in their innermost private thoughts
     had the worry that maybe some of these issues
     might actually be real.  It is weaponized shame.
  </p>
  <p>
     It is the ultimate in adverse deception leveraged
     against people who are sincerely trying to
     do the right thing.  In that sense, it is a
     an especially and surprisingly dangerous sentence,
     designed to deceive the innocent, completely.
  </p>
  <p>
     Thus, each time I encounter this way of thinking,
     it really bothers me.  Largely this is because
     it is overall _dismissive_ of issues that are
     actually really important and serious, as I
     have been at some effort to describe (@ elsewhere https://mflb.com/ai_alignment_1/no_people_as_pets_psr.html),
     and which I know are _not_at_all_ idle concerns.
  </p>
  <p>
     Where in regards to the worry that x-risk
     is 'really dumb', and that 'experts' have
     considered this already, these noticings are
     first that the overall action is (actually?)
     (seemingly) (motivated?) (political?) biased
     insofar as it suggests that 'regular people'
     do not get to speak, or have their own choices
     in regards to what is done "on their behalf",
     by "experts", etc.  Ie, it favors one class
     of people (the experts) over and at the expense
     of others, and that those experts may be self
     motivated, or make different mistakes, etc,
     rather than actually world or commons motivated.
  </p>
  <p>
     Moreover, it is far too often, as is especially
     the case in this topic of AI/AGI/APS/SAS safety
     that the "experts" have <b>not</b> actually been able
     to solve the problem, or even to come up with
     a coherent explanation as to how to even
     consider it (see EY posts in (@ 2008 https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible) and (@ 2022 https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy)).
     The experts have been maybe trying to consider
     the topics, but have consistently been unable to,
     and their (maybe private) feelings of failure
     are not to be taken out on those ambient, as
     to hold them as also non-contributive.
  </p>
  <p>
     Second, it is noticed that the the overall
     feeling is that there is a very high level
     of ambient judgement and entitlement that
     is actually entirely unearned/unwarranted.
  </p>
  <p>
     One of the main issues with this particular
     category of x-risk (as with at least one other)
     is that one very small group of 'AI experts'
     and 'alignment researchers' is effectively
     making a choice on behalf of the entire rest
     of the world, all other people, and not even
     being willing to consider that as anything
     other than fully completely hubris/arrogance.
     Ie, people act in a way that is dismissive,
     as if anything inconvenient or bothersome
     is to simply be ignored, and that is what
     makes it even worse.
  </p>
  <p>
     Third, expert skill in one area does not always
     imply similar level of skill in any others,
     and in the commons, the commons must speak.
     Else there is a noted lack of care in those
     places where care is especially warranted.
  </p>
  <p>
     It should be obvious, moreover, that the basis
     of motivation, aim, care, intention, and choice
     are all very different criteria than just skill.
     That one's values are different than one's purposes
     and that both are different than meaning is clear.
     Just because some expert can,
     does not mean that they will.
       - as entirely two different unrelated aspects;
       that assumptions about one do not imply any
       assumptions about the others.
  </p>
  <p>
     As such, we should not simply dismiss critical
     x-risk issues as "mere future speculation" or
     as "really dumb" simply because someone, with
     some apparent motivation or (@ bias https://mflb.com/ai_alignment_1/bias_effect_4ai_psr.html), happened to
     suggest that.  Merely making the claim itself
     is not at all equivalent to the justification
     for the level of pejorative judgement implied.
  </p>
  <p>
     Anytime I find similar language in any context,
     I find myself wondering sincerely at the
     motivations and/or degree of intellectual
     honesty, let alone emotional integrity and
     overall intentions of process and outcome,
     of any such person as who makes such claims
     (ie, whoever suggested such thinking to the
     Future Fund people in the first place, then).
  </p>
  <p>
     In any case, such a statement is for sure,
     best avoided in any rational discourse about
     things that actually matter.
  </p>
</body>
</html>